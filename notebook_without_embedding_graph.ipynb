{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: OMP_NUM_THREADS=None =>\n",
      "... If you are using openblas if you are using openblas set OMP_NUM_THREADS=1 or risk subprocess calls hanging indefinitely\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\33631\\AppData\\Local\\Temp\\ipykernel_109288\\4233750148.py:22: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
      "  from scipy.stats.stats import pearsonr\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import os\n",
    "import networkx as nx\n",
    "from gensim.models import Word2Vec\n",
    "from node2vec import Node2Vec\n",
    "import pickle\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from hpsklearn import HyperoptEstimator, any_classifier, any_preprocessing\n",
    "from hyperopt import tpe\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score \n",
    "from sklearn.preprocessing import LabelBinarizer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from namematcher import NameMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usefull stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_path = \"Data/raw_data/node_information.csv\"\n",
    "test_set_path = \"Data/raw_data/testing_set.txt\"\n",
    "train_set_path = \"Data/raw_data/training_set.txt\"\n",
    "random_preds_path = \"Data/raw_data/random_predictions.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(a,b):\n",
    "    return(a.dot(b) / (np.linalg.norm(a) * np.linalg.norm(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian(a,b):\n",
    "    return(distance.euclidean(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>pub_year</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>new_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16634</th>\n",
       "      <td>9509093</td>\n",
       "      <td>1995</td>\n",
       "      <td>entropy and topology for manifolds with bounda...</td>\n",
       "      <td>Stefano Liberati, Giuseppe Pollifrone</td>\n",
       "      <td>Nucl.Phys.Proc.Suppl.</td>\n",
       "      <td>in this work a deep relation between topology ...</td>\n",
       "      <td>16634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12882</th>\n",
       "      <td>9311092</td>\n",
       "      <td>1993</td>\n",
       "      <td>exact c 1 boundary conformal field theories</td>\n",
       "      <td>Curtis G. Callan, Igor R. Klebanov</td>\n",
       "      <td>Phys.Rev.Lett.</td>\n",
       "      <td>we present a solution of the problem of a free...</td>\n",
       "      <td>12882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27161</th>\n",
       "      <td>9910178</td>\n",
       "      <td>2000</td>\n",
       "      <td>path integral quantization of the poisson-sigm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Annalen</td>\n",
       "      <td>university of dortmund we apply the antifield ...</td>\n",
       "      <td>27161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  pub_year                                              title  \\\n",
       "16634  9509093      1995  entropy and topology for manifolds with bounda...   \n",
       "12882  9311092      1993        exact c 1 boundary conformal field theories   \n",
       "27161  9910178      2000  path integral quantization of the poisson-sigm...   \n",
       "\n",
       "                                     authors           journal_name  \\\n",
       "16634  Stefano Liberati, Giuseppe Pollifrone  Nucl.Phys.Proc.Suppl.   \n",
       "12882     Curtis G. Callan, Igor R. Klebanov         Phys.Rev.Lett.   \n",
       "27161                                    NaN                Annalen   \n",
       "\n",
       "                                                abstract  new_ID  \n",
       "16634  in this work a deep relation between topology ...   16634  \n",
       "12882  we present a solution of the problem of a free...   12882  \n",
       "27161  university of dortmund we apply the antifield ...   27161  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_df = pd.read_csv(information_path, header=None)\n",
    "information_df.columns = [\"ID\",'pub_year','title','authors','journal_name','abstract']\n",
    "### !!!! We have to use new index starting from 0 because of the implementation of karate-club library\n",
    "information_df = information_df.assign(new_ID = [i for i in range(information_df.shape[0])])\n",
    "information_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>node1</th>\n",
       "      <th>node2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>389971</th>\n",
       "      <td>1</td>\n",
       "      <td>19500</td>\n",
       "      <td>19339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144663</th>\n",
       "      <td>0</td>\n",
       "      <td>4255</td>\n",
       "      <td>23226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103437</th>\n",
       "      <td>1</td>\n",
       "      <td>5403</td>\n",
       "      <td>12614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118255</th>\n",
       "      <td>1</td>\n",
       "      <td>15850</td>\n",
       "      <td>12197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413272</th>\n",
       "      <td>0</td>\n",
       "      <td>23311</td>\n",
       "      <td>13657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label  node1  node2\n",
       "389971      1  19500  19339\n",
       "144663      0   4255  23226\n",
       "103437      1   5403  12614\n",
       "118255      1  15850  12197\n",
       "413272      0  23311  13657"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_train_set = pd.read_csv(train_set_path, sep =\" \", header = None)\n",
    "initial_train_set.columns = ['node1','node2','label']\n",
    "### !!! we will use the new indices!!! (see information_df for correspondances)\n",
    "initial_train_set = (initial_train_set\n",
    "    .merge(information_df[['ID','new_ID']], how = 'left', left_on = ['node1'], right_on = ['ID'])\n",
    "    .drop(columns = ['node1','ID'])\n",
    "    .rename(columns = {'new_ID':'node1'})\n",
    "    .merge(information_df[['ID','new_ID']], how = 'left', left_on = ['node2'], right_on = ['ID'])\n",
    "    .drop(columns = ['node2','ID'])\n",
    "    .rename(columns = {'new_ID':'node2'})\n",
    ")\n",
    "initial_train_set.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validation_ratio = 0.1\n",
    "\n",
    "if os.path.isfile(\"Data/processed_data/train_set_no_emb.csv\") and  os.path.isfile(\"Data/processed_data/validation_set_no_emb.csv\"):\n",
    "    # load sets\n",
    "    train_set = pd.read_csv(\"Data/processed_data/train_set_no_emb.csv\",)\n",
    "    validation_set = pd.read_csv(\"Data/processed_data/validation_set_no_emb.csv\")\n",
    "else:\n",
    "\n",
    "\n",
    "    # split train set into train and validation set\n",
    "    X = initial_train_set.drop(columns = ['label'])\n",
    "    y = initial_train_set[['label']]\n",
    "\n",
    "    train_samples, validation_samples, train_labels, validation_labels = train_test_split(X,y, test_size=train_validation_ratio, random_state=0, shuffle=True, stratify=y)\n",
    "    \n",
    "    train_set = pd.concat([train_samples, train_labels], axis = 1)\n",
    "    validation_set = pd.concat([validation_samples, validation_labels], axis = 1)\n",
    "\n",
    "    # save sets\n",
    "    train_set.to_csv(\"Data/processed_data/train_set_no_emb.csv\", index = False)\n",
    "    validation_set.to_csv(\"Data/processed_data/validation_set_no_emb.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We also need to make an \"embedding graph\" to compute the various features (otherwise we would train a model to predict edges on a graph in which the edges are present)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information pre_processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                 0\n",
       "pub_year           0\n",
       "title              0\n",
       "authors         4033\n",
       "journal_name    7472\n",
       "abstract           0\n",
       "new_ID             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_df = information_df.fillna({'authors':'', 'journal_name':''})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_df.authors = information_df.authors.apply(lambda x:x.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\"Data/processed_data/information.csv\"):\n",
    "    information_df = pickle.load(open(\"Data/processed_data/information.csv\",'rb'))\n",
    "else:\n",
    "    information_df['title_lemma'] = information_df.title.apply(lambda x: [token.lemma_ for token in spacy_nlp(x) if not token.is_punct if not token.is_digit if not token.is_stop])\n",
    "    pickle.dump(information_df, open(\"Data/processed_data/information.csv\",'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Articles based graph (based on embeddings graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of nodes: 27770\n",
      "The number of edges: 301262\n"
     ]
    }
   ],
   "source": [
    "nodes = list(information_df.new_ID.unique())\n",
    "edges = set(train_set.query(\"label==1\").apply(lambda x: (x.node1,x.node2), axis = 1))\n",
    "\n",
    "\n",
    "G_articles_embedding = nx.Graph()\n",
    "G_articles_embedding.add_nodes_from(nodes)\n",
    "G_articles_embedding.add_edges_from(edges)\n",
    "\n",
    "print(\"The number of nodes: {}\".format(G_articles_embedding .number_of_nodes()))\n",
    "print(\"The number of edges: {}\".format(G_articles_embedding .number_of_edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors co-authorship based graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# convert to lower case, remove punctuation, strip the names\n",
    "authors_raw_set = set([auth.strip().lower().translate(str.maketrans('', '', string.punctuation)) for list_auth in information_df.authors for auth in list_auth if len(auth)>1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name matching: to make identify people name by different ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from namematcher import NameMatcher\n",
    "name_matcher = NameMatcher()\n",
    "\n",
    "def compute_unique_names(authors_raw_set):\n",
    "    \"\"\"\n",
    "    one author can be named differently on different papers\n",
    "    this function aims at finding a 'representant' (longest name that describe an author) for each \n",
    "    author\n",
    "    inputs:\n",
    "        - authors_raw_set: set of previously extracted author names\n",
    "    outputs:\n",
    "        - dict: keys are the name in authors_raw_set and the values are the representant\n",
    "    \"\"\"\n",
    "    representant_dict = {}\n",
    "    attributed_nodes = [] # names that already have a representant\n",
    "    for name in tqdm(authors_raw_set, position = 0):\n",
    "        sim_list = [] # similar names \n",
    "        if name not in attributed_nodes:\n",
    "            for name2 in authors_raw_set:\n",
    "                try:\n",
    "                    if name != name2 and name[0]==name2[0] and name2 not in attributed_nodes:\n",
    "                        # two names need to start by the same letter to be consider as potential equivalents\n",
    "                        score = name_matcher.match_names(name, name2)\n",
    "                        if score > 0.9: # if names are close enough\n",
    "                            sim_list.append(name2)\n",
    "                except:\n",
    "                    continue\n",
    "            sim_list.append(name) # the representant is in this list\n",
    "            attributed_nodes.extend(sim_list) # we have fund a representant for those names\n",
    "            representant = max(sim_list, key=len) # the representant is the longest name\n",
    "            for name in sim_list: # all those names have the same representant\n",
    "                representant_dict[name] = representant\n",
    "    return(representant_dict)\n",
    "\n",
    "if os.path.isfile('Data/processed_data/representant_dict.pkl'):\n",
    "    representant_dict = pickle.load(open('Data/processed_data/representant_dict.pkl','rb'))\n",
    "else:\n",
    "    representant_dict = compute_unique_names(authors_raw_set)\n",
    "    pickle.dump(representant_dict, open('Data/processed_data/representant_dict.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set each name to its representant value\n",
    "information_df.authors = information_df.authors.apply(lambda x: [representant_dict[auth.strip().lower().translate(str.maketrans('', '', string.punctuation))] for auth in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a unique index for each author\n",
    "representants_list = list(set(representant_dict.values()))\n",
    "authors2idx = {k: v for v, k in enumerate(representants_list)}\n",
    "\n",
    "information_df[\"authors_id\"] = information_df.authors.apply(lambda x: [authors2idx[auth] for auth in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>pub_year</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>new_ID</th>\n",
       "      <th>title_lemma</th>\n",
       "      <th>authors_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19302</th>\n",
       "      <td>9610199</td>\n",
       "      <td>1996</td>\n",
       "      <td>combinatorics of solitons in noncritical strin...</td>\n",
       "      <td>[masafumi fukuma, shigeaki yahikozawa]</td>\n",
       "      <td>Phys.Lett.</td>\n",
       "      <td>we study the combinatorics of solitons in d 2 ...</td>\n",
       "      <td>19302</td>\n",
       "      <td>[combinatoric, soliton, noncritical, string, t...</td>\n",
       "      <td>[3054, 4069]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16943</th>\n",
       "      <td>9511006</td>\n",
       "      <td>1995</td>\n",
       "      <td>reduction of coupling parameters</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>energy physics and field theory moscow-protvin...</td>\n",
       "      <td>16943</td>\n",
       "      <td>[reduction, couple, parameter]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  pub_year                                              title  \\\n",
       "19302  9610199      1996  combinatorics of solitons in noncritical strin...   \n",
       "16943  9511006      1995                   reduction of coupling parameters   \n",
       "\n",
       "                                      authors journal_name  \\\n",
       "19302  [masafumi fukuma, shigeaki yahikozawa]   Phys.Lett.   \n",
       "16943                                      []                \n",
       "\n",
       "                                                abstract  new_ID  \\\n",
       "19302  we study the combinatorics of solitons in d 2 ...   19302   \n",
       "16943  energy physics and field theory moscow-protvin...   16943   \n",
       "\n",
       "                                             title_lemma    authors_id  \n",
       "19302  [combinatoric, soliton, noncritical, string, t...  [3054, 4069]  \n",
       "16943                     [reduction, couple, parameter]           [0]  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute nodes and edges\n",
    "pre_edges = list(information_df.authors_id.apply(lambda x : [(x[i],x[j]) for i in range(len(x)) for j in range(len(x)) if i>j]))\n",
    "authors_edges = [edge for list_edge in pre_edges for edge in list_edge]\n",
    "authors_edges_dict = Counter(authors_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of nodes: 14447\n",
      "The number of edges: 29111\n"
     ]
    }
   ],
   "source": [
    "G_authors_co_auth = nx.Graph()\n",
    "G_authors_co_auth.add_nodes_from(authors2idx.values())\n",
    "G_authors_co_auth.add_weighted_edges_from([(a,b,weight) for (a,b),weight in authors_edges_dict.items()])\n",
    "\n",
    "print(\"The number of nodes: {}\".format(G_authors_co_auth.number_of_nodes()))\n",
    "print(\"The number of edges: {}\".format(G_authors_co_auth.number_of_edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create autho co_citation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_citation = (train_set\n",
    "    .query('label==1')\n",
    "    .merge(information_df,how = 'left', left_on = [\"node1\"], right_on = 'new_ID')\n",
    "    [['node1',\t'node2', 'authors_id']]\n",
    "    .rename(columns = {\"authors_id\":'authors1'})\n",
    "    .merge(information_df,how = 'left', left_on = [\"node2\"], right_on = 'new_ID')\n",
    "    [['node1',\t'node2', 'authors1',\"authors_id\"]]\n",
    "    .rename(columns = {\"authors_id\":'authors2'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_citation_list = list(co_citation.apply(lambda x: [(auth1,auth2) for auth1 in x.authors1 for auth2 in x.authors2 if auth1!=auth2  if auth1!='' if auth2!='' ], axis = 1))\n",
    "edges_list = [edge for edge_list in co_citation_list for edge in edge_list]\n",
    "authors_citation_edges_dict = Counter(edges_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of nodes: 14447\n",
      "The number of edges: 550677\n"
     ]
    }
   ],
   "source": [
    "G_authors_co_citation = nx.Graph()\n",
    "G_authors_co_citation.add_nodes_from(authors2idx.values())\n",
    "G_authors_co_citation.add_weighted_edges_from([(a,b,weight) for (a,b),weight in authors_citation_edges_dict.items()])\n",
    "\n",
    "print(\"The number of nodes: {}\".format(G_authors_co_citation.number_of_nodes()))\n",
    "print(\"The number of edges: {}\".format(G_authors_co_citation.number_of_edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various embeddings computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Based embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using articles graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Walklets\n",
    "from karateclub import Walklets\n",
    "if os.path.isfile('Data/processed_data/articles_walklets_embeddings_no_emb.pkl'):\n",
    "    walklets_articles_embeddings = pickle.load(open('Data/processed_data/articles_walklets_embeddings_no_emb.pkl','rb'))\n",
    "else:\n",
    "    walklets = Walklets() # we leave the defaults parameters for the other values\n",
    "    walklets.fit(G_articles_embedding)\n",
    "    walklets_articles_embeddings = walklets.get_embedding()\n",
    "    pickle.dump(walklets_articles_embeddings, open('Data/processed_data/articles_walklets_embeddings_no_emb.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Node2Vec\n",
    "from karateclub import Node2Vec\n",
    "if os.path.isfile('Data/processed_data/articles_node2vec_embeddings_no_emb.pkl'):\n",
    "    articles_node2vec_embeddings = pickle.load(open('Data/processed_data/articles_node2vec_embeddings_no_emb.pkl','rb'))\n",
    "else:\n",
    "    node2vec = Node2Vec() # we leave the defaults parameters for the other values\n",
    "    node2vec.fit(G_articles_embedding)\n",
    "    articles_node2vec_embeddings = node2vec.get_embedding()\n",
    "    pickle.dump(articles_node2vec_embeddings, open('Data/processed_data/articles_node2vec_embeddings_no_emb.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### deepwalk\n",
    "from karateclub import DeepWalk\n",
    "if os.path.isfile('Data/processed_data/articles_DeepWalk_embeddings_no_emb.pkl'):\n",
    "    articles_DeepWalk_embeddings = pickle.load(open('Data/processed_data/articles_DeepWalk_embeddings_no_emb.pkl','rb'))\n",
    "else:\n",
    "    DeepWalk = DeepWalk() # we leave the defaults parameters for the other values\n",
    "    DeepWalk.fit(G_articles_embedding)\n",
    "    articles_DeepWalk_embeddings = DeepWalk.get_embedding()\n",
    "    pickle.dump(articles_DeepWalk_embeddings, open('Data/processed_data/articles_DeepWalk_embeddings_no_emb.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using author graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### authors embedding leveraging author graphs (random walk for instance)\n",
    "\n",
    "### Node2Vec\n",
    "if os.path.isfile('Data/processed_data/authors_node2vec_embeddings_no_emb.pkl'):\n",
    "    authors_node2vec_embeddings = pickle.load(open('Data/processed_data/authors_node2vec_embeddings_no_emb.pkl', 'rb'))\n",
    "else:\n",
    "    node2vec_authors = Node2Vec()\n",
    "    node2vec_authors.fit(G_authors_co_auth)\n",
    "    authors_node2vec_embeddings = node2vec_authors.get_embedding()\n",
    "    pickle.dump(authors_node2vec_embeddings, open('Data/processed_data/authors_node2vec_embeddings_no_emb.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### authors embedding leveraging author graphs co citation  (random walk for instance)\n",
    "\n",
    "### Node2Vec\n",
    "if os.path.isfile('Data/processed_data/authors_node2vec_embeddings_no_emb_citation.pkl'):\n",
    "    authors_node2vec_embeddings_citation = pickle.load(open('Data/processed_data/authors_node2vec_embeddings_no_emb_citation.pkl', 'rb'))\n",
    "else:\n",
    "    node2vec_authors_citation = Node2Vec()\n",
    "    node2vec_authors_citation.fit(G_authors_co_citation)\n",
    "    authors_node2vec_embeddings_citation = node2vec_authors_citation.get_embedding()\n",
    "    pickle.dump(authors_node2vec_embeddings_citation, open('Data/processed_data/authors_node2vec_embeddings_no_emb_citation.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>pub_year</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>new_ID</th>\n",
       "      <th>title_lemma</th>\n",
       "      <th>authors_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14340</th>\n",
       "      <td>9408088</td>\n",
       "      <td>1994</td>\n",
       "      <td>postmodern string theory</td>\n",
       "      <td>[aaurilia, espallucci, ivanzetta]</td>\n",
       "      <td>Phys.Rev.</td>\n",
       "      <td>in this paper we study the dynamics of a stati...</td>\n",
       "      <td>14340</td>\n",
       "      <td>[postmodern, string, theory]</td>\n",
       "      <td>[5032, 5327, 63]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20434</th>\n",
       "      <td>9704015</td>\n",
       "      <td>1997</td>\n",
       "      <td>spinons and parafermions in fermion cosets</td>\n",
       "      <td>[dccabra]</td>\n",
       "      <td></td>\n",
       "      <td>to the memory of d v volkov kharkov january 5-...</td>\n",
       "      <td>20434</td>\n",
       "      <td>[spinon, parafermion, fermion, coset]</td>\n",
       "      <td>[2384]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  pub_year                                       title  \\\n",
       "14340  9408088      1994                    postmodern string theory   \n",
       "20434  9704015      1997  spinons and parafermions in fermion cosets   \n",
       "\n",
       "                                 authors journal_name  \\\n",
       "14340  [aaurilia, espallucci, ivanzetta]    Phys.Rev.   \n",
       "20434                          [dccabra]                \n",
       "\n",
       "                                                abstract  new_ID  \\\n",
       "14340  in this paper we study the dynamics of a stati...   14340   \n",
       "20434  to the memory of d v volkov kharkov january 5-...   20434   \n",
       "\n",
       "                                 title_lemma        authors_id  \n",
       "14340           [postmodern, string, theory]  [5032, 5327, 63]  \n",
       "20434  [spinon, parafermion, fermion, coset]            [2384]  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each article take a mean of the authors embedding as global autors embedding (idem for citation)\n",
    "articles_authors_embedding = []\n",
    "articles_authors_embedding_citation = []\n",
    "for i in range(information_df.shape[0]):\n",
    "    value = information_df[information_df.new_ID == i]\n",
    "    authors_id = value.authors_id\n",
    "    embeddings = np.array([0 for i in range(128)]).astype('float64')\n",
    "    embeddings_citation = np.array([0 for i in range(128)]).astype('float64')\n",
    "    for author in authors_id:\n",
    "        embeddings+=authors_node2vec_embeddings[author][0]\n",
    "        embeddings_citation+=authors_node2vec_embeddings_citation[author][0]\n",
    "    articles_authors_embedding.append(embeddings/len(authors_id))\n",
    "    articles_authors_embedding_citation.append(embeddings_citation/len(authors_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node based embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute abstracts embeddings using specter network\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = 'gpu'\n",
    "\n",
    "if os.path.isfile('Data/processed_data/abstracts_embeddings.pkl'):\n",
    "    abstracts_embeddings = pickle.load(open('Data/processed_data/abstracts_embeddings.pkl','rb'))\n",
    "else:\n",
    "    \n",
    "    # load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('allenai/specter')\n",
    "    model = AutoModel.from_pretrained('allenai/specter').to(device)\n",
    "    model.eval()\n",
    "    abstracts_embeddings = []\n",
    "    for i in tqdm(range(information_df.shape[0]), position = 0):\n",
    "        article = information_df.loc[i]\n",
    "        title = article.title\n",
    "        abstract = article.abstract\n",
    "        paper = [{'title':title, 'abstract':abstract}]\n",
    "\n",
    "        # concatenate title and abstract\n",
    "        title_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in paper]\n",
    "        # preprocess the input\n",
    "        inputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "        inputs = inputs.to(device)\n",
    "        result = model(**inputs)\n",
    "        # take the first token in the batch as the embedding\n",
    "        embedding = result.last_hidden_state[:, 0, :].detach().cpu().numpy()\n",
    "        abstracts_embeddings.append(embedding)\n",
    "    pickle.dump(abstracts_embeddings, open('Data/processed_data/abstracts_embeddings.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Class.Quant.Grav.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(information_df[information_df.new_ID==1].journal_name.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute page rank\n",
    "page_rank_dict = nx.pagerank(G_articles_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compute degree centrality\n",
    "centrality_dict = nx.degree_centrality(G_articles_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute edges features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(graph, edge):\n",
    "\n",
    "    inter_size = len(list(nx.common_neighbors(graph, edge[0], edge[1])))\n",
    "    union_size = len(set(graph[edge[0]]) | set(graph[edge[1]]))\n",
    "    try:\n",
    "        jacard = inter_size / union_size\n",
    "    except:\n",
    "        jacard = np.nan\n",
    "\n",
    "    return jacard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdamicAdar(graph, edge):\n",
    "\n",
    "    inter_list = nx.common_neighbors(graph, edge[0], edge[1])\n",
    "    try:\n",
    "        adamic_adar =  sum( [1/np.log(graph.degree(node)) for node in inter_list])\n",
    "    except:\n",
    "        adamic_adar = np.nan\n",
    "    \n",
    "    return adamic_adar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preferential_attachement(graph, edge):\n",
    "    pa = graph.degree(edge[0]) * graph.degree(edge[1])\n",
    "    return pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_connected(graph,edge):\n",
    "    try:\n",
    "        connect = nx.shortest_path(graph, source=edge[0], target=edge[1])\n",
    "        connected  = 1\n",
    "    except:\n",
    "        connected = 0\n",
    "    return(connected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_path(graph,edge):\n",
    "    try:\n",
    "        l = nx.shortest_path_length(graph, source=edge[0], target=edge[1])\n",
    "    except:\n",
    "        l = -1\n",
    "    return(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_journal(information_df, node1, node2):\n",
    "    journal1 = str(information_df[information_df.new_ID==node1].journal_name.values[0])\n",
    "    journal2 = str(information_df[information_df.new_ID==node2].journal_name.values[0])\n",
    "\n",
    "    if journal1 == '' or journal2 == '':\n",
    "        return(-1)\n",
    "    elif journal1 == journal2:\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_non_embeddings_features(df, information_df, G_articles):\n",
    "    useful_information_df = information_df[['new_ID','authors','pub_year', 'title_lemma']]\n",
    "\n",
    "    # prepare data frame for common authors computation\n",
    "    df = (df\n",
    "    .merge(useful_information_df, how ='left', left_on = ['node1'], right_on = ['new_ID'])\n",
    "    .rename(columns = {'authors':'authors_node_1', 'pub_year':'pub_year1', 'title_lemma':'title_lemma1'})\n",
    "    .merge(useful_information_df, how ='left', left_on = ['node2'], right_on = ['new_ID'])\n",
    "    .rename(columns = {'authors':'authors_node_2', 'pub_year':'pub_year2', 'title_lemma':'title_lemma2'})\n",
    "    )\n",
    "\n",
    "    print(\"common_journal\")\n",
    "    df['common_journals'] = df.apply(lambda x: common_journal(information_df, x.node1, x.node2),axis = 1)\n",
    "    \n",
    "    print('computing common authors')\n",
    "    #  compute common authors\n",
    "    df['common_authors'] = df.apply(lambda x:len(set(x.authors_node_1)&set(x.authors_node_2)),axis = 1)\n",
    "\n",
    "    print('computing common words')\n",
    "    #  compute common words in titles\n",
    "    df['common_title_words'] = df.apply(lambda x:len(set(x.title_lemma1)&set(x.title_lemma2)),axis = 1)\n",
    "\n",
    "    print('computing delta publication year')\n",
    "    # compute delta publication year\n",
    "    df['delta_publication'] = df.apply(lambda x:np.abs(x.pub_year2 - x.pub_year1),axis = 1)\n",
    "\n",
    "    # compute edges features\n",
    "    print('computing jacard index')\n",
    "    df['jacard'] = df.apply(lambda x: Jaccard(G_articles, (x.node1, x.node2)),axis = 1)\n",
    "\n",
    "    print('computing preferential attachement')\n",
    "    df['pa'] = df.apply(lambda x: preferential_attachement(G_articles, (x.node1, x.node2)),axis = 1)\n",
    "\n",
    "    print('computing adamic_adar')\n",
    "    df['adamic_adar'] = df.apply(lambda x: AdamicAdar(G_articles, (x.node1, x.node2)),axis = 1)\n",
    "\n",
    "    print('are connected')\n",
    "    df['connection'] = df.apply(lambda x: are_connected(G_articles, (x.node1, x.node2)),axis = 1)\n",
    "\n",
    "\n",
    "    print('page ranks')\n",
    "    df['page_rank1'] = df.apply(lambda x: page_rank_dict[x.node1],axis = 1)\n",
    "    df['page_rank2'] = df.apply(lambda x: page_rank_dict[x.node2],axis = 1)\n",
    "    \n",
    "    print('compute degree')\n",
    "\n",
    "    df['degree1'] = df.apply(lambda x: centrality_dict[x.node1],axis = 1)\n",
    "    df['degree2'] = df.apply(lambda x: centrality_dict[x.node2],axis = 1)\n",
    "\n",
    "    \n",
    "    df = df.fillna({ 'jacard':df.jacard.mean(),\n",
    "                     'adamic_adar':df.adamic_adar.mean()\n",
    "                     })\n",
    "\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding_cosines_features(df,\n",
    "                                         articles_node2vec_embeddings,\n",
    "                                         walklets_articles_embeddings,\n",
    "                                         articles_authors_embedding, \n",
    "                                         abstracts_embeddings):\n",
    "    # compute some cosine based distances\n",
    "    df['articles_node2vec_cosine'] = df.apply(lambda x:cosine(articles_node2vec_embeddings[x.node1],articles_node2vec_embeddings[x.node2]), axis = 1)\n",
    "    df['articles_walklets_cosine'] = df.apply(lambda x:cosine(walklets_articles_embeddings[x.node1],walklets_articles_embeddings[x.node2]), axis = 1)\n",
    "    df['authors_embeddings_cosine'] = df.apply(lambda x:cosine(articles_authors_embedding[x.node1],articles_authors_embedding[x.node2]), axis = 1)\n",
    "    df['abstracts_embeddings_cosine'] = df.apply(lambda x:cosine(abstracts_embeddings[x.node1][0],abstracts_embeddings[x.node2][0]), axis = 1)\n",
    "    df['articles_deepwalk_cosine'] = df.apply(lambda x:cosine(articles_DeepWalk_embeddings[x.node1],articles_DeepWalk_embeddings[x.node2]), axis = 1)\n",
    "    df['authors_embeddings_cosine_citation'] = df.apply(lambda x:cosine(articles_authors_embedding_citation[x.node1],articles_authors_embedding_citation[x.node2]), axis = 1)\n",
    "\n",
    "    df['articles_node2vec_euclidian'] = df.apply(lambda x:euclidian(articles_node2vec_embeddings[x.node1],articles_node2vec_embeddings[x.node2]), axis = 1)\n",
    "    df['articles_walklets_euclidian'] = df.apply(lambda x:euclidian(walklets_articles_embeddings[x.node1],walklets_articles_embeddings[x.node2]), axis = 1)\n",
    "    df['authors_embeddings_euclidian'] = df.apply(lambda x:euclidian(articles_authors_embedding[x.node1],articles_authors_embedding[x.node2]), axis = 1)\n",
    "    df['abstracts_embeddings_euclidian'] = df.apply(lambda x:euclidian(abstracts_embeddings[x.node1][0],abstracts_embeddings[x.node2][0]), axis = 1)\n",
    "    df['articles_deepwalk_euclidian'] = df.apply(lambda x:euclidian(articles_DeepWalk_embeddings[x.node1],articles_DeepWalk_embeddings[x.node2]), axis = 1)\n",
    "    df['authors_embeddings_euclidian_citation'] = df.apply(lambda x:euclidian(articles_authors_embedding_citation[x.node1],articles_authors_embedding_citation[x.node2]), axis = 1)\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute features on train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common_journal\n",
      "computing common authors\n",
      "computing common words\n",
      "computing delta publication year\n",
      "computing jacard index\n",
      "computing preferential attachement\n",
      "computing adamic_adar\n",
      "are connected\n",
      "page ranks\n",
      "compute degree\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(\"Data/processed_data/train_set_with_features_no_emb.csv\"):\n",
    "    train_set_with_features = pd.read_csv(\"Data/processed_data/train_set_with_features_no_emb.csv\")\n",
    "else:\n",
    "    train_set_with_features = compute_non_embeddings_features(initial_train_set, information_df, G_articles_embedding)\n",
    "    train_set_with_features = compute_embedding_cosines_features(train_set_with_features,\n",
    "                                            articles_node2vec_embeddings,\n",
    "                                            walklets_articles_embeddings,\n",
    "                                            articles_authors_embedding, \n",
    "                                            abstracts_embeddings)\n",
    "    train_set_with_features.to_csv(\"Data/processed_data/train_set_with_features_no_emb.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep= ['common_authors', 'common_title_words','common_journals',\n",
    "       'delta_publication', 'jacard', 'pa', 'adamic_adar', 'connection',\n",
    "       'page_rank1', 'page_rank2', 'degree1', 'degree2',\n",
    "       'articles_node2vec_cosine', 'articles_walklets_cosine',\n",
    "       'authors_embeddings_cosine', 'abstracts_embeddings_cosine','articles_deepwalk_cosine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_with_features = train_set_with_features[columns_to_keep+['label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute features on validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common_journal\n",
      "computing common authors\n",
      "computing common words\n",
      "computing delta publication year\n",
      "computing jacard index\n",
      "computing preferential attachement\n",
      "computing adamic_adar\n",
      "are connected\n",
      "page ranks\n",
      "compute degree\n"
     ]
    }
   ],
   "source": [
    "validation_set_with_features = compute_non_embeddings_features(validation_set, information_df, G_articles_embedding)\n",
    "validation_set_with_features = compute_embedding_cosines_features(validation_set_with_features,\n",
    "                                         articles_node2vec_embeddings,\n",
    "                                         walklets_articles_embeddings,\n",
    "                                         articles_authors_embedding, \n",
    "                                         abstracts_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set_with_features = validation_set_with_features[columns_to_keep+['label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples, train_labels = train_set_with_features.drop(columns = ['label']), train_set_with_features[['label']]\n",
    "validation_samples, validation_labels = validation_set_with_features.drop(columns = ['label']), validation_set_with_features[['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "train_samples_scaled = scaler.fit_transform(np.float32(train_samples))\n",
    "validation_samples_scaled = scaler.transform(np.float32(validation_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nestim = HyperoptEstimator(classifier=any_classifier('my_clf'),\\n                          preprocessing=any_preprocessing('my_pre'),\\n                          algo=tpe.suggest,\\n                          max_evals=100,\\n                          trial_timeout=120)\\n\\nestim.fit(train_samples_scaled,list(train_labels.label))\\n\\n# Show the results\\n\\nprint(estim.score(validation_samples_scaled, list(validation_labels.label)))\\n\\nprint(estim.best_model())\\n\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### use hyper opt to find the best model\n",
    "'''\n",
    "estim = HyperoptEstimator(classifier=any_classifier('my_clf'),\n",
    "                          preprocessing=any_preprocessing('my_pre'),\n",
    "                          algo=tpe.suggest,\n",
    "                          max_evals=100,\n",
    "                          trial_timeout=120)\n",
    "\n",
    "estim.fit(train_samples_scaled,list(train_labels.label))\n",
    "\n",
    "# Show the results\n",
    "\n",
    "print(estim.score(validation_samples_scaled, list(validation_labels.label)))\n",
    "\n",
    "print(estim.best_model())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best :{'learner': ExtraTreesClassifier(bootstrap=True, max_features=0.7327186439657982,\n",
    "                     n_estimators=95, n_jobs=1, random_state=3, verbose=False), 'preprocs': (), 'ex_preprocs': ()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# train a model \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf_xt = ExtraTreesClassifier(bootstrap=True, max_features=0.7327186439657982,\n",
    "                     n_estimators=95, n_jobs=1, random_state=3, verbose=False)\n",
    "\n",
    "clf_xt.fit(train_samples_scaled,list(train_labels.label))\n",
    "\n",
    "# test the model\n",
    "predicted_labels_xt = clf_xt.predict(validation_samples_scaled)\n",
    "acc = accuracy_score(validation_labels, list(predicted_labels_xt))\n",
    "print(f\"accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\33631\\Documents\\Etudes\\centrale 3A\\MLNS\\MLNS_Kaggle_Challenge\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:18:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "accuracy: 0.9696191837795685\n"
     ]
    }
   ],
   "source": [
    "# train a model \n",
    "\n",
    "clf_xgb = XGBClassifier()\n",
    "\n",
    "clf_xgb.fit(train_samples_scaled,list(train_labels.label))\n",
    "\n",
    "# test the model\n",
    "predicted_labels_xgb = clf_xgb.predict(validation_samples_scaled)\n",
    "acc = accuracy_score(validation_labels, list(predicted_labels_xgb))\n",
    "print(f\"accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9634617884065506\n"
     ]
    }
   ],
   "source": [
    "# train a model \n",
    "from sklearn.svm import SVC\n",
    "clf_svc = SVC()\n",
    "\n",
    "clf_svc.fit(train_samples_scaled,list(train_labels.label))\n",
    "\n",
    "# test the model\n",
    "predicted_labels_svc = clf_svc.predict(validation_samples_scaled)\n",
    "acc = accuracy_score(validation_labels, list(predicted_labels_svc))\n",
    "print(f\"accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9774987002859371\n"
     ]
    }
   ],
   "source": [
    "# voting\n",
    "total_pred = np.array(predicted_labels_svc)+np.array(predicted_labels_xt)+np.array(predicted_labels_xgb)\n",
    "total_pred = (total_pred > 1.5)\n",
    "acc = accuracy_score(validation_labels, list(total_pred))\n",
    "print(f\"accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node1</th>\n",
       "      <th>node2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24652</th>\n",
       "      <td>19437</td>\n",
       "      <td>14328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21635</th>\n",
       "      <td>20872</td>\n",
       "      <td>12630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6826</th>\n",
       "      <td>1252</td>\n",
       "      <td>23550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22704</th>\n",
       "      <td>2261</td>\n",
       "      <td>1646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20351</th>\n",
       "      <td>23448</td>\n",
       "      <td>21339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       node1  node2\n",
       "24652  19437  14328\n",
       "21635  20872  12630\n",
       "6826    1252  23550\n",
       "22704   2261   1646\n",
       "20351  23448  21339"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## start by using the new ids\n",
    "\n",
    "test_set = pd.read_csv(test_set_path, sep =\" \", header = None)\n",
    "test_set.columns = ['node1','node2']\n",
    "\n",
    "test_set = (test_set\n",
    "    .merge(information_df[['ID','new_ID']], how = 'left', left_on = ['node1'], right_on = ['ID'])\n",
    "    .drop(columns = ['node1','ID'])\n",
    "    .rename(columns = {'new_ID':'node1'})\n",
    "    .merge(information_df[['ID','new_ID']], how = 'left', left_on = ['node2'], right_on = ['ID'])\n",
    "    .drop(columns = ['node2','ID'])\n",
    "    .rename(columns = {'new_ID':'node2'})\n",
    ")\n",
    "test_set.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common_journal\n",
      "computing common authors\n",
      "computing common words\n",
      "computing delta publication year\n",
      "computing jacard index\n",
      "computing preferential attachement\n",
      "computing adamic_adar\n",
      "are connected\n",
      "page ranks\n",
      "compute degree\n"
     ]
    }
   ],
   "source": [
    "test_set_with_features = compute_non_embeddings_features(test_set, information_df, G_articles_embedding)\n",
    "test_set_with_features = compute_embedding_cosines_features(test_set_with_features,\n",
    "                                         articles_node2vec_embeddings,\n",
    "                                         walklets_articles_embeddings,\n",
    "                                         articles_authors_embedding, \n",
    "                                         abstracts_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_with_features = test_set_with_features[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16801"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## scale data\n",
    "test_samples_scaled = scaler.transform(np.float32(test_set_with_features))\n",
    "# prediction\n",
    "test_predicted_labels_svc = clf_svc.predict(test_samples_scaled)\n",
    "test_predicted_labels_xgb = clf_xgb.predict(test_samples_scaled)\n",
    "test_predicted_labels_xt = clf_xt.predict(test_samples_scaled)\n",
    "\n",
    "# voting \n",
    "total_pred_test = np.array(test_predicted_labels_svc)+np.array(test_predicted_labels_xt)+np.array(test_predicted_labels_xgb)\n",
    "total_pred_test = (total_pred_test > 1.5)\n",
    "\n",
    "sum(total_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['category'] = total_pred_test\n",
    "\n",
    "test_set = (test_set\n",
    ".reset_index()\n",
    ".rename(columns = {'index':'id'})\n",
    ".drop(columns = ['node1','node2'])\n",
    ")\n",
    "\n",
    "test_set.to_csv('final_predictions_no_emb.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6657d8cf73e4192045730bbde1f7a947d4725a27f96025bfcb1ab47bc67665b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
