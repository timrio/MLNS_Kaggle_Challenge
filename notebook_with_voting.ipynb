{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: OMP_NUM_THREADS=None =>\n",
      "... If you are using openblas if you are using openblas set OMP_NUM_THREADS=1 or risk subprocess calls hanging indefinitely\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\33631\\AppData\\Local\\Temp\\ipykernel_2448\\2283238867.py:22: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
      "  from scipy.stats.stats import pearsonr\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import os\n",
    "import networkx as nx\n",
    "from gensim.models import Word2Vec\n",
    "from node2vec import Node2Vec\n",
    "import pickle\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from hpsklearn import HyperoptEstimator, any_classifier, any_preprocessing\n",
    "from hyperopt import tpe\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score \n",
    "from sklearn.preprocessing import LabelBinarizer \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from namematcher import NameMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usefull stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_path = \"Data/raw_data/node_information.csv\"\n",
    "test_set_path = \"Data/raw_data/testing_set.txt\"\n",
    "train_set_path = \"Data/raw_data/training_set.txt\"\n",
    "random_preds_path = \"Data/raw_data/random_predictions.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_embedding_subgraph(graph,embedding_ratio = 0.3):\n",
    "    \n",
    "    # --- Step 1: Generate positive edge samples for testing set ---\n",
    "    residual_g = graph.copy()\n",
    "    embedding_samples = []\n",
    "      \n",
    "    # Store the shuffled list of current edges of the graph\n",
    "    edges = list(residual_g.edges())\n",
    "    np.random.shuffle(edges)\n",
    "    \n",
    "    # Define number of positive test samples desired\n",
    "    embedding_set_size = int(embedding_ratio * graph.number_of_edges())\n",
    "    num_of_embedding_samples = 0\n",
    "    \n",
    "    # Remove random edges from the graph, leaving it connected\n",
    "    # Fill in the blanks\n",
    "    for edge in edges:\n",
    "        if graph.degree[edge[0]]<=1 or graph.degree[edge[1]]<=1:\n",
    "            continue\n",
    "        else:\n",
    "            # Remove the edge\n",
    "            residual_g.remove_edge(edge[0], edge[1])\n",
    "            num_of_embedding_samples += 1\n",
    "            embedding_samples.append(edge)\n",
    "        \n",
    "        # If we have collected enough number of edges for testing set, we can terminate the loop\n",
    "        if num_of_embedding_samples == embedding_set_size:\n",
    "            break\n",
    "\n",
    "    train_samples = list(residual_g.edges())\n",
    "            \n",
    "    \n",
    "    return embedding_samples,train_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(a,b):\n",
    "    return(a.dot(b) / (np.linalg.norm(a) * np.linalg.norm(b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>pub_year</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>new_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>5090</td>\n",
       "      <td>2000</td>\n",
       "      <td>e8 gauge theory and a derivation of k-theory f...</td>\n",
       "      <td>Duiliu-Emanuel Diaconescu, Gregory Moore, , Ed...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the partition function of ramond-ramond p-form...</td>\n",
       "      <td>1044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23751</th>\n",
       "      <td>9807052</td>\n",
       "      <td>1998</td>\n",
       "      <td>spinning particles on spacelike hypersurfaces ...</td>\n",
       "      <td>F.Bigazzi (Milano Univ.), L.Lusanna (Firenze U...</td>\n",
       "      <td>Int.J.Mod.Phys.</td>\n",
       "      <td>description a new spinning particle with a def...</td>\n",
       "      <td>23751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14208</th>\n",
       "      <td>9407138</td>\n",
       "      <td>1994</td>\n",
       "      <td>drinfel'd algebra deformations homotopy comodu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the aim of this work is to construct a cohomol...</td>\n",
       "      <td>14208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  pub_year                                              title  \\\n",
       "1044      5090      2000  e8 gauge theory and a derivation of k-theory f...   \n",
       "23751  9807052      1998  spinning particles on spacelike hypersurfaces ...   \n",
       "14208  9407138      1994  drinfel'd algebra deformations homotopy comodu...   \n",
       "\n",
       "                                                 authors     journal_name  \\\n",
       "1044   Duiliu-Emanuel Diaconescu, Gregory Moore, , Ed...              NaN   \n",
       "23751  F.Bigazzi (Milano Univ.), L.Lusanna (Firenze U...  Int.J.Mod.Phys.   \n",
       "14208                                                NaN              NaN   \n",
       "\n",
       "                                                abstract  new_ID  \n",
       "1044   the partition function of ramond-ramond p-form...    1044  \n",
       "23751  description a new spinning particle with a def...   23751  \n",
       "14208  the aim of this work is to construct a cohomol...   14208  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_df = pd.read_csv(information_path, header=None)\n",
    "information_df.columns = [\"ID\",'pub_year','title','authors','journal_name','abstract']\n",
    "### !!!! We have to use new index starting from 0 because of the implementation of karate-club library\n",
    "information_df = information_df.assign(new_ID = [i for i in range(information_df.shape[0])])\n",
    "information_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>node1</th>\n",
       "      <th>node2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>212643</th>\n",
       "      <td>0</td>\n",
       "      <td>9124</td>\n",
       "      <td>12759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430319</th>\n",
       "      <td>1</td>\n",
       "      <td>15292</td>\n",
       "      <td>13272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4208</th>\n",
       "      <td>0</td>\n",
       "      <td>17562</td>\n",
       "      <td>2299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11933</th>\n",
       "      <td>1</td>\n",
       "      <td>13067</td>\n",
       "      <td>11295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47650</th>\n",
       "      <td>0</td>\n",
       "      <td>5450</td>\n",
       "      <td>23844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label  node1  node2\n",
       "212643      0   9124  12759\n",
       "430319      1  15292  13272\n",
       "4208        0  17562   2299\n",
       "11933       1  13067  11295\n",
       "47650       0   5450  23844"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_train_set = pd.read_csv(train_set_path, sep =\" \", header = None)\n",
    "initial_train_set.columns = ['node1','node2','label']\n",
    "### !!! we will use the new indices!!! (see information_df for correspondances)\n",
    "initial_train_set = (initial_train_set\n",
    "    .merge(information_df[['ID','new_ID']], how = 'left', left_on = ['node1'], right_on = ['ID'])\n",
    "    .drop(columns = ['node1','ID'])\n",
    "    .rename(columns = {'new_ID':'node1'})\n",
    "    .merge(information_df[['ID','new_ID']], how = 'left', left_on = ['node2'], right_on = ['ID'])\n",
    "    .drop(columns = ['node2','ID'])\n",
    "    .rename(columns = {'new_ID':'node2'})\n",
    ")\n",
    "initial_train_set.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_ratio = 0.7\n",
    "train_validation_ratio = 0.05\n",
    "\n",
    "if os.path.isfile(\"Data/processed_data/train_set.csv\") and  os.path.isfile(\"Data/processed_data/validation_set.csv\") and  os.path.isfile(\"Data/processed_data/embedding_set.csv\"):\n",
    "    # load sets\n",
    "    train_set = pd.read_csv(\"Data/processed_data/train_set.csv\",)\n",
    "    validation_set = pd.read_csv(\"Data/processed_data/validation_set.csv\")\n",
    "    embedding_set = pd.read_csv(\"Data/processed_data/embedding_set.csv\")\n",
    "else:\n",
    "    all_nodes = list(information_df.new_ID.unique())\n",
    "    all_edges = set(initial_train_set.query('label==1').apply(lambda x: (x.node1,x.node2), axis = 1))\n",
    "\n",
    "    # create subset for embeddings\n",
    "    nodes = list(information_df.new_ID.unique())\n",
    "    edges = set(initial_train_set.query('label==1').apply(lambda x: (x.node1,x.node2), axis = 1))\n",
    "    negative_edges = set(initial_train_set.query('label==0').apply(lambda x: (x.node1,x.node2), axis = 1))\n",
    "    G_total = nx.Graph()\n",
    "    G_total.add_nodes_from(nodes)\n",
    "    G_total.add_edges_from(edges)\n",
    "\n",
    "    embeddings_samples, train_samples = split_embedding_subgraph(G_total,embedding_ratio = embeddings_ratio)\n",
    "\n",
    "    embedding_set = pd.DataFrame([[a,b,1] for (a,b) in embeddings_samples], columns = ['node1','node2','label'])\n",
    "    \n",
    "    train_set = pd.DataFrame([[a,b,1] for (a,b) in train_samples], columns = ['node1','node2','label'])\n",
    "    train_set = pd.concat([train_set, initial_train_set.query('label==0').sample(train_set.shape[0])],axis = 0)\n",
    "\n",
    "    # split train set into train and validation set\n",
    "    X = train_set.drop(columns = ['label'])\n",
    "    y = train_set[['label']]\n",
    "\n",
    "    train_samples, validation_samples, train_labels, validation_labels = train_test_split(X,y, test_size=train_validation_ratio, random_state=0, shuffle=True, stratify=y)\n",
    "    \n",
    "    train_set = pd.concat([train_samples, train_labels], axis = 1)\n",
    "    validation_set = pd.concat([validation_samples, validation_labels], axis = 1)\n",
    "\n",
    "    # save sets\n",
    "    train_set.to_csv(\"Data/processed_data/train_set.csv\", index = False)\n",
    "    validation_set.to_csv(\"Data/processed_data/validation_set.csv\", index = False)\n",
    "    embedding_set.to_csv(\"Data/processed_data/embedding_set.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We also need to make an \"embedding graph\" to compute the various features (otherwise we would train a model to predict edges on a graph in which the edges are present)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information pre_processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                 0\n",
       "pub_year           0\n",
       "title              0\n",
       "authors         4033\n",
       "journal_name    7472\n",
       "abstract           0\n",
       "new_ID             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_df = information_df.fillna({'authors':'', 'journal_name':''})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_df.authors = information_df.authors.apply(lambda x:x.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\"Data/processed_data/information.csv\"):\n",
    "    information_df = pickle.load(open(\"Data/processed_data/information.csv\",'rb'))\n",
    "else:\n",
    "    information_df['title_lemma'] = information_df.title.apply(lambda x: [token.lemma_ for token in spacy_nlp(x) if not token.is_punct if not token.is_digit if not token.is_stop])\n",
    "    pickle.dump(information_df, open(\"Data/processed_data/information.csv\",'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Articles based graph (based on embeddings graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of nodes: 27770\n",
      "The number of edges: 234282\n"
     ]
    }
   ],
   "source": [
    "nodes = list(information_df.new_ID.unique())\n",
    "edges = set(embedding_set.apply(lambda x: (x.node1,x.node2), axis = 1))\n",
    "\n",
    "\n",
    "G_articles_embedding = nx.Graph()\n",
    "G_articles_embedding.add_nodes_from(nodes)\n",
    "G_articles_embedding.add_edges_from(edges)\n",
    "\n",
    "print(\"The number of nodes: {}\".format(G_articles_embedding .number_of_nodes()))\n",
    "print(\"The number of edges: {}\".format(G_articles_embedding .number_of_edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors co-authorship based graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# convert to lower case, remove punctuation, strip the names\n",
    "authors_raw_set = set([auth.strip().lower().translate(str.maketrans('', '', string.punctuation)) for list_auth in information_df.authors for auth in list_auth if len(auth)>1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name matching: to make identify people name by different ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from namematcher import NameMatcher\n",
    "name_matcher = NameMatcher()\n",
    "\n",
    "def compute_unique_names(authors_raw_set):\n",
    "    \"\"\"\n",
    "    one author can be named differently on different papers\n",
    "    this function aims at finding a 'representant' (longest name that describe an author) for each \n",
    "    author\n",
    "    inputs:\n",
    "        - authors_raw_set: set of previously extracted author names\n",
    "    outputs:\n",
    "        - dict: keys are the name in authors_raw_set and the values are the representant\n",
    "    \"\"\"\n",
    "    representant_dict = {}\n",
    "    attributed_nodes = [] # names that already have a representant\n",
    "    for name in tqdm(authors_raw_set, position = 0):\n",
    "        sim_list = [] # similar names \n",
    "        if name not in attributed_nodes:\n",
    "            for name2 in authors_raw_set:\n",
    "                try:\n",
    "                    if name != name2 and name[0]==name2[0] and name2 not in attributed_nodes:\n",
    "                        # two names need to start by the same letter to be consider as potential equivalents\n",
    "                        score = name_matcher.match_names(name, name2)\n",
    "                        if score > 0.9: # if names are close enough\n",
    "                            sim_list.append(name2)\n",
    "                except:\n",
    "                    continue\n",
    "            sim_list.append(name) # the representant is in this list\n",
    "            attributed_nodes.extend(sim_list) # we have fund a representant for those names\n",
    "            representant = max(sim_list, key=len) # the representant is the longest name\n",
    "            for name in sim_list: # all those names have the same representant\n",
    "                representant_dict[name] = representant\n",
    "    return(representant_dict)\n",
    "\n",
    "if os.path.isfile('Data/processed_data/representant_dict.pkl'):\n",
    "    representant_dict = pickle.load(open('Data/processed_data/representant_dict.pkl','rb'))\n",
    "else:\n",
    "    representant_dict = compute_unique_names(authors_raw_set)\n",
    "    pickle.dump(representant_dict, open('Data/processed_data/representant_dict.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set each name to its representant value\n",
    "information_df.authors = information_df.authors.apply(lambda x: [representant_dict[auth.strip().lower().translate(str.maketrans('', '', string.punctuation))] for auth in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a unique index for each author\n",
    "representants_list = list(set(representant_dict.values()))\n",
    "authors2idx = {k: v for v, k in enumerate(representants_list)}\n",
    "\n",
    "information_df[\"authors_id\"] = information_df.authors.apply(lambda x: [authors2idx[auth] for auth in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>pub_year</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>new_ID</th>\n",
       "      <th>title_lemma</th>\n",
       "      <th>authors_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17089</th>\n",
       "      <td>9511163</td>\n",
       "      <td>1995</td>\n",
       "      <td>colliding plane waves in einstein-maxwell-dila...</td>\n",
       "      <td>[nora breton, tonatiuh matos, alberto garcia d...</td>\n",
       "      <td>Phys.Rev.</td>\n",
       "      <td>cinvestav mexico within the metric structure e...</td>\n",
       "      <td>17089</td>\n",
       "      <td>[collide, plane, wave, einstein, maxwell, dila...</td>\n",
       "      <td>[11849, 9435, 10351, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18519</th>\n",
       "      <td>9607036</td>\n",
       "      <td>1996</td>\n",
       "      <td>fusion of twisted representations</td>\n",
       "      <td>[]</td>\n",
       "      <td>Int.J.Mod.Phys.</td>\n",
       "      <td>the comultiplication formula for fusion produc...</td>\n",
       "      <td>18519</td>\n",
       "      <td>[fusion, twisted, representation]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  pub_year                                              title  \\\n",
       "17089  9511163      1995  colliding plane waves in einstein-maxwell-dila...   \n",
       "18519  9607036      1996                  fusion of twisted representations   \n",
       "\n",
       "                                                 authors     journal_name  \\\n",
       "17089  [nora breton, tonatiuh matos, alberto garcia d...        Phys.Rev.   \n",
       "18519                                                 []  Int.J.Mod.Phys.   \n",
       "\n",
       "                                                abstract  new_ID  \\\n",
       "17089  cinvestav mexico within the metric structure e...   17089   \n",
       "18519  the comultiplication formula for fusion produc...   18519   \n",
       "\n",
       "                                             title_lemma  \\\n",
       "17089  [collide, plane, wave, einstein, maxwell, dila...   \n",
       "18519                  [fusion, twisted, representation]   \n",
       "\n",
       "                    authors_id  \n",
       "17089  [11849, 9435, 10351, 0]  \n",
       "18519                      [0]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute nodes and edges\n",
    "pre_edges = list(information_df.authors_id.apply(lambda x : [(x[i],x[j]) for i in range(len(x)) for j in range(len(x)) if i>j]))\n",
    "authors_edges = [edge for list_edge in pre_edges for edge in list_edge]\n",
    "authors_edges_dict = Counter(authors_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of nodes: 14447\n",
      "The number of edges: 29111\n"
     ]
    }
   ],
   "source": [
    "G_authors = nx.Graph()\n",
    "G_authors.add_nodes_from(authors2idx.values())\n",
    "G_authors.add_weighted_edges_from([(a,b,weight) for (a,b),weight in authors_edges_dict.items()])\n",
    "\n",
    "print(\"The number of nodes: {}\".format(G_authors.number_of_nodes()))\n",
    "print(\"The number of edges: {}\".format(G_authors.number_of_edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various embeddings computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Based embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using articles graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Walklets\n",
    "from karateclub import Walklets\n",
    "if os.path.isfile('Data/processed_data/articles_walklets_embeddings.pkl'):\n",
    "    walklets_articles_embeddings = pickle.load(open('Data/processed_data/articles_walklets_embeddings.pkl','rb'))\n",
    "else:\n",
    "    walklets = Walklets() # we leave the defaults parameters for the other values\n",
    "    walklets.fit(G_articles_embedding)\n",
    "    walklets_articles_embeddings = walklets.get_embedding()\n",
    "    pickle.dump(walklets_articles_embeddings, open('Data/processed_data/articles_walklets_embeddings.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Node2Vec\n",
    "from karateclub import Node2Vec\n",
    "if os.path.isfile('Data/processed_data/articles_node2vec_embeddings.pkl'):\n",
    "    articles_node2vec_embeddings = pickle.load(open('Data/processed_data/articles_node2vec_embeddings.pkl','rb'))\n",
    "else:\n",
    "    node2vec = Node2Vec() # we leave the defaults parameters for the other values\n",
    "    node2vec.fit(G_articles_embedding)\n",
    "    articles_node2vec_embeddings = node2vec.get_embedding()\n",
    "    pickle.dump(articles_node2vec_embeddings, open('Data/processed_data/articles_node2vec_embeddings.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### deepwalk\n",
    "from karateclub import DeepWalk\n",
    "if os.path.isfile('Data/processed_data/articles_DeepWalk_embeddings.pkl'):\n",
    "    articles_DeepWalk_embeddings = pickle.load(open('Data/processed_data/articles_DeepWalk_embeddings.pkl','rb'))\n",
    "else:\n",
    "    DeepWalk = DeepWalk() # we leave the defaults parameters for the other values\n",
    "    DeepWalk.fit(G_articles_embedding)\n",
    "    articles_DeepWalk_embeddings = DeepWalk.get_embedding()\n",
    "    pickle.dump(articles_DeepWalk_embeddings, open('Data/processed_data/articles_DeepWalk_embeddings.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using author graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### authors embedding leveraging author graphs (random walk for instance)\n",
    "\n",
    "### Node2Vec\n",
    "if os.path.isfile('Data/processed_data/authors_node2vec_embeddings.pkl'):\n",
    "    authors_node2vec_embeddings = pickle.load(open('Data/processed_data/authors_node2vec_embeddings.pkl', 'rb'))\n",
    "else:\n",
    "    node2vec_authors = Node2Vec(walk_length=15)\n",
    "    node2vec_authors.fit(G_authors)\n",
    "    authors_node2vec_embeddings = node2vec_authors.get_embedding()\n",
    "    pickle.dump(authors_node2vec_embeddings, open('Data/processed_data/authors_node2vec_embeddings.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>pub_year</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>new_ID</th>\n",
       "      <th>title_lemma</th>\n",
       "      <th>authors_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12131</th>\n",
       "      <td>9306080</td>\n",
       "      <td>1993</td>\n",
       "      <td>topological sigma-models in four dimensions an...</td>\n",
       "      <td>[damiano anselmi, pietro fre]</td>\n",
       "      <td>Nucl.Phys.</td>\n",
       "      <td>it is well-known that topological sigma-models...</td>\n",
       "      <td>12131</td>\n",
       "      <td>[topological, sigma, model, dimension, triholo...</td>\n",
       "      <td>[10381, 10529]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23513</th>\n",
       "      <td>9806054</td>\n",
       "      <td>1998</td>\n",
       "      <td>variational principle and a perturbative solut...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Nucl.Phys.</td>\n",
       "      <td>equations in curved space string dynamics in a...</td>\n",
       "      <td>23513</td>\n",
       "      <td>[variational, principle, perturbative, solutio...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  pub_year                                              title  \\\n",
       "12131  9306080      1993  topological sigma-models in four dimensions an...   \n",
       "23513  9806054      1998  variational principle and a perturbative solut...   \n",
       "\n",
       "                             authors journal_name  \\\n",
       "12131  [damiano anselmi, pietro fre]   Nucl.Phys.   \n",
       "23513                             []   Nucl.Phys.   \n",
       "\n",
       "                                                abstract  new_ID  \\\n",
       "12131  it is well-known that topological sigma-models...   12131   \n",
       "23513  equations in curved space string dynamics in a...   23513   \n",
       "\n",
       "                                             title_lemma      authors_id  \n",
       "12131  [topological, sigma, model, dimension, triholo...  [10381, 10529]  \n",
       "23513  [variational, principle, perturbative, solutio...             [0]  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each article take a mean of the authors embedding as global autors embedding\n",
    "articles_authors_embedding = []\n",
    "for i in range(information_df.shape[0]):\n",
    "    value = information_df[information_df.new_ID == i]\n",
    "    authors_id = value.authors_id\n",
    "    embeddings = np.array([0 for i in range(128)]).astype('float64')\n",
    "    for author in authors_id:\n",
    "        embeddings+=authors_node2vec_embeddings[author][0]\n",
    "    articles_authors_embedding.append(embeddings/len(authors_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node based embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute abstracts embeddings using specter network\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = 'gpu'\n",
    "\n",
    "if os.path.isfile('Data/processed_data/abstracts_embeddings.pkl'):\n",
    "    abstracts_embeddings = pickle.load(open('Data/processed_data/abstracts_embeddings.pkl','rb'))\n",
    "else:\n",
    "    \n",
    "    # load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('allenai/specter')\n",
    "    model = AutoModel.from_pretrained('allenai/specter').to(device)\n",
    "    model.eval()\n",
    "    abstracts_embeddings = []\n",
    "    for i in tqdm(range(information_df.shape[0]), position = 0):\n",
    "        article = information_df.loc[i]\n",
    "        title = article.title\n",
    "        abstract = article.abstract\n",
    "        paper = [{'title':title, 'abstract':abstract}]\n",
    "\n",
    "        # concatenate title and abstract\n",
    "        title_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in paper]\n",
    "        # preprocess the input\n",
    "        inputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "        inputs = inputs.to(device)\n",
    "        result = model(**inputs)\n",
    "        # take the first token in the batch as the embedding\n",
    "        embedding = result.last_hidden_state[:, 0, :].detach().cpu().numpy()\n",
    "        abstracts_embeddings.append(embedding)\n",
    "    pickle.dump(abstracts_embeddings, open('Data/processed_data/abstracts_embeddings.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Class.Quant.Grav.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(information_df[information_df.new_ID==1].journal_name.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute page rank\n",
    "page_rank_dict = nx.pagerank(G_articles_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compute degree centrality\n",
    "centrality_dict = nx.degree_centrality(G_articles_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute edges features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(graph, edge):\n",
    "\n",
    "    inter_size = len(list(nx.common_neighbors(graph, edge[0], edge[1])))\n",
    "    union_size = len(set(graph[edge[0]]) | set(graph[edge[1]]))\n",
    "    try:\n",
    "        jacard = inter_size / union_size\n",
    "    except:\n",
    "        jacard = np.nan\n",
    "\n",
    "    return jacard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdamicAdar(graph, edge):\n",
    "\n",
    "    inter_list = nx.common_neighbors(graph, edge[0], edge[1])\n",
    "    try:\n",
    "        adamic_adar =  sum( [1/np.log(graph.degree(node)) for node in inter_list])\n",
    "    except:\n",
    "        adamic_adar = np.nan\n",
    "    \n",
    "    return adamic_adar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preferential_attachement(graph, edge):\n",
    "    pa = graph.degree(edge[0]) * graph.degree(edge[1])\n",
    "    return pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_connected(graph,edge):\n",
    "    try:\n",
    "        connect = nx.shortest_path(graph, source=edge[0], target=edge[1])\n",
    "        connected  = 1\n",
    "    except:\n",
    "        connected = 0\n",
    "    return(connected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_path(graph,edge):\n",
    "    try:\n",
    "        l = nx.shortest_path_length(graph, source=edge[0], target=edge[1])\n",
    "    except:\n",
    "        l = -1\n",
    "    return(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_journal(information_df, node1, node2):\n",
    "    journal1 = str(information_df[information_df.new_ID==node1].journal_name.values[0])\n",
    "    journal2 = str(information_df[information_df.new_ID==node2].journal_name.values[0])\n",
    "\n",
    "    if journal1 == '' or journal2 == '':\n",
    "        return(-1)\n",
    "    elif journal1 == journal2:\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_non_embeddings_features(df, information_df, G_articles):\n",
    "    useful_information_df = information_df[['new_ID','authors','pub_year', 'title_lemma']]\n",
    "\n",
    "    # prepare data frame for common authors computation\n",
    "    df = (df\n",
    "    .merge(useful_information_df, how ='left', left_on = ['node1'], right_on = ['new_ID'])\n",
    "    .rename(columns = {'authors':'authors_node_1', 'pub_year':'pub_year1', 'title_lemma':'title_lemma1'})\n",
    "    .merge(useful_information_df, how ='left', left_on = ['node2'], right_on = ['new_ID'])\n",
    "    .rename(columns = {'authors':'authors_node_2', 'pub_year':'pub_year2', 'title_lemma':'title_lemma2'})\n",
    "    )\n",
    "\n",
    "    print(\"common_journal\")\n",
    "    df['common_journals'] = df.apply(lambda x: common_journal(information_df, x.node1, x.node2),axis = 1)\n",
    "    \n",
    "    print('computing common authors')\n",
    "    #  compute common authors\n",
    "    df['common_authors'] = df.apply(lambda x:len(set(x.authors_node_1)&set(x.authors_node_2)),axis = 1)\n",
    "\n",
    "    print('computing common words')\n",
    "    #  compute common words in titles\n",
    "    df['common_title_words'] = df.apply(lambda x:len(set(x.title_lemma1)&set(x.title_lemma2)),axis = 1)\n",
    "\n",
    "    print('computing delta publication year')\n",
    "    # compute delta publication year\n",
    "    df['delta_publication'] = df.apply(lambda x:np.abs(x.pub_year2 - x.pub_year1),axis = 1)\n",
    "\n",
    "    # compute edges features\n",
    "    print('computing jacard index')\n",
    "    df['jacard'] = df.apply(lambda x: Jaccard(G_articles, (x.node1, x.node2)),axis = 1)\n",
    "\n",
    "    print('computing preferential attachement')\n",
    "    df['pa'] = df.apply(lambda x: preferential_attachement(G_articles, (x.node1, x.node2)),axis = 1)\n",
    "\n",
    "    print('computing adamic_adar')\n",
    "    df['adamic_adar'] = df.apply(lambda x: AdamicAdar(G_articles, (x.node1, x.node2)),axis = 1)\n",
    "\n",
    "    print('are connected')\n",
    "    df['connection'] = df.apply(lambda x: are_connected(G_articles, (x.node1, x.node2)),axis = 1)\n",
    "\n",
    "    print('shortest_path')\n",
    "    df['shortest_path'] = df.apply(lambda x: shortest_path(G_articles, (x.node1, x.node2)),axis = 1)\n",
    "\n",
    "    print('page ranks')\n",
    "    df['page_rank1'] = df.apply(lambda x: page_rank_dict[x.node1],axis = 1)\n",
    "    df['page_rank2'] = df.apply(lambda x: page_rank_dict[x.node2],axis = 1)\n",
    "    \n",
    "    print('compute degree')\n",
    "\n",
    "    df['degree1'] = df.apply(lambda x: centrality_dict[x.node1],axis = 1)\n",
    "    df['degree2'] = df.apply(lambda x: centrality_dict[x.node2],axis = 1)\n",
    "\n",
    "    \n",
    "    df = df.fillna({ 'jacard':df.jacard.mean(),\n",
    "                     'adamic_adar':df.adamic_adar.mean()\n",
    "                     })\n",
    "\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding_cosines_features(df,\n",
    "                                         articles_node2vec_embeddings,\n",
    "                                         walklets_articles_embeddings,\n",
    "                                         articles_authors_embedding, \n",
    "                                         abstracts_embeddings):\n",
    "    # compute some cosine based distances\n",
    "    df['articles_node2vec_cosine'] = df.apply(lambda x:cosine(articles_node2vec_embeddings[x.node1],articles_node2vec_embeddings[x.node2]), axis = 1)\n",
    "    df['articles_walklets_cosine'] = df.apply(lambda x:cosine(walklets_articles_embeddings[x.node1],walklets_articles_embeddings[x.node2]), axis = 1)\n",
    "    df['authors_embeddings_cosine'] = df.apply(lambda x:cosine(articles_authors_embedding[x.node1],articles_authors_embedding[x.node2]), axis = 1)\n",
    "    df['abstracts_embeddings_cosine'] = df.apply(lambda x:cosine(abstracts_embeddings[x.node1][0],abstracts_embeddings[x.node2][0]), axis = 1)\n",
    "    df['articles_deepwalk_cosine'] = df.apply(lambda x:cosine(articles_DeepWalk_embeddings[x.node1],articles_DeepWalk_embeddings[x.node2]), axis = 1)\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute features on train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\"Data/processed_data/train_set_with_features.csv\"):\n",
    "    train_set_with_features = pd.read_csv(\"Data/processed_data/train_set_with_features.csv\")\n",
    "else:\n",
    "    train_set_with_features = compute_non_embeddings_features(initial_train_set, information_df, G_articles_embedding)\n",
    "    train_set_with_features = compute_embedding_cosines_features(train_set_with_features,\n",
    "                                            articles_node2vec_embeddings,\n",
    "                                            walklets_articles_embeddings,\n",
    "                                            articles_authors_embedding, \n",
    "                                            abstracts_embeddings)\n",
    "    train_set_with_features.to_csv(\"Data/processed_data/train_set_with_features.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep= ['common_authors', 'common_title_words','common_journals',\n",
    "       'delta_publication', 'jacard', 'pa', 'adamic_adar', 'connection',\n",
    "       'shortest_path', 'page_rank1', 'page_rank2', 'degree1', 'degree2',\n",
    "       'articles_node2vec_cosine', 'articles_walklets_cosine',\n",
    "       'authors_embeddings_cosine', 'abstracts_embeddings_cosine','articles_deepwalk_cosine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_with_features = train_set_with_features[columns_to_keep+['label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute features on validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common_journal\n",
      "computing common authors\n",
      "computing common words\n",
      "computing delta publication year\n",
      "computing jacard index\n",
      "computing preferential attachement\n",
      "computing adamic_adar\n",
      "are connected\n",
      "shortest_path\n",
      "page ranks\n",
      "compute degree\n"
     ]
    }
   ],
   "source": [
    "validation_set_with_features = compute_non_embeddings_features(validation_set, information_df, G_articles_embedding)\n",
    "validation_set_with_features = compute_embedding_cosines_features(validation_set_with_features,\n",
    "                                         articles_node2vec_embeddings,\n",
    "                                         walklets_articles_embeddings,\n",
    "                                         articles_authors_embedding, \n",
    "                                         abstracts_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set_with_features = validation_set_with_features[columns_to_keep+['label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples, train_labels = train_set_with_features.drop(columns = ['label']), train_set_with_features[['label']]\n",
    "validation_samples, validation_labels = validation_set_with_features.drop(columns = ['label']), validation_set_with_features[['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "train_samples_scaled = scaler.fit_transform(np.float32(train_samples))\n",
    "validation_samples_scaled = scaler.transform(np.float32(validation_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nestim = HyperoptEstimator(classifier=any_classifier('my_clf'),\\n                          preprocessing=any_preprocessing('my_pre'),\\n                          algo=tpe.suggest,\\n                          max_evals=100,\\n                          trial_timeout=120)\\n\\nestim.fit(train_samples_scaled,list(train_labels.label))\\n\\n# Show the results\\n\\nprint(estim.score(validation_samples_scaled, list(validation_labels.label)))\\n\\nprint(estim.best_model())\\n\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### use hyper opt to find the best model\n",
    "'''\n",
    "estim = HyperoptEstimator(classifier=any_classifier('my_clf'),\n",
    "                          preprocessing=any_preprocessing('my_pre'),\n",
    "                          algo=tpe.suggest,\n",
    "                          max_evals=100,\n",
    "                          trial_timeout=120)\n",
    "\n",
    "estim.fit(train_samples_scaled,list(train_labels.label))\n",
    "\n",
    "# Show the results\n",
    "\n",
    "print(estim.score(validation_samples_scaled, list(validation_labels.label)))\n",
    "\n",
    "print(estim.best_model())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best :{'learner': ExtraTreesClassifier(bootstrap=True, max_features=0.7327186439657982,\n",
    "                     n_estimators=95, n_jobs=1, random_state=3, verbose=False), 'preprocs': (), 'ex_preprocs': ()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9658400557713375\n"
     ]
    }
   ],
   "source": [
    "# train a model \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf_xt = ExtraTreesClassifier(bootstrap=True, max_features=0.7327186439657982,\n",
    "                     n_estimators=95, n_jobs=1, random_state=3, verbose=False)\n",
    "\n",
    "clf_xt.fit(train_samples_scaled,list(train_labels.label))\n",
    "\n",
    "# test the model\n",
    "predicted_labels_xt = clf_xt.predict(validation_samples_scaled)\n",
    "acc = accuracy_score(validation_labels, list(predicted_labels_xt))\n",
    "print(f\"accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\33631\\Documents\\Etudes\\centrale 3A\\MLNS\\MLNS_Kaggle_Challenge\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:31:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "accuracy: 0.9524947714371078\n"
     ]
    }
   ],
   "source": [
    "# train a model \n",
    "\n",
    "clf_xgb = XGBClassifier()\n",
    "\n",
    "clf_xgb.fit(train_samples_scaled,list(train_labels.label))\n",
    "\n",
    "# test the model\n",
    "predicted_labels_xgb = clf_xgb.predict(validation_samples_scaled)\n",
    "acc = accuracy_score(validation_labels, list(predicted_labels_xgb))\n",
    "print(f\"accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9522955880888357\n"
     ]
    }
   ],
   "source": [
    "# train a model \n",
    "from sklearn.svm import SVC\n",
    "clf_svc = SVC()\n",
    "\n",
    "clf_svc.fit(train_samples_scaled,list(train_labels.label))\n",
    "\n",
    "# test the model\n",
    "predicted_labels_svc = clf_svc.predict(validation_samples_scaled)\n",
    "acc = accuracy_score(validation_labels, list(predicted_labels_svc))\n",
    "print(f\"accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9566776217508216\n"
     ]
    }
   ],
   "source": [
    "# voting\n",
    "total_pred = np.array(predicted_labels_svc)+np.array(predicted_labels_xt)+np.array(predicted_labels_xgb)\n",
    "total_pred = (total_pred > 1.5)\n",
    "acc = accuracy_score(validation_labels, list(total_pred))\n",
    "print(f\"accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node1</th>\n",
       "      <th>node2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>20963</td>\n",
       "      <td>11436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24155</th>\n",
       "      <td>9897</td>\n",
       "      <td>7632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11904</th>\n",
       "      <td>8270</td>\n",
       "      <td>27533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6960</th>\n",
       "      <td>5981</td>\n",
       "      <td>1789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29862</th>\n",
       "      <td>23888</td>\n",
       "      <td>6702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       node1  node2\n",
       "395    20963  11436\n",
       "24155   9897   7632\n",
       "11904   8270  27533\n",
       "6960    5981   1789\n",
       "29862  23888   6702"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## start by using the new ids\n",
    "\n",
    "test_set = pd.read_csv(test_set_path, sep =\" \", header = None)\n",
    "test_set.columns = ['node1','node2']\n",
    "\n",
    "test_set = (test_set\n",
    "    .merge(information_df[['ID','new_ID']], how = 'left', left_on = ['node1'], right_on = ['ID'])\n",
    "    .drop(columns = ['node1','ID'])\n",
    "    .rename(columns = {'new_ID':'node1'})\n",
    "    .merge(information_df[['ID','new_ID']], how = 'left', left_on = ['node2'], right_on = ['ID'])\n",
    "    .drop(columns = ['node2','ID'])\n",
    "    .rename(columns = {'new_ID':'node2'})\n",
    ")\n",
    "test_set.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common_journal\n",
      "computing common authors\n",
      "computing common words\n",
      "computing delta publication year\n",
      "computing jacard index\n",
      "computing preferential attachement\n",
      "computing adamic_adar\n",
      "are connected\n",
      "shortest_path\n",
      "page ranks\n",
      "compute degree\n"
     ]
    }
   ],
   "source": [
    "test_set_with_features = compute_non_embeddings_features(test_set, information_df, G_articles_embedding)\n",
    "test_set_with_features = compute_embedding_cosines_features(test_set_with_features,\n",
    "                                         articles_node2vec_embeddings,\n",
    "                                         walklets_articles_embeddings,\n",
    "                                         articles_authors_embedding, \n",
    "                                         abstracts_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_with_features = test_set_with_features[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16859"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## scale data\n",
    "test_samples_scaled = scaler.transform(np.float32(test_set_with_features))\n",
    "# prediction\n",
    "test_predicted_labels_svc = clf_svc.predict(test_samples_scaled)\n",
    "test_predicted_labels_xgb = clf_xgb.predict(test_samples_scaled)\n",
    "test_predicted_labels_xt = clf_xt.predict(test_samples_scaled)\n",
    "\n",
    "# voting \n",
    "total_pred_test = np.array(test_predicted_labels_svc)+np.array(test_predicted_labels_xt)+np.array(test_predicted_labels_xgb)\n",
    "total_pred_test = (total_pred_test > 1.5)\n",
    "\n",
    "sum(total_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32648"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['category'] = total_pred_test\n",
    "\n",
    "test_set = (test_set\n",
    ".reset_index()\n",
    ".rename(columns = {'index':'id'})\n",
    ".drop(columns = ['node1','node2'])\n",
    ")\n",
    "\n",
    "test_set.to_csv('final_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6657d8cf73e4192045730bbde1f7a947d4725a27f96025bfcb1ab47bc67665b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
