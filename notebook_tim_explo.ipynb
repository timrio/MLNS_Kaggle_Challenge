{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\33631\\AppData\\Local\\Temp\\ipykernel_105752\\3408994771.py:16: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
      "  from scipy.stats.stats import pearsonr\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import os\n",
    "import networkx as nx\n",
    "from gensim.models import Word2Vec\n",
    "from node2vec import Node2Vec\n",
    "import pickle\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score \n",
    "from sklearn.preprocessing import LabelBinarizer \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from namematcher import NameMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usefull stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_path = \"Data/raw_data/node_information.csv\"\n",
    "test_set_path = \"Data/raw_data/testing_set.txt\"\n",
    "train_set_path = \"Data/raw_data/training_set.txt\"\n",
    "random_preds_path = \"Data/raw_data/random_predictions.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(graph, train_set_ratio):\n",
    "    \"\"\"\n",
    "    Graph pre-processing step required to perform supervised link prediction\n",
    "    Create training and test sets\n",
    "    \"\"\"    \n",
    "    # --- Step 1: Generate positive edge samples for testing set ---\n",
    "    residual_g = graph.copy()\n",
    "    test_pos_samples = []\n",
    "      \n",
    "    # Store the shuffled list of current edges of the graph\n",
    "    edges = list(residual_g.edges())\n",
    "    np.random.shuffle(edges)\n",
    "    \n",
    "    # Define number of positive test samples desired\n",
    "    test_set_size = int((1.0 - train_set_ratio) * graph.number_of_edges())\n",
    "    train_set_size = graph.number_of_edges() - test_set_size\n",
    "    \n",
    "    # Remove random edges from the graph, leaving it connected\n",
    "    # Fill in the blanks\n",
    "    for i,edge in enumerate(edges[:test_set_size]):\n",
    "        if i%1000==0:\n",
    "            print(i)\n",
    "        \n",
    "        # Remove the edge\n",
    "        residual_g.remove_edge(edge[0], edge[1])\n",
    "        \n",
    "        # Add the removed edge to the positive sample list \n",
    "        test_pos_samples.append(edge)\n",
    "        \n",
    "        \n",
    "    # --- Step 2: Generate positive edge samples for training set ---\n",
    "    # The remaining edges are simply considered for positive samples of the training set\n",
    "    train_pos_samples = list(residual_g.edges())\n",
    "        \n",
    "        \n",
    "    # --- Step 3: Generate the negative samples for testing and training sets ---\n",
    "    # Fill in the blanks\n",
    "\n",
    "    print(\"compute negative samples\")\n",
    "    train_neg_samples = []\n",
    "    test_neg_samples = []\n",
    "\n",
    "    print('train neg samples')\n",
    "    i = 0\n",
    "    while i < train_set_size:\n",
    "        a = np.random.choice(nx.nodes(G),1)[0]\n",
    "        b = np.random.choice(nx.nodes(G),1)[0]\n",
    "        if (a,b) not in edges and (a,b) not in train_neg_samples:\n",
    "            i+=1\n",
    "            train_neg_samples.append((a,b))\n",
    "\n",
    "    print('test neg samples')\n",
    "    j = 0\n",
    "    while j < test_set_size:\n",
    "        a = np.random.choice(nx.nodes(G),1)[0]\n",
    "        b = np.random.choice(nx.nodes(G),1)[0]\n",
    "        if (a,b) not in edges and (a,b) not in test_neg_samples and (a,b) not in train_neg_samples:\n",
    "            j+=1\n",
    "            test_neg_samples.append((a,b))\n",
    "\n",
    "\n",
    "    print(\"done\")\n",
    "    \n",
    "    # --- Step 4: Combine sample lists and create corresponding labels ---\n",
    "    # For training set\n",
    "    print(\"final step\")\n",
    "    train_samples = train_pos_samples + train_neg_samples\n",
    "    train_labels = [1 for _ in train_pos_samples] + [0 for _ in train_neg_samples]\n",
    "    # For testing set\n",
    "    test_samples = test_pos_samples + test_neg_samples\n",
    "    test_labels = [1 for _ in test_pos_samples] + [0 for _ in test_neg_samples]\n",
    "    \n",
    "    return train_samples, train_labels, test_samples, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>pub_year</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal_name</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5501</th>\n",
       "      <td>110225</td>\n",
       "      <td>2001</td>\n",
       "      <td>brane gases on k3 and calabi-yau manifolds</td>\n",
       "      <td>Damien A. Easson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>presentation improved we initiate the study of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21696</th>\n",
       "      <td>9709206</td>\n",
       "      <td>1997</td>\n",
       "      <td>d-brane decay and hawking radiation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nucl.Phys.Proc.Suppl.</td>\n",
       "      <td>at amsterdam june 1997 references added tree l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8738</th>\n",
       "      <td>211044</td>\n",
       "      <td>2002</td>\n",
       "      <td>thick domain walls and charged dilaton black h...</td>\n",
       "      <td>R. Moderski, M. Rogatko</td>\n",
       "      <td>Phys.Rev.</td>\n",
       "      <td>we study a black hole domain wall system in di...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  pub_year                                              title  \\\n",
       "5501    110225      2001         brane gases on k3 and calabi-yau manifolds   \n",
       "21696  9709206      1997                d-brane decay and hawking radiation   \n",
       "8738    211044      2002  thick domain walls and charged dilaton black h...   \n",
       "\n",
       "                       authors           journal_name  \\\n",
       "5501          Damien A. Easson                    NaN   \n",
       "21696                      NaN  Nucl.Phys.Proc.Suppl.   \n",
       "8738   R. Moderski, M. Rogatko              Phys.Rev.   \n",
       "\n",
       "                                                abstract  \n",
       "5501   presentation improved we initiate the study of...  \n",
       "21696  at amsterdam june 1997 references added tree l...  \n",
       "8738   we study a black hole domain wall system in di...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_df = pd.read_csv(information_path, header=None)\n",
    "information_df.columns = [\"ID\",'pub_year','title','authors','journal_name','abstract']\n",
    "information_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node1</th>\n",
       "      <th>node2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>329743</th>\n",
       "      <td>9703080</td>\n",
       "      <td>9209063</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137352</th>\n",
       "      <td>2156</td>\n",
       "      <td>207119</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457904</th>\n",
       "      <td>9411199</td>\n",
       "      <td>9803192</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136809</th>\n",
       "      <td>204058</td>\n",
       "      <td>110108</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416567</th>\n",
       "      <td>304222</td>\n",
       "      <td>9907027</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          node1    node2  label\n",
       "329743  9703080  9209063      0\n",
       "137352     2156   207119      0\n",
       "457904  9411199  9803192      0\n",
       "136809   204058   110108      1\n",
       "416567   304222  9907027      1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_set = pd.read_csv(train_set_path, sep =\" \", header = None)\n",
    "pre_train_set.columns = ['node1','node2','label']\n",
    "pre_train_set.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node1</th>\n",
       "      <th>node2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14955</th>\n",
       "      <td>9606193</td>\n",
       "      <td>9603090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21729</th>\n",
       "      <td>9610084</td>\n",
       "      <td>9503232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5551</th>\n",
       "      <td>9904095</td>\n",
       "      <td>9710230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4419</th>\n",
       "      <td>205273</td>\n",
       "      <td>110140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28753</th>\n",
       "      <td>9911220</td>\n",
       "      <td>9306096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         node1    node2\n",
       "14955  9606193  9603090\n",
       "21729  9610084  9503232\n",
       "5551   9904095  9710230\n",
       "4419    205273   110140\n",
       "28753  9911220  9306096"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = pd.read_csv(test_set_path, sep =\" \", header = None)\n",
    "test_set.columns = ['node1','node2']\n",
    "test_set.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pre_train_set.drop(columns = ['label'])\n",
    "y = pre_train_set[['label']]\n",
    "train_samples, validation_samples, train_labels, validation_labels = train_test_split(X,y, test_size=0.2, random_state=0, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.concat([train_samples, train_labels], axis = 1)\n",
    "validation_set = pd.concat([validation_samples, validation_labels], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information pre_processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                 0\n",
       "pub_year           0\n",
       "title              0\n",
       "authors         4033\n",
       "journal_name    7472\n",
       "abstract           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_df = information_df.fillna({'authors':'', 'journal_name':''})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_df.authors = information_df.authors.apply(lambda x:x.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_df['title_lemma'] = information_df.title.apply(lambda x: [token.lemma_ for token in spacy_nlp(x) if not token.is_punct if not token.is_digit if not token.is_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save information df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(information_df, open(\"Data/processed_data/information.csv\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_df = pickle.load(open(\"Data/processed_data/information.csv\",'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Articles based graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = set(np.concatenate((train_set.node1,train_set.node2), axis = 0))\n",
    "edges = set(train_set.query(\"label == 1\").apply(lambda x: (x.node1,x.node2), axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of nodes: 27770\n",
      "The number of edges: 267828\n"
     ]
    }
   ],
   "source": [
    "G_articles = nx.Graph()\n",
    "G_articles.add_nodes_from(nodes)\n",
    "G_articles.add_edges_from(edges)\n",
    "\n",
    "print(\"The number of nodes: {}\".format(G_articles.number_of_nodes()))\n",
    "print(\"The number of edges: {}\".format(G_articles.number_of_edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors co-authorship based graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# convert to lower case, remove punctuation, strip the names\n",
    "authors_raw_set = set([auth.strip().lower().translate(str.maketrans('', '', string.punctuation)) for list_auth in information_df.authors for auth in list_auth if len(auth)>1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name matching: to make identify people name by different ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16049/16049 [50:28<00:00,  5.30it/s] \n"
     ]
    }
   ],
   "source": [
    "from namematcher import NameMatcher\n",
    "name_matcher = NameMatcher()\n",
    "\n",
    "def compute_unique_names(authors_raw_set):\n",
    "    \"\"\"\n",
    "    one author can be named differently on different papers\n",
    "    this function aims at finding a 'representant' (longest name that describe an author) for each \n",
    "    author\n",
    "    inputs:\n",
    "        - authors_raw_set: set of previously extracted author names\n",
    "    outputs:\n",
    "        - dict: keys are the name in authors_raw_set and the values are the representant\n",
    "    \"\"\"\n",
    "    representant_dict = {}\n",
    "    attributed_nodes = [] # names that already have a representant\n",
    "    for name in tqdm(authors_raw_set, position = 0):\n",
    "        sim_list = [] # similar names \n",
    "        if name not in attributed_nodes:\n",
    "            for name2 in authors_raw_set:\n",
    "                try:\n",
    "                    if name != name2 and name[0]==name2[0] and name2 not in attributed_nodes:\n",
    "                        # two names need to start by the same letter to be consider as potential equivalents\n",
    "                        score = name_matcher.match_names(name, name2)\n",
    "                        if score > 0.9: # if names are close enough\n",
    "                            sim_list.append(name2)\n",
    "                except:\n",
    "                    continue\n",
    "            sim_list.append(name) # the representant is in this list\n",
    "            attributed_nodes.extend(sim_list) # we have fund a representant for those names\n",
    "            representant = max(sim_list, key=len) # the representant is the longest name\n",
    "            for name in sim_list: # all those names have the same representant\n",
    "                representant_dict[name] = representant\n",
    "    return(representant_dict)\n",
    "\n",
    "if os.path.isfile('Data/processed_data/representant_dict.pkl'):\n",
    "    representant_dict = pickle.load(open('Data/processed_data/representant_dict.pkl','rb'))\n",
    "else:\n",
    "    representant_dict = compute_unique_names(authors_raw_set)\n",
    "    pickle.dump(representant_dict, open('Data/processed_data/representant_dict.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set each name to its representant value\n",
    "information_df['author_representant'] = information_df.authors.apply(lambda x: [representant_dict[auth.strip().lower().translate(str.maketrans('', '', string.punctuation))] for auth in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>pub_year</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title_lemma</th>\n",
       "      <th>author_representant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>2000</td>\n",
       "      <td>compactification geometry and duality</td>\n",
       "      <td>[Paul S. Aspinwall]</td>\n",
       "      <td></td>\n",
       "      <td>these are notes based on lectures given at tas...</td>\n",
       "      <td>[compactification, geometry, duality]</td>\n",
       "      <td>[paul s aspinwall]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002</td>\n",
       "      <td>2000</td>\n",
       "      <td>domain walls and massive gauged supergravity p...</td>\n",
       "      <td>[M. Cvetic,  H. Lu,  C.N. Pope]</td>\n",
       "      <td>Class.Quant.Grav.</td>\n",
       "      <td>we point out that massive gauged supergravity ...</td>\n",
       "      <td>[domain, wall, massive, gauge, supergravity, p...</td>\n",
       "      <td>[m cvetivc, h lu, cn pope]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003</td>\n",
       "      <td>2000</td>\n",
       "      <td>comment on metric fluctuations in brane worlds</td>\n",
       "      <td>[Y.S. Myung,  Gungwon Kang]</td>\n",
       "      <td></td>\n",
       "      <td>recently ivanov and volovich hep-th 9912242 cl...</td>\n",
       "      <td>[comment, metric, fluctuation, brane, world]</td>\n",
       "      <td>[ys myung, gungwon kang]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>2000</td>\n",
       "      <td>moving mirrors and thermodynamic paradoxes</td>\n",
       "      <td>[Adam D. Helfer]</td>\n",
       "      <td>Phys.Rev.</td>\n",
       "      <td>quantum fields responding to moving mirrors ha...</td>\n",
       "      <td>[move, mirror, thermodynamic, paradox]</td>\n",
       "      <td>[adam d helfer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>2000</td>\n",
       "      <td>bundles of chiral blocks and boundary conditio...</td>\n",
       "      <td>[J. Fuchs,  C. Schweigert]</td>\n",
       "      <td></td>\n",
       "      <td>proceedings of lie iii clausthal july 1999 var...</td>\n",
       "      <td>[bundle, chiral, block, boundary, condition, cft]</td>\n",
       "      <td>[j fuchs, c schweigert]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27765</th>\n",
       "      <td>9912289</td>\n",
       "      <td>2002</td>\n",
       "      <td>gauge fixing in the chain by chain method</td>\n",
       "      <td>[A Shirzad,  F Loran]</td>\n",
       "      <td></td>\n",
       "      <td>in a recent work we showed that for a hamilton...</td>\n",
       "      <td>[gauge, fixing, chain, chain, method]</td>\n",
       "      <td>[a shirzad, f loran]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27766</th>\n",
       "      <td>9912290</td>\n",
       "      <td>2000</td>\n",
       "      <td>shuffling quantum field theory</td>\n",
       "      <td>[Dirk Kreimer]</td>\n",
       "      <td>Lett.Math.Phys.</td>\n",
       "      <td>we discuss shuffle identities between feynman ...</td>\n",
       "      <td>[shuffle, quantum, field, theory]</td>\n",
       "      <td>[dirk kreimer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27767</th>\n",
       "      <td>9912291</td>\n",
       "      <td>1999</td>\n",
       "      <td>small object limit of casimir effect and the s...</td>\n",
       "      <td>[O. Kenneth,  S. Nussinov]</td>\n",
       "      <td>Phys.Rev.</td>\n",
       "      <td>we show a simple way of deriving the casimir p...</td>\n",
       "      <td>[small, object, limit, casimir, effect, sign, ...</td>\n",
       "      <td>[o kenneth, s nussinov]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27768</th>\n",
       "      <td>9912292</td>\n",
       "      <td>1999</td>\n",
       "      <td>1 4 pbgs and superparticle actions</td>\n",
       "      <td>[F.Delduc,  E. Ivanov,  S. Krivonos]</td>\n",
       "      <td></td>\n",
       "      <td>karpacz poland september 21-25 1999 we constru...</td>\n",
       "      <td>[pbgs, superparticle, action]</td>\n",
       "      <td>[fdelduc, e a ivanov, s krivonos]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27769</th>\n",
       "      <td>9912293</td>\n",
       "      <td>2000</td>\n",
       "      <td>corrections to the abelian born-infeld action ...</td>\n",
       "      <td>[L. Cornalba (I.H.E.S.)]</td>\n",
       "      <td>JHEP</td>\n",
       "      <td>noncommutative geometry in a recent paper seib...</td>\n",
       "      <td>[correction, abelian, bear, infeld, action, ar...</td>\n",
       "      <td>[l cornalba ihes]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27770 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  pub_year                                              title  \\\n",
       "0         1001      2000              compactification geometry and duality   \n",
       "1         1002      2000  domain walls and massive gauged supergravity p...   \n",
       "2         1003      2000     comment on metric fluctuations in brane worlds   \n",
       "3         1004      2000         moving mirrors and thermodynamic paradoxes   \n",
       "4         1005      2000  bundles of chiral blocks and boundary conditio...   \n",
       "...        ...       ...                                                ...   \n",
       "27765  9912289      2002          gauge fixing in the chain by chain method   \n",
       "27766  9912290      2000                     shuffling quantum field theory   \n",
       "27767  9912291      1999  small object limit of casimir effect and the s...   \n",
       "27768  9912292      1999                 1 4 pbgs and superparticle actions   \n",
       "27769  9912293      2000  corrections to the abelian born-infeld action ...   \n",
       "\n",
       "                                    authors       journal_name  \\\n",
       "0                       [Paul S. Aspinwall]                      \n",
       "1           [M. Cvetic,  H. Lu,  C.N. Pope]  Class.Quant.Grav.   \n",
       "2               [Y.S. Myung,  Gungwon Kang]                      \n",
       "3                          [Adam D. Helfer]          Phys.Rev.   \n",
       "4                [J. Fuchs,  C. Schweigert]                      \n",
       "...                                     ...                ...   \n",
       "27765                 [A Shirzad,  F Loran]                      \n",
       "27766                        [Dirk Kreimer]    Lett.Math.Phys.   \n",
       "27767            [O. Kenneth,  S. Nussinov]          Phys.Rev.   \n",
       "27768  [F.Delduc,  E. Ivanov,  S. Krivonos]                      \n",
       "27769              [L. Cornalba (I.H.E.S.)]               JHEP   \n",
       "\n",
       "                                                abstract  \\\n",
       "0      these are notes based on lectures given at tas...   \n",
       "1      we point out that massive gauged supergravity ...   \n",
       "2      recently ivanov and volovich hep-th 9912242 cl...   \n",
       "3      quantum fields responding to moving mirrors ha...   \n",
       "4      proceedings of lie iii clausthal july 1999 var...   \n",
       "...                                                  ...   \n",
       "27765  in a recent work we showed that for a hamilton...   \n",
       "27766  we discuss shuffle identities between feynman ...   \n",
       "27767  we show a simple way of deriving the casimir p...   \n",
       "27768  karpacz poland september 21-25 1999 we constru...   \n",
       "27769  noncommutative geometry in a recent paper seib...   \n",
       "\n",
       "                                             title_lemma  \\\n",
       "0                  [compactification, geometry, duality]   \n",
       "1      [domain, wall, massive, gauge, supergravity, p...   \n",
       "2           [comment, metric, fluctuation, brane, world]   \n",
       "3                 [move, mirror, thermodynamic, paradox]   \n",
       "4      [bundle, chiral, block, boundary, condition, cft]   \n",
       "...                                                  ...   \n",
       "27765              [gauge, fixing, chain, chain, method]   \n",
       "27766                  [shuffle, quantum, field, theory]   \n",
       "27767  [small, object, limit, casimir, effect, sign, ...   \n",
       "27768                      [pbgs, superparticle, action]   \n",
       "27769  [correction, abelian, bear, infeld, action, ar...   \n",
       "\n",
       "                     author_representant  \n",
       "0                     [paul s aspinwall]  \n",
       "1             [m cvetivc, h lu, cn pope]  \n",
       "2               [ys myung, gungwon kang]  \n",
       "3                        [adam d helfer]  \n",
       "4                [j fuchs, c schweigert]  \n",
       "...                                  ...  \n",
       "27765               [a shirzad, f loran]  \n",
       "27766                     [dirk kreimer]  \n",
       "27767            [o kenneth, s nussinov]  \n",
       "27768  [fdelduc, e a ivanov, s krivonos]  \n",
       "27769                  [l cornalba ihes]  \n",
       "\n",
       "[27770 rows x 8 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute nodes and edges\n",
    "pre_edges = list(information_df.author_representant.apply(lambda x : [(x[i],x[j]) for i in range(len(x)) for j in range(len(x)) if i>j]))\n",
    "authors_edges = [edge for list_edge in pre_edges for edge in list_edge]\n",
    "authors_edges_dict = Counter(authors_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of nodes: 14447\n",
      "The number of edges: 29111\n"
     ]
    }
   ],
   "source": [
    "G_authors = nx.Graph()\n",
    "G_authors.add_nodes_from(set(representant_dict.values()))\n",
    "G_authors.add_weighted_edges_from([(a,b,weight) for (a,b),weight in authors_edges_dict.items()])\n",
    "\n",
    "print(\"The number of nodes: {}\".format(G_authors.number_of_nodes()))\n",
    "print(\"The number of edges: {}\".format(G_authors.number_of_edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Walklets\n",
    "from karateclub import Walklets\n",
    "if os.path.isfile('Data/processed_data/articles_walklets_embeddings.pkl','wb'):\n",
    "    walklets_embeddings = pickle.load(open('Data/processed_data/articles_walklets_embeddings.pkl','rb'))\n",
    "else:\n",
    "    walklets = Walklets(walk_length=80) # we leave the defaults parameters for the other values\n",
    "    walklets.fit(G_articles)\n",
    "    walklets_embeddings = walklets.get_embedding()\n",
    "    pickle.dump(walklets_embeddings, open('Data/processed_data/articles_walklets_embeddings.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 27770/27770 [03:12<00:00, 143.96it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 200/200 [09:47<00:00,  2.94s/it]\n"
     ]
    }
   ],
   "source": [
    "### Node2Vec\n",
    "node2vec = Node2Vec(G_articles, dimensions=32, walk_length=8, num_walks=200, workers=1, p=1, q=1)\n",
    "# Embed nodes\n",
    "model = node2vec.fit(window=5, min_count=1, batch_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model.wv, open('Data/processed_data/articles_node2vec_embeddings.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve node embeddings and corresponding subjects\n",
    "node_ids = model.wv.index2word  # list of node IDs\n",
    "node_embeddings = (\n",
    "    model.wv.vectors\n",
    ")  # numpy.ndarray of size number of nodes times embeddings dimensionality\n",
    "node_targets = node_subjects[[int(node_id) for node_id in node_ids]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute abstracts embeddings using specter network\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "if os.path.isfile('Data/processed_data/abstracts_embeddings.pkl'):\n",
    "    abstracts_embeddings = pickle.load(open('Data/processed_data/abstracts_embeddings.pkl','rb'))\n",
    "else:\n",
    "    \n",
    "    # load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('allenai/specter')\n",
    "    model = AutoModel.from_pretrained('allenai/specter')\n",
    "    papers = []\n",
    "    for i in range(information_df.shape[0]):\n",
    "        article = information_df.loc[i]\n",
    "        title = article.title\n",
    "        abstract = article.abstract\n",
    "        papers.append({'title':title, 'abstract':abstract})\n",
    "\n",
    "    # concatenate title and abstract\n",
    "    title_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\n",
    "    # preprocess the input\n",
    "    inputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    result = model(**inputs)\n",
    "    # take the first token in the batch as the embedding\n",
    "    abstracts_embeddings = result.last_hidden_state[:, 0, :]\n",
    "    pickle.dump(abstracts_embeddings, open('Data/processed_data/abstracts_embeddings.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Common authors\n",
    "authors_information_df = information_df[['ID','author_representant']]\n",
    "\n",
    "# prepare data frame for common authors computation\n",
    "common_authors_df = (train_set\n",
    ".merge(authors_information_df, how ='left', left_on = ['node1'], right_on = ['ID'])\n",
    ".rename(columns = {'author_representant':'authors_node_1'})\n",
    ".merge(authors_information_df, how ='left', left_on = ['node2'], right_on = ['ID'])\n",
    ".rename(columns = {'author_representant':'authors_node_2'})\n",
    ")\n",
    "\n",
    "#  compute common auhtors\n",
    "train_set['common_authors'] = common_authors_df.apply(lambda x:set(x.authors_node_1)&set(x.authors_node_2),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### authors embedding leveraging author graphs (random walk for instance)\n",
    "\n",
    "### Node2Vec\n",
    "if os.path.isfile('Data/processed_data/articles_node2vec_embeddings.pkl'):\n",
    "    articles_node2vec_embeddings = pickle.load(open('Data/processed_data/articles_node2vec_embeddings.pkl', 'rb'))\n",
    "else:\n",
    "    node2vec_authors = Node2Vec(G_authors, dimensions=32, walk_length=8, num_walks=200, workers=1, p=1, q=1)\n",
    "    model_authors = node2vec_authors.fit(window=5, min_count=1, batch_words=4)\n",
    "    articles_node2vec_embeddings = model_authors.wv\n",
    "    pickle.dump(articles_node2vec_embeddings, open('Data/processed_data/articles_node2vec_embeddings.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Delta publication year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### titles common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6657d8cf73e4192045730bbde1f7a947d4725a27f96025bfcb1ab47bc67665b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
