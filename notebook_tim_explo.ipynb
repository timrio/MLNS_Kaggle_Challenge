{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\33631\\AppData\\Local\\Temp\\ipykernel_141976\\1108344814.py:17: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
      "  from scipy.stats.stats import pearsonr\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import os\n",
    "import networkx as nx\n",
    "from gensim.models import Word2Vec\n",
    "from node2vec import Node2Vec\n",
    "import pickle\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score \n",
    "from sklearn.preprocessing import LabelBinarizer \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from namematcher import NameMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usefull stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_path = \"Data/raw_data/node_information.csv\"\n",
    "test_set_path = \"Data/raw_data/testing_set.txt\"\n",
    "train_set_path = \"Data/raw_data/training_set.txt\"\n",
    "random_preds_path = \"Data/raw_data/random_predictions.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(a,b):\n",
    "    return(a.dot(b) / (np.linalg.norm(a) * np.linalg.norm(b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>pub_year</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>new_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16959</th>\n",
       "      <td>9511024</td>\n",
       "      <td>1996</td>\n",
       "      <td>classical geometry and target space duality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sentence comment on a reference was amended th...</td>\n",
       "      <td>16959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23871</th>\n",
       "      <td>9807177</td>\n",
       "      <td>1998</td>\n",
       "      <td>nonlocal electrodynamics in 2 1 dimensions fro...</td>\n",
       "      <td>Qiong-gui Lin</td>\n",
       "      <td>Commun.Theor.Phys.</td>\n",
       "      <td>the theory of a spinor field interacting with ...</td>\n",
       "      <td>23871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11523</th>\n",
       "      <td>9302008</td>\n",
       "      <td>1993</td>\n",
       "      <td>parameter restrictions in a non-commutative ge...</td>\n",
       "      <td>E. Alvarez, J.M. Gracia-Bondia, C.P. Martin</td>\n",
       "      <td>Phys.Lett.</td>\n",
       "      <td>survive standard quantum corrections we have i...</td>\n",
       "      <td>11523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  pub_year                                              title  \\\n",
       "16959  9511024      1996        classical geometry and target space duality   \n",
       "23871  9807177      1998  nonlocal electrodynamics in 2 1 dimensions fro...   \n",
       "11523  9302008      1993  parameter restrictions in a non-commutative ge...   \n",
       "\n",
       "                                           authors        journal_name  \\\n",
       "16959                                          NaN                 NaN   \n",
       "23871                                Qiong-gui Lin  Commun.Theor.Phys.   \n",
       "11523  E. Alvarez, J.M. Gracia-Bondia, C.P. Martin          Phys.Lett.   \n",
       "\n",
       "                                                abstract  new_ID  \n",
       "16959  sentence comment on a reference was amended th...   16959  \n",
       "23871  the theory of a spinor field interacting with ...   23871  \n",
       "11523  survive standard quantum corrections we have i...   11523  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_df = pd.read_csv(information_path, header=None)\n",
    "information_df.columns = [\"ID\",'pub_year','title','authors','journal_name','abstract']\n",
    "### !!!! We have to use new index starting from 0 because of the implementation of karate-club library\n",
    "information_df = information_df.assign(new_ID = [i for i in range(information_df.shape[0])])\n",
    "information_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>node1</th>\n",
       "      <th>node2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22161</th>\n",
       "      <td>1</td>\n",
       "      <td>16802</td>\n",
       "      <td>16378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292422</th>\n",
       "      <td>1</td>\n",
       "      <td>6760</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222854</th>\n",
       "      <td>1</td>\n",
       "      <td>9913</td>\n",
       "      <td>8026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567115</th>\n",
       "      <td>1</td>\n",
       "      <td>25601</td>\n",
       "      <td>24561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196647</th>\n",
       "      <td>1</td>\n",
       "      <td>5507</td>\n",
       "      <td>4053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label  node1  node2\n",
       "22161       1  16802  16378\n",
       "292422      1   6760    450\n",
       "222854      1   9913   8026\n",
       "567115      1  25601  24561\n",
       "196647      1   5507   4053"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = pd.read_csv(train_set_path, sep =\" \", header = None)\n",
    "train_set.columns = ['node1','node2','label']\n",
    "### !!! we will use the new indices!!! (see information_df for correspondances)\n",
    "train_set = (train_set\n",
    "    .merge(information_df[['ID','new_ID']], how = 'left', left_on = ['node1'], right_on = ['ID'])\n",
    "    .drop(columns = ['node1','ID'])\n",
    "    .rename(columns = {'new_ID':'node1'})\n",
    "    .merge(information_df[['ID','new_ID']], how = 'left', left_on = ['node2'], right_on = ['ID'])\n",
    "    .drop(columns = ['node2','ID'])\n",
    "    .rename(columns = {'new_ID':'node2'})\n",
    ")\n",
    "train_set.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node1</th>\n",
       "      <th>node2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9771</th>\n",
       "      <td>9711205</td>\n",
       "      <td>107082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20185</th>\n",
       "      <td>9701082</td>\n",
       "      <td>9511006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24511</th>\n",
       "      <td>9703094</td>\n",
       "      <td>9201027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24628</th>\n",
       "      <td>9901150</td>\n",
       "      <td>9207025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19063</th>\n",
       "      <td>11041</td>\n",
       "      <td>201070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         node1    node2\n",
       "9771   9711205   107082\n",
       "20185  9701082  9511006\n",
       "24511  9703094  9201027\n",
       "24628  9901150  9207025\n",
       "19063    11041   201070"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = pd.read_csv(test_set_path, sep =\" \", header = None)\n",
    "test_set.columns = ['node1','node2']\n",
    "test_set.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information pre_processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                 0\n",
       "pub_year           0\n",
       "title              0\n",
       "authors         4033\n",
       "journal_name    7472\n",
       "abstract           0\n",
       "new_ID             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_df = information_df.fillna({'authors':'', 'journal_name':''})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_df.authors = information_df.authors.apply(lambda x:x.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\"Data/processed_data/information.csv\"):\n",
    "    information_df = pickle.load(open(\"Data/processed_data/information.csv\",'rb'))\n",
    "else:\n",
    "    information_df['title_lemma'] = information_df.title.apply(lambda x: [token.lemma_ for token in spacy_nlp(x) if not token.is_punct if not token.is_digit if not token.is_stop])\n",
    "    pickle.dump(information_df, open(\"Data/processed_data/information.csv\",'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Articles based graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = set(np.concatenate((train_set.node1,train_set.node2), axis = 0))\n",
    "edges = set(train_set.query(\"label == 1\").apply(lambda x: (x.node1,x.node2), axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of nodes: 27770\n",
      "The number of edges: 267828\n"
     ]
    }
   ],
   "source": [
    "G_articles = nx.Graph()\n",
    "G_articles.add_nodes_from(nodes)\n",
    "G_articles.add_edges_from(edges)\n",
    "\n",
    "print(\"The number of nodes: {}\".format(G_articles.number_of_nodes()))\n",
    "print(\"The number of edges: {}\".format(G_articles.number_of_edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors co-authorship based graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# convert to lower case, remove punctuation, strip the names\n",
    "authors_raw_set = set([auth.strip().lower().translate(str.maketrans('', '', string.punctuation)) for list_auth in information_df.authors for auth in list_auth if len(auth)>1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name matching: to make identify people name by different ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from namematcher import NameMatcher\n",
    "name_matcher = NameMatcher()\n",
    "\n",
    "def compute_unique_names(authors_raw_set):\n",
    "    \"\"\"\n",
    "    one author can be named differently on different papers\n",
    "    this function aims at finding a 'representant' (longest name that describe an author) for each \n",
    "    author\n",
    "    inputs:\n",
    "        - authors_raw_set: set of previously extracted author names\n",
    "    outputs:\n",
    "        - dict: keys are the name in authors_raw_set and the values are the representant\n",
    "    \"\"\"\n",
    "    representant_dict = {}\n",
    "    attributed_nodes = [] # names that already have a representant\n",
    "    for name in tqdm(authors_raw_set, position = 0):\n",
    "        sim_list = [] # similar names \n",
    "        if name not in attributed_nodes:\n",
    "            for name2 in authors_raw_set:\n",
    "                try:\n",
    "                    if name != name2 and name[0]==name2[0] and name2 not in attributed_nodes:\n",
    "                        # two names need to start by the same letter to be consider as potential equivalents\n",
    "                        score = name_matcher.match_names(name, name2)\n",
    "                        if score > 0.9: # if names are close enough\n",
    "                            sim_list.append(name2)\n",
    "                except:\n",
    "                    continue\n",
    "            sim_list.append(name) # the representant is in this list\n",
    "            attributed_nodes.extend(sim_list) # we have fund a representant for those names\n",
    "            representant = max(sim_list, key=len) # the representant is the longest name\n",
    "            for name in sim_list: # all those names have the same representant\n",
    "                representant_dict[name] = representant\n",
    "    return(representant_dict)\n",
    "\n",
    "if os.path.isfile('Data/processed_data/representant_dict.pkl'):\n",
    "    representant_dict = pickle.load(open('Data/processed_data/representant_dict.pkl','rb'))\n",
    "else:\n",
    "    representant_dict = compute_unique_names(authors_raw_set)\n",
    "    pickle.dump(representant_dict, open('Data/processed_data/representant_dict.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set each name to its representant value\n",
    "information_df.authors = information_df.authors.apply(lambda x: [representant_dict[auth.strip().lower().translate(str.maketrans('', '', string.punctuation))] for auth in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a unique index for each author\n",
    "representants_list = list(set(representant_dict.values()))\n",
    "authors2idx = {k: v for v, k in enumerate(representants_list)}\n",
    "\n",
    "information_df[\"authors_id\"] = information_df.authors.apply(lambda x: [authors2idx[auth] for auth in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>pub_year</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>new_ID</th>\n",
       "      <th>title_lemma</th>\n",
       "      <th>authors_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12845</th>\n",
       "      <td>9311053</td>\n",
       "      <td>1993</td>\n",
       "      <td>anyonic construction of the sl q s 2 algebra</td>\n",
       "      <td>[jl matheusvalle, mrmonteiro]</td>\n",
       "      <td>Mod.Phys.Lett.</td>\n",
       "      <td>after the end document on the directory you ar...</td>\n",
       "      <td>12845</td>\n",
       "      <td>[anyonic, construction, sl, q, s, algebra]</td>\n",
       "      <td>[4477, 9493]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10020</th>\n",
       "      <td>303266</td>\n",
       "      <td>2003</td>\n",
       "      <td>time-dependent backgrounds from supergravity w...</td>\n",
       "      <td>[klaus behrndt, mirjam cvetivc]</td>\n",
       "      <td></td>\n",
       "      <td>r-symmetry we obtain a general class of time-d...</td>\n",
       "      <td>10020</td>\n",
       "      <td>[time, dependent, background, supergravity, ga...</td>\n",
       "      <td>[8749, 4752]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  pub_year                                              title  \\\n",
       "12845  9311053      1993       anyonic construction of the sl q s 2 algebra   \n",
       "10020   303266      2003  time-dependent backgrounds from supergravity w...   \n",
       "\n",
       "                               authors    journal_name  \\\n",
       "12845    [jl matheusvalle, mrmonteiro]  Mod.Phys.Lett.   \n",
       "10020  [klaus behrndt, mirjam cvetivc]                   \n",
       "\n",
       "                                                abstract  new_ID  \\\n",
       "12845  after the end document on the directory you ar...   12845   \n",
       "10020  r-symmetry we obtain a general class of time-d...   10020   \n",
       "\n",
       "                                             title_lemma    authors_id  \n",
       "12845         [anyonic, construction, sl, q, s, algebra]  [4477, 9493]  \n",
       "10020  [time, dependent, background, supergravity, ga...  [8749, 4752]  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute nodes and edges\n",
    "pre_edges = list(information_df.authors_id.apply(lambda x : [(x[i],x[j]) for i in range(len(x)) for j in range(len(x)) if i>j]))\n",
    "authors_edges = [edge for list_edge in pre_edges for edge in list_edge]\n",
    "authors_edges_dict = Counter(authors_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of nodes: 14447\n",
      "The number of edges: 29111\n"
     ]
    }
   ],
   "source": [
    "G_authors = nx.Graph()\n",
    "G_authors.add_nodes_from(authors2idx.values())\n",
    "G_authors.add_weighted_edges_from([(a,b,weight) for (a,b),weight in authors_edges_dict.items()])\n",
    "\n",
    "print(\"The number of nodes: {}\".format(G_authors.number_of_nodes()))\n",
    "print(\"The number of edges: {}\".format(G_authors.number_of_edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various embeddings computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Based embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using articles graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Walklets\n",
    "from karateclub import Walklets\n",
    "if os.path.isfile('Data/processed_data/articles_walklets_embeddings.pkl'):\n",
    "    walklets_articles_embeddings = pickle.load(open('Data/processed_data/articles_walklets_embeddings.pkl','rb'))\n",
    "else:\n",
    "    walklets = Walklets(walk_length=80) # we leave the defaults parameters for the other values\n",
    "    walklets.fit(G_articles)\n",
    "    walklets_articles_embeddings = walklets.get_embedding()\n",
    "    pickle.dump(walklets_articles_embeddings, open('Data/processed_data/articles_walklets_embeddings.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Node2Vec\n",
    "from karateclub import Node2Vec\n",
    "if os.path.isfile('Data/processed_data/articles_node2vec_embeddings.pkl'):\n",
    "    articles_node2vec_embeddings = pickle.load(open('Data/processed_data/articles_node2vec_embeddings.pkl','rb'))\n",
    "else:\n",
    "    node2vec = Node2Vec(walk_length=15) # we leave the defaults parameters for the other values\n",
    "    node2vec.fit(G_articles)\n",
    "    articles_node2vec_embeddings = node2vec.get_embedding()\n",
    "    pickle.dump(articles_node2vec_embeddings, open('Data/processed_data/articles_node2vec_embeddings.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 492409/492409 [02:34<00:00, 3192.48it/s]\n"
     ]
    }
   ],
   "source": [
    "### compute shortes path between nodes\n",
    "shortest_path_lengths = []\n",
    "for i in tqdm(range(train_set.shape[0]), position = 0):\n",
    "    value = train_set.loc[i]\n",
    "    try:\n",
    "        path_len = nx.shortest_path_length(G_articles, source=value.node1, target=value.node2)\n",
    "    except:\n",
    "        path_len = np.nan\n",
    "    shortest_path_lengths.append(path_len)\n",
    "train_set['shortest_path'] = shortest_path_lengths\n",
    "train_set = train_set.fillna({\"shortest_path\":train_set.shortest_path+10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using author graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### authors embedding leveraging author graphs (random walk for instance)\n",
    "\n",
    "### Node2Vec\n",
    "if os.path.isfile('Data/processed_data/authors_node2vec_embeddings.pkl'):\n",
    "    authors_node2vec_embeddings = pickle.load(open('Data/processed_data/authors_node2vec_embeddings.pkl', 'rb'))\n",
    "else:\n",
    "    node2vec_authors = Node2Vec(walk_length=15)\n",
    "    node2vec_authors.fit(G_authors)\n",
    "    authors_node2vec_embeddings = node2vec_authors.get_embedding()\n",
    "    pickle.dump(authors_node2vec_embeddings, open('Data/processed_data/authors_node2vec_embeddings.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>pub_year</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>new_ID</th>\n",
       "      <th>title_lemma</th>\n",
       "      <th>authors_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8421</th>\n",
       "      <td>210029</td>\n",
       "      <td>2003</td>\n",
       "      <td>cosmology with radion and bulk scalar field in...</td>\n",
       "      <td>[shinpei kobayashi, kazuya koyama]</td>\n",
       "      <td>JHEP</td>\n",
       "      <td>we investigate cosmological evolutions of the ...</td>\n",
       "      <td>8421</td>\n",
       "      <td>[cosmology, radion, bulk, scalar, field, brane...</td>\n",
       "      <td>[11142, 4747]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19964</th>\n",
       "      <td>9701146</td>\n",
       "      <td>1997</td>\n",
       "      <td>expanding and contracting universes in third q...</td>\n",
       "      <td>[a buonanno, m gasperini, m maggiore, c ungare...</td>\n",
       "      <td>Class.Quant.Grav.</td>\n",
       "      <td>collection of papers on the pre-big bang scena...</td>\n",
       "      <td>19964</td>\n",
       "      <td>[expand, contracting, universe, quantize, stri...</td>\n",
       "      <td>[4822, 7060, 7149, 5218]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  pub_year                                              title  \\\n",
       "8421    210029      2003  cosmology with radion and bulk scalar field in...   \n",
       "19964  9701146      1997  expanding and contracting universes in third q...   \n",
       "\n",
       "                                                 authors       journal_name  \\\n",
       "8421                  [shinpei kobayashi, kazuya koyama]               JHEP   \n",
       "19964  [a buonanno, m gasperini, m maggiore, c ungare...  Class.Quant.Grav.   \n",
       "\n",
       "                                                abstract  new_ID  \\\n",
       "8421   we investigate cosmological evolutions of the ...    8421   \n",
       "19964  collection of papers on the pre-big bang scena...   19964   \n",
       "\n",
       "                                             title_lemma  \\\n",
       "8421   [cosmology, radion, bulk, scalar, field, brane...   \n",
       "19964  [expand, contracting, universe, quantize, stri...   \n",
       "\n",
       "                     authors_id  \n",
       "8421              [11142, 4747]  \n",
       "19964  [4822, 7060, 7149, 5218]  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each article take a mean of the authors embedding as global autors embedding\n",
    "articles_authors_embedding = []\n",
    "for i in range(information_df.shape[0]):\n",
    "    value = information_df[information_df.new_ID == i]\n",
    "    authors_id = value.authors_id\n",
    "    embeddings = np.array([0 for i in range(128)]).astype('float64')\n",
    "    for author in authors_id:\n",
    "        embeddings+=authors_node2vec_embeddings[author][0]\n",
    "    articles_authors_embedding.append(embeddings/len(authors_id))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node based embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute abstracts embeddings using specter network\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = 'gpu'\n",
    "\n",
    "if os.path.isfile('Data/processed_data/abstracts_embeddings.pkl'):\n",
    "    abstracts_embeddings = pickle.load(open('Data/processed_data/abstracts_embeddings.pkl','rb'))\n",
    "else:\n",
    "    \n",
    "    # load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('allenai/specter')\n",
    "    model = AutoModel.from_pretrained('allenai/specter').to(device)\n",
    "    model.eval()\n",
    "    abstracts_embeddings = []\n",
    "    for i in tqdm(range(information_df.shape[0]), position = 0):\n",
    "        article = information_df.loc[i]\n",
    "        title = article.title\n",
    "        abstract = article.abstract\n",
    "        paper = [{'title':title, 'abstract':abstract}]\n",
    "\n",
    "        # concatenate title and abstract\n",
    "        title_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in paper]\n",
    "        # preprocess the input\n",
    "        inputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "        inputs = inputs.to(device)\n",
    "        result = model(**inputs)\n",
    "        # take the first token in the batch as the embedding\n",
    "        embedding = result.last_hidden_state[:, 0, :].detach().cpu().numpy()\n",
    "        abstracts_embeddings.append(embedding)\n",
    "    pickle.dump(abstracts_embeddings, open('Data/processed_data/abstracts_embeddings.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute edges features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(graph, edge):\n",
    "\n",
    "    inter_size = len(list(nx.common_neighbors(graph, edge[0], edge[1])))\n",
    "    union_size = len(set(graph[edge[0]]) | set(graph[edge[1]]))\n",
    "    jacard = inter_size / union_size\n",
    "\n",
    "    return jacard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdamicAdar(graph, edge):\n",
    "\n",
    "    inter_list = nx.common_neighbors(graph, edge[0], edge[1])\n",
    "    adamic_adar = AdamicAdar[edge] = sum( [1/np.log(graph.degree(node)) for node in inter_list])\n",
    "    \n",
    "    return adamic_adar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preferential_attachement(graph, edge):\n",
    "\n",
    "\n",
    "    pa = graph.degree(edge[0]) * graph.degree(edge[1])\n",
    "        \n",
    "    return pa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_non_embeddings_features(df, information_df, G_articles):\n",
    "    useful_information_df = information_df[['new_ID','authors','pub_year', 'title_lemma']]\n",
    "\n",
    "    # prepare data frame for common authors computation\n",
    "    df = (df\n",
    "    .merge(useful_information_df, how ='left', left_on = ['node1'], right_on = ['new_ID'])\n",
    "    .rename(columns = {'authors':'authors_node_1', 'pub_year':'pub_year1', 'title_lemma':'title_lemma1'})\n",
    "    .merge(useful_information_df, how ='left', left_on = ['node2'], right_on = ['new_ID'])\n",
    "    .rename(columns = {'authors':'authors_node_2', 'pub_year':'pub_year2', 'title_lemma':'title_lemma2'})\n",
    "    )\n",
    "\n",
    "    print('computing common authors')\n",
    "    #  compute common authors\n",
    "    df['common_authors'] = df.apply(lambda x:len(set(x.authors_node_1)&set(x.authors_node_2)),axis = 1)\n",
    "\n",
    "    print('computing common words')\n",
    "    #  compute common words in titles\n",
    "    df['common_title_words'] = df.apply(lambda x:len(set(x.title_lemma1)&set(x.title_lemma2)),axis = 1)\n",
    "\n",
    "    print('computing delta publication year')\n",
    "    # compute delta publication year\n",
    "    df['delta_publication'] = df.apply(lambda x:np.abs(x.pub_year2 - x.pub_year1),axis = 1)\n",
    "\n",
    "    # compute edges features\n",
    "    print('computing jacard index')\n",
    "    df['jacard'] = df.apply(lambda x: Jaccard(G_articles, (x.node1, x.node2)),axis = 1)\n",
    "\n",
    "    print('computing preferential attachement')\n",
    "    df['pa'] = df.apply(lambda x: preferential_attachement(G_articles, (x.node1, x.node2)),axis = 1)\n",
    "\n",
    "    print('computing adamic_adar')\n",
    "    df['adamic_adar'] = df.apply(lambda x: AdamicAdar(G_articles, (x.node1, x.node2)),axis = 1)\n",
    "\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding_cosines_features(df,\n",
    "                                         articles_node2vec_embeddings,\n",
    "                                         walklets_articles_embeddings,\n",
    "                                         articles_authors_embedding, \n",
    "                                         abstracts_embeddings):\n",
    "    # compute some cosine based distances\n",
    "    df['articles_node2vec_cosine'] = df.apply(lambda x:cosine(articles_node2vec_embeddings[x.node1],articles_node2vec_embeddings[x.node2]), axis = 1)\n",
    "    df['articles_walklets_cosine'] = df.apply(lambda x:cosine(walklets_articles_embeddings[x.node1],walklets_articles_embeddings[x.node2]), axis = 1)\n",
    "    df['authors_embeddings_cosine'] = df.apply(lambda x:cosine(articles_authors_embedding[x.node1],articles_authors_embedding[x.node2]), axis = 1)\n",
    "    df['abstracts_embeddings_cosine'] = df.apply(lambda x:cosine(abstracts_embeddings[x.node1],abstracts_embeddings[x.node2]), axis = 1)\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train/validation/test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_set.drop(columns = ['label'])\n",
    "y = train_set[['label']]\n",
    "train_samples, validation_samples, train_labels, validation_labels = train_test_split(X,y, test_size=0.2, random_state=0, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.concat([train_samples, train_labels], axis = 1).reset_index(drop = True)\n",
    "validation_set = pd.concat([validation_samples, validation_labels], axis = 1).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run models"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6657d8cf73e4192045730bbde1f7a947d4725a27f96025bfcb1ab47bc67665b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
