{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: OMP_NUM_THREADS=None =>\n",
      "... If you are using openblas if you are using openblas set OMP_NUM_THREADS=1 or risk subprocess calls hanging indefinitely\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\33631\\AppData\\Local\\Temp\\ipykernel_171892\\997310428.py:24: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
      "  from scipy.stats.stats import pearsonr\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import os\n",
    "import networkx as nx\n",
    "from gensim.models import Word2Vec\n",
    "from node2vec import Node2Vec\n",
    "import pickle\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from hpsklearn import HyperoptEstimator, any_classifier, any_preprocessing\n",
    "from hyperopt import tpe\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score \n",
    "from sklearn.preprocessing import LabelBinarizer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from link_pred.utils import cosine, euclidian, retrieve_and_pre_processed_informations, compute_unique_names\n",
    "from link_pred.folds_creation import create_and_save_folds\n",
    "from link_pred.create_graphs import create_articles_graph, create_co_authorship_graph, create_authors_co_citation_graph\n",
    "from link_pred.node_embeddings import compute_abstracts_embeddings, compute_walklets, compute_node2vec, compute_deep_walks\n",
    "from link_pred.edges_features import Jaccard, AdamicAdar, preferential_attachement, are_connected, common_journal\n",
    "from link_pred.grid_search import get_best_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_path = \"Data/raw_data/node_information.csv\"\n",
    "test_set_path = \"Data/raw_data/testing_set.txt\"\n",
    "train_set_path = \"Data/raw_data/training_set.txt\"\n",
    "random_preds_path = \"Data/raw_data/random_predictions.csv\"\n",
    "\n",
    "number_of_folds = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and pre_process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_df = retrieve_and_pre_processed_informations(information_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign new id to each node (needs to start from 0)\n",
    "id_old2new = {k: v for v, k in enumerate(list(information_df.ID))}\n",
    "id_new2old = {v: k for v, k in id_old2new.items()}\n",
    "\n",
    "information_df['new_ID'] = information_df.ID.apply(lambda x: id_old2new[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_train_set = pd.read_csv(train_set_path, sep =\" \", header = None)\n",
    "initial_train_set.columns = ['node1','node2','label']\n",
    "\n",
    "## update nodes values to new indices\n",
    "initial_train_set.node1 = initial_train_set.apply(lambda x:id_old2new[x.node1], axis = 1)\n",
    "initial_train_set.node2 = initial_train_set.apply(lambda x:id_old2new[x.node2], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load test set\n",
    "\n",
    "test_set = pd.read_csv(test_set_path, sep =\" \", header = None)\n",
    "test_set.columns = ['node1','node2']\n",
    "\n",
    "## update nodes values to new indices\n",
    "test_set.node1 = test_set.apply(lambda x:id_old2new[x.node1], axis = 1)\n",
    "test_set.node2 = test_set.apply(lambda x:id_old2new[x.node2], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1 already exists !\n",
      "fold 2 already exists !\n",
      "fold 3 already exists !\n",
      "fold 4 already exists !\n",
      "fold 5 already exists !\n"
     ]
    }
   ],
   "source": [
    "create_and_save_folds(initial_train_set, number_of_folds = number_of_folds, validation_size = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with authors various names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# convert to lower case, remove punctuation, strip the names\n",
    "authors_raw_set = set([auth.strip().lower().translate(str.maketrans('', '', string.punctuation)) for list_auth in information_df.authors for auth in list_auth if len(auth)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# several authors can be named differently (eg. Jean DUPONT, J.Dupont, etc.)\n",
    "# we create a name matcher function to try to indentify each author and assign each denomination a \"representant\"\n",
    "\n",
    "if os.path.isfile('Data/processed_data/representant_dict.pkl'):\n",
    "    representant_dict = pickle.load(open('Data/processed_data/representant_dict.pkl','rb'))\n",
    "else:\n",
    "    representant_dict = compute_unique_names(authors_raw_set)\n",
    "    pickle.dump(representant_dict, open('Data/processed_data/representant_dict.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set each name to its representant value\n",
    "information_df.authors = information_df.authors.apply(lambda x: [representant_dict[auth.strip().lower().translate(str.maketrans('', '', string.punctuation))] for auth in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>pub_year</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title_lemma</th>\n",
       "      <th>authors_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11573</th>\n",
       "      <td>9302075</td>\n",
       "      <td>1993</td>\n",
       "      <td>tachyon splits the d 2 string black hole horiz...</td>\n",
       "      <td>[s kalyana rama]</td>\n",
       "      <td>Phys.Rev.Lett.</td>\n",
       "      <td>singular minor change in the acknowledgement o...</td>\n",
       "      <td>[tachyon, split, d, string, black, hole, horiz...</td>\n",
       "      <td>[13234]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22650</th>\n",
       "      <td>9802048</td>\n",
       "      <td>1998</td>\n",
       "      <td>study of wilson loop functionals in 2d yang-mi...</td>\n",
       "      <td>[jm aroca, yuri a kubyshin]</td>\n",
       "      <td></td>\n",
       "      <td>to the proceedings of the xiith workshop on hi...</td>\n",
       "      <td>[study, wilson, loop, functional, 2d, yang, mi...</td>\n",
       "      <td>[5636, 9242]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  pub_year                                              title  \\\n",
       "11573  9302075      1993  tachyon splits the d 2 string black hole horiz...   \n",
       "22650  9802048      1998  study of wilson loop functionals in 2d yang-mi...   \n",
       "\n",
       "                           authors    journal_name  \\\n",
       "11573             [s kalyana rama]  Phys.Rev.Lett.   \n",
       "22650  [jm aroca, yuri a kubyshin]                   \n",
       "\n",
       "                                                abstract  \\\n",
       "11573  singular minor change in the acknowledgement o...   \n",
       "22650  to the proceedings of the xiith workshop on hi...   \n",
       "\n",
       "                                             title_lemma    authors_id  \n",
       "11573  [tachyon, split, d, string, black, hole, horiz...       [13234]  \n",
       "22650  [study, wilson, loop, functional, 2d, yang, mi...  [5636, 9242]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a unique index for each author\n",
    "representants_list = list(set(representant_dict.values()))\n",
    "authors2idx = {k: v for v, k in enumerate(representants_list)}\n",
    "information_df[\"authors_id\"] = information_df.authors.apply(lambda x: [authors2idx[auth] for auth in x])\n",
    "\n",
    "information_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_authors_co_auth = create_co_authorship_graph(information_df, authors2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# those embeddings do not depend on the fold\n",
    "abstracts_embeddings = compute_abstracts_embeddings(information_df)\n",
    "if os.path.isfile(f'Data/embeddings/walklets_co_auth_embeddings.pkl'):\n",
    "    walklets_co_auth_embeddings = pickle.load(open('Data/embeddings/walklets_co_auth_embeddings.pkl','rb'))\n",
    "else:\n",
    "    walklets_co_auth_embeddings = compute_walklets(G_authors_co_auth)\n",
    "    pickle.dump(walklets_co_auth_embeddings,open('Data/embeddings/walklets_co_auth_embeddings.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# those embeddings depend on the fold\n",
    "for i in range(number_of_folds):\n",
    "    if os.path.isfile(f'Data/embeddings/articles_walklets_{i+1}.pkl'):\n",
    "        continue\n",
    "    else:\n",
    "        train_set = pd.read_csv(f\"Data/folds/train_set_{i+1}\")\n",
    "        articles_graph = create_articles_graph(train_set,information_df)\n",
    "        authors_citation_graph = create_authors_co_citation_graph(train_set, information_df, authors2idx)\n",
    "\n",
    "        walklets_articles_embeddings = compute_walklets(articles_graph)\n",
    "        walklets_co_citation_embeddings = compute_walklets(authors_citation_graph)\n",
    "\n",
    "        pickle.dump(walklets_articles_embeddings, open(f'Data/embeddings/articles_walklets_{i+1}.pkl','wb'))\n",
    "        pickle.dump(walklets_co_citation_embeddings, open(f'Data/embeddings/co_citation_walklets_{i+1}.pkl','wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_non_embeddings_features(df, information_df, G_articles):\n",
    "    information_df['new_ID'] = information_df.ID.apply(lambda x:id_old2new[x])\n",
    "    useful_information_df = information_df[['new_ID','authors','pub_year', 'title_lemma']]\n",
    "\n",
    "    # prepare data frame for common authors computation\n",
    "    df = (df\n",
    "    .merge(useful_information_df, how ='left', left_on = ['node1'], right_on = ['new_ID'])\n",
    "    .rename(columns = {'authors':'authors_node_1', 'pub_year':'pub_year1', 'title_lemma':'title_lemma1'})\n",
    "    .merge(useful_information_df, how ='left', left_on = ['node2'], right_on = ['new_ID'])\n",
    "    .rename(columns = {'authors':'authors_node_2', 'pub_year':'pub_year2', 'title_lemma':'title_lemma2'})\n",
    "    )\n",
    "\n",
    "    ### Compute page rank\n",
    "    page_rank_dict = nx.pagerank(G_articles)\n",
    "\n",
    "    ### compute degree centrality\n",
    "    centrality_dict = nx.degree_centrality(G_articles)\n",
    "\n",
    "    print(\"common_journal\")\n",
    "    df['common_journals'] = df.apply(lambda x: common_journal(information_df, x.node1, x.node2),axis = 1)\n",
    "    \n",
    "    print('computing common authors')\n",
    "    #  compute common authors\n",
    "    df['common_authors'] = df.apply(lambda x:len(set(x.authors_node_1)&set(x.authors_node_2)),axis = 1)\n",
    "\n",
    "    print('computing common words')\n",
    "    #  compute common words in titles\n",
    "    df['common_title_words'] = df.apply(lambda x:len(set(x.title_lemma1)&set(x.title_lemma2)),axis = 1)\n",
    "\n",
    "    print('computing delta publication year')\n",
    "    # compute delta publication year\n",
    "    df['delta_publication'] = df.apply(lambda x:np.abs(x.pub_year2 - x.pub_year1),axis = 1)\n",
    "\n",
    "    # compute edges features\n",
    "    print('computing jacard index')\n",
    "    df['jacard'] = df.apply(lambda x: Jaccard(G_articles, (x.node1, x.node2)),axis = 1)\n",
    "\n",
    "    print('computing preferential attachement')\n",
    "    df['pa'] = df.apply(lambda x: preferential_attachement(G_articles, (x.node1, x.node2)),axis = 1)\n",
    "\n",
    "    print('computing adamic_adar')\n",
    "    df['adamic_adar'] = df.apply(lambda x: AdamicAdar(G_articles, (x.node1, x.node2)),axis = 1)\n",
    "\n",
    "    print('are connected')\n",
    "    df['connection'] = df.apply(lambda x: are_connected(G_articles, (x.node1, x.node2)),axis = 1)\n",
    "\n",
    "    print('page ranks')\n",
    "    df['page_rank1'] = df.apply(lambda x: page_rank_dict[x.node1],axis = 1)\n",
    "    df['page_rank2'] = df.apply(lambda x: page_rank_dict[x.node2],axis = 1)\n",
    "    \n",
    "    print('compute degree')\n",
    "\n",
    "    df['degree1'] = df.apply(lambda x: centrality_dict[x.node1],axis = 1)\n",
    "    df['degree2'] = df.apply(lambda x: centrality_dict[x.node2],axis = 1)\n",
    "\n",
    "    \n",
    "    df = df.fillna({ 'jacard':df.jacard.mean(),\n",
    "                     'adamic_adar':df.adamic_adar.mean()\n",
    "                     })\n",
    "\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding_features(df,\n",
    "                                information_df,\n",
    "                                abstracts_embeddings,\n",
    "                                walklets_articles_embeddings,\n",
    "                                walklets_co_auth_embeddings,\n",
    "                                walklets_co_citation_embeddings):\n",
    "\n",
    "    # for each article take a mean of the authors embedding as global autors embedding (idem for citation)\n",
    "    articles_authors_embedding = []\n",
    "    articles_authors_embedding_citation = []\n",
    "    for i in range(information_df.shape[0]):\n",
    "        value = information_df[information_df.new_ID == i]\n",
    "        authors_id = value.authors_id\n",
    "        embeddings = np.array([0 for i in range(128)]).astype('float64')\n",
    "        embeddings_citation = np.array([0 for i in range(128)]).astype('float64')\n",
    "        for author in authors_id:\n",
    "            embeddings+=walklets_co_auth_embeddings[author][0]\n",
    "            embeddings_citation+=walklets_co_citation_embeddings[author][0]\n",
    "        articles_authors_embedding.append(embeddings/len(authors_id))\n",
    "        articles_authors_embedding_citation.append(embeddings_citation/len(authors_id))\n",
    "\n",
    "\n",
    "    # compute some cosine and euclidian based distances\n",
    "    df['articles_walklets_cosine'] = df.apply(lambda x:cosine(walklets_articles_embeddings[x.node1],walklets_articles_embeddings[x.node2]), axis = 1)\n",
    "    df['abstracts_embeddings_cosine'] = df.apply(lambda x:cosine(abstracts_embeddings[x.node1][0],abstracts_embeddings[x.node2][0]), axis = 1)\n",
    "    \n",
    "    df['articles_walklets_euclidian'] = df.apply(lambda x:euclidian(walklets_articles_embeddings[x.node1],walklets_articles_embeddings[x.node2]), axis = 1)\n",
    "    df['abstracts_embeddings_euclidian'] = df.apply(lambda x:euclidian(abstracts_embeddings[x.node1][0],abstracts_embeddings[x.node2][0]), axis = 1)\n",
    "    \n",
    "    # compute some cosine and euclidian based distances for authors\n",
    "    df['co_authorship_embeddings_cosine'] = df.apply(lambda x:cosine(articles_authors_embedding[x.node1],articles_authors_embedding[x.node2]), axis = 1)\n",
    "    df['authors_embeddings_cosine_citation'] = df.apply(lambda x:cosine(articles_authors_embedding_citation[x.node1],articles_authors_embedding_citation[x.node2]), axis = 1)    \n",
    "    df['co_authorship_embeddings_euclidian'] = df.apply(lambda x:euclidian(articles_authors_embedding[x.node1],articles_authors_embedding[x.node2]), axis = 1)\n",
    "    df['authors_embeddings_euclidian_citation'] = df.apply(lambda x:euclidian(articles_authors_embedding_citation[x.node1],articles_authors_embedding_citation[x.node2]), axis = 1)\n",
    "\n",
    "    # node1 and node2 article embedding\n",
    "    print(\"add articles embeddings\")\n",
    "    node_embeddings_df = pd.DataFrame(walklets_articles_embeddings, columns = [f'emb_{i}' for i in range(len(walklets_articles_embeddings[0]))])\n",
    "    node_embeddings_df = node_embeddings_df.reset_index().rename(columns = {'index':'node'})\n",
    "    df = (df\n",
    "        .merge(node_embeddings_df, how ='left', left_on = ['node1'], right_on = ['node'])\n",
    "        .drop(columns = ['node'])\n",
    "        .merge(node_embeddings_df, how ='left', left_on = ['node2'], right_on = ['node'])\n",
    "        .drop(columns = ['node'])\n",
    "    )\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Compute features for all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0\n",
      "compute train features\n",
      "common_journal\n"
     ]
    }
   ],
   "source": [
    "for i in range(number_of_folds):\n",
    "    print(f\"fold: {i}\")\n",
    "    if os.path.isfile(f\"Data/processed_data/train_set_features{i+1}.csv\") and os.path.isfile(f\"Data/processed_data/val_set_features{i+1}.csv\") and os.path.isfile(f\"Data/processed_data/test_set_features{i+1}.csv\"):\n",
    "        continue\n",
    "    else:\n",
    "        # load sets\n",
    "        train_set = pd.read_csv(f\"Data/folds/train_set_{i+1}\")\n",
    "        validation_set = pd.read_csv(f\"Data/folds/validation_set_{i+1}\")\n",
    "\n",
    "        # compute graphs\n",
    "        G_articles = create_articles_graph(train_set,information_df)\n",
    "\n",
    "        # load embeddings\n",
    "        walklets_articles_embeddings = pickle.load(open(f'Data/embeddings/articles_walklets_{i+1}.pkl','rb'))\n",
    "        walklets_co_citation_embeddings = pickle.load(open(f'Data/embeddings/co_citation_walklets_{i+1}.pkl','rb'))\n",
    "\n",
    "        # compute features for train\n",
    "        print(\"compute train features\")\n",
    "        train_set_with_features = compute_non_embeddings_features(train_set, information_df, G_articles)\n",
    "        train_set_with_features = compute_embedding_features(train_set_with_features, information_df, abstracts_embeddings,\n",
    "                                                                walklets_articles_embeddings,\n",
    "                                                                walklets_co_auth_embeddings,\n",
    "                                                                walklets_co_citation_embeddings)\n",
    "        # compute features for val\n",
    "        print(\"compute validation features\")\n",
    "        val_set_with_features = compute_non_embeddings_features(validation_set, information_df, G_articles)\n",
    "        val_set_with_features = compute_embedding_features(val_set_with_features, information_df, abstracts_embeddings,\n",
    "                                                                walklets_articles_embeddings,\n",
    "                                                                walklets_co_auth_embeddings,\n",
    "                                                                walklets_co_citation_embeddings)\n",
    "\n",
    "        # compute features for test\n",
    "        print(\"compute test features\")\n",
    "        test_set_with_features = compute_non_embeddings_features(test_set, information_df, G_articles)\n",
    "        test_set_with_features = compute_embedding_features(test_set_with_features, information_df, abstracts_embeddings,\n",
    "                                                                walklets_articles_embeddings,\n",
    "                                                                walklets_co_auth_embeddings,\n",
    "                                                                walklets_co_citation_embeddings)\n",
    "\n",
    "        train_set_with_features.to_csv(f\"Data/processed_data/train_set_features{i+1}.csv\", index = False)\n",
    "        val_set_with_features.to_csv(f\"Data/processed_data/val_set_features{i+1}.csv\", index = False)\n",
    "        test_set_with_features.to_csv(f\"Data/processed_data/test_set_features{i+1}.csv\", index = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute columns of interest\n",
    "all_columns = set(pd.read_csv(\"Data/processed_data/train_set_features1.csv\").comumns)\n",
    "\n",
    "to_remove = set(['node1', 'node2', 'label', 'new_ID_x', 'authors_node_1', 'pub_year1',\n",
    "       'title_lemma1', 'new_ID_y', 'authors_node_2', 'pub_year2',\n",
    "       'title_lemma2'])\n",
    "columns_to_keep= list(all_columns-to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'ntpath' has no attribute 'is_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(number_of_folds):\n\u001b[1;32m----> 2\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mis_file(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mData/models/clf_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.pkl\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m      3\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'ntpath' has no attribute 'is_file'"
     ]
    }
   ],
   "source": [
    "for i in range(number_of_folds):\n",
    "    if os.path.is_file(f\"Data/models/clf_{i+1}.pkl\"):\n",
    "        continue\n",
    "    else:\n",
    "        train_set_with_features = pd.read_csv(f\"Data/processed_data/train_set_features{i+1}.csv\")\n",
    "        validation_set_with_features = pd.read_csv(f\"Data/processed_data/val_set_features{i+1}.csv\")\n",
    "        \n",
    "        # only keep columns of interest\n",
    "        train_set_with_features = train_set_with_features[columns_to_keep+['label']]\n",
    "        validation_set_with_features = validation_set_with_features[columns_to_keep+['label']]\n",
    "\n",
    "        train_samples, train_labels = train_set_with_features.drop(columns = ['label']), train_set_with_features[['label']]\n",
    "        validation_samples, validation_labels = validation_set_with_features.drop(columns = ['label']), validation_set_with_features[['label']]\n",
    "\n",
    "        # scale data\n",
    "        scaler = StandardScaler()\n",
    "        train_samples_scaled = scaler.fit_transform(np.float32(train_samples))\n",
    "        validation_samples_scaled = scaler.transform(np.float32(validation_samples))\n",
    "\n",
    "\n",
    "        # train classifier (grid search best params)\n",
    "        clf = get_best_xgb(train_samples_scaled, train_labels, validation_samples_scaled, validation_labels)\n",
    "        \n",
    "        pickle.dump(clf, open(f\"Data/models/clf_{i+1}.pkl\", 'wb'))\n",
    "        pickle.dump(scaler, open(f\"Data/models/scaler_{i+1}.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common_journal\n",
      "computing common authors\n",
      "computing common words\n",
      "computing delta publication year\n",
      "computing jacard index\n",
      "computing preferential attachement\n",
      "computing adamic_adar\n",
      "are connected\n",
      "page ranks\n",
      "compute degree\n"
     ]
    }
   ],
   "source": [
    "preds = None\n",
    "for i in range(number_of_folds):\n",
    "    \n",
    "    test_set_with_features = pd.read_csv(f\"Data/processed_data/test_set_features{i+1}.csv\")\n",
    "\n",
    "    # scale data\n",
    "    scaler = pickle.load(open(f\"Data/models/scaler_{i+1}.pkl\", 'rb'))\n",
    "\n",
    "    # only keep columns of interest\n",
    "    test_set_with_features = test_set_with_features[columns_to_keep]\n",
    "\n",
    "    test_set_with_features = scaler.fit_transform(np.float32(test_set_with_features))\n",
    "\n",
    "\n",
    "    # train classifier (grid search best params)\n",
    "    clf = pickle.load(open(f\"Data/models/clf_{i+1}.pkl\", 'rb'))\n",
    "\n",
    "    if preds == None:\n",
    "        preds = np.array(clf.predict(test_set_with_features))\n",
    "    else:\n",
    "        preds += np.array(clf.predict(test_set_with_features))\n",
    "\n",
    "# voting \n",
    "final_preds = (preds>=(number_of_folds/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['category'] = final_preds\n",
    "\n",
    "test_set = (test_set\n",
    ".reset_index()\n",
    ".rename(columns = {'index':'id'})\n",
    ".drop(columns = ['node1','node2'])\n",
    ")\n",
    "\n",
    "test_set.to_csv('final_predictions_no_emb.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6657d8cf73e4192045730bbde1f7a947d4725a27f96025bfcb1ab47bc67665b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
