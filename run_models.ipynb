{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: OMP_NUM_THREADS=None =>\n",
      "... If you are using openblas if you are using openblas set OMP_NUM_THREADS=1 or risk subprocess calls hanging indefinitely\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\33631\\AppData\\Local\\Temp\\ipykernel_61576\\934934288.py:26: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
      "  from scipy.stats.stats import pearsonr\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import os\n",
    "import networkx as nx\n",
    "from gensim.models import Word2Vec\n",
    "from node2vec import Node2Vec\n",
    "import pickle\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from hpsklearn import HyperoptEstimator, any_classifier, any_preprocessing\n",
    "from hyperopt import tpe\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score \n",
    "from sklearn.preprocessing import LabelBinarizer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from link_pred.utils import cosine, euclidian, retrieve_and_pre_processed_informations, compute_unique_names\n",
    "from link_pred.folds_creation import create_and_save_folds\n",
    "from link_pred.create_graphs import create_articles_graph, create_co_authorship_graph, create_authors_co_citation_graph\n",
    "from link_pred.node_embeddings import compute_abstracts_embeddings, compute_titles_embeddings, compute_walklets, compute_node2vec, compute_deep_walks\n",
    "from link_pred.edges_features import Jaccard, AdamicAdar, preferential_attachement, are_connected, common_journal\n",
    "from link_pred.models_training import get_best_xgb, get_xgb, get_best_MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_path = \"Data/raw_data/node_information.csv\"\n",
    "test_set_path = \"Data/raw_data/testing_set.txt\"\n",
    "train_set_path = \"Data/raw_data/training_set.txt\"\n",
    "random_preds_path = \"Data/raw_data/random_predictions.csv\"\n",
    "\n",
    "number_of_folds = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and pre_process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_df = retrieve_and_pre_processed_informations(information_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign new id to each node (needs to start from 0)\n",
    "id_old2new = {k: v for v, k in enumerate(list(information_df.ID))}\n",
    "id_new2old = {v: k for v, k in id_old2new.items()}\n",
    "\n",
    "information_df['new_ID'] = information_df.ID.apply(lambda x: id_old2new[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_train_set = pd.read_csv(train_set_path, sep =\" \", header = None)\n",
    "initial_train_set.columns = ['node1','node2','label']\n",
    "\n",
    "## update nodes values to new indices\n",
    "initial_train_set.node1 = initial_train_set.apply(lambda x:id_old2new[x.node1], axis = 1)\n",
    "initial_train_set.node2 = initial_train_set.apply(lambda x:id_old2new[x.node2], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load test set\n",
    "\n",
    "test_set = pd.read_csv(test_set_path, sep =\" \", header = None)\n",
    "test_set.columns = ['node1','node2']\n",
    "\n",
    "## update nodes values to new indices\n",
    "test_set.node1 = test_set.apply(lambda x:id_old2new[x.node1], axis = 1)\n",
    "test_set.node2 = test_set.apply(lambda x:id_old2new[x.node2], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_1 created and saved !\n",
      "fold_2 created and saved !\n",
      "fold_3 created and saved !\n"
     ]
    }
   ],
   "source": [
    "create_and_save_folds(initial_train_set, number_of_folds = number_of_folds, validation_size = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with authors various names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# convert to lower case, remove punctuation, strip the names\n",
    "authors_raw_set = set([auth.strip().lower().translate(str.maketrans('', '', string.punctuation)) for list_auth in information_df.authors for auth in list_auth if len(auth)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# several authors can be named differently (eg. Jean DUPONT, J.Dupont, etc.)\n",
    "# we create a name matcher function to try to indentify each author and assign each denomination a \"representant\"\n",
    "\n",
    "if os.path.isfile('Data/processed_data/representant_dict.pkl'):\n",
    "    representant_dict = pickle.load(open('Data/processed_data/representant_dict.pkl','rb'))\n",
    "else:\n",
    "    representant_dict = compute_unique_names(authors_raw_set)\n",
    "    pickle.dump(representant_dict, open('Data/processed_data/representant_dict.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set each name to its representant value\n",
    "information_df.authors = information_df.authors.apply(lambda x: [representant_dict[auth.strip().lower().translate(str.maketrans('', '', string.punctuation))] for auth in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>pub_year</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title_lemma</th>\n",
       "      <th>new_ID</th>\n",
       "      <th>authors_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11267</th>\n",
       "      <td>9211116</td>\n",
       "      <td>1992</td>\n",
       "      <td>cpt strings and the k bar k system</td>\n",
       "      <td>[alan kostelecky, robertus potting]</td>\n",
       "      <td></td>\n",
       "      <td>this talk contains a summary of our work on dy...</td>\n",
       "      <td>[cpt, string, k, bar, k, system]</td>\n",
       "      <td>11267</td>\n",
       "      <td>[2579, 7544]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4198</th>\n",
       "      <td>105216</td>\n",
       "      <td>2001</td>\n",
       "      <td>d-branes and vector bundles on calabi-yau mani...</td>\n",
       "      <td>[suresh govindarajan iitm, t jayaraman imsc]</td>\n",
       "      <td></td>\n",
       "      <td>helix we review some recent results on d-brane...</td>\n",
       "      <td>[d, brane, vector, bundle, calabi, yau, manifold]</td>\n",
       "      <td>4198</td>\n",
       "      <td>[9722, 14201]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  pub_year                                              title  \\\n",
       "11267  9211116      1992                 cpt strings and the k bar k system   \n",
       "4198    105216      2001  d-branes and vector bundles on calabi-yau mani...   \n",
       "\n",
       "                                            authors journal_name  \\\n",
       "11267           [alan kostelecky, robertus potting]                \n",
       "4198   [suresh govindarajan iitm, t jayaraman imsc]                \n",
       "\n",
       "                                                abstract  \\\n",
       "11267  this talk contains a summary of our work on dy...   \n",
       "4198   helix we review some recent results on d-brane...   \n",
       "\n",
       "                                             title_lemma  new_ID  \\\n",
       "11267                   [cpt, string, k, bar, k, system]   11267   \n",
       "4198   [d, brane, vector, bundle, calabi, yau, manifold]    4198   \n",
       "\n",
       "          authors_id  \n",
       "11267   [2579, 7544]  \n",
       "4198   [9722, 14201]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a unique index for each author\n",
    "representants_list = list(set(representant_dict.values()))\n",
    "authors2idx = {k: v for v, k in enumerate(representants_list)}\n",
    "information_df[\"authors_id\"] = information_df.authors.apply(lambda x: [authors2idx[auth] for auth in x])\n",
    "\n",
    "information_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_authors_co_auth = create_co_authorship_graph(information_df, authors2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# those embeddings do not depend on the fold\n",
    "abstracts_embeddings = compute_abstracts_embeddings(information_df)\n",
    "abstracts_embeddings = compute_titles_embeddings(information_df)\n",
    "\n",
    "\n",
    "if os.path.isfile(f'Data/embeddings/walklets_co_auth_embeddings.pkl'):\n",
    "    walklets_co_auth_embeddings = pickle.load(open('Data/embeddings/walklets_co_auth_embeddings.pkl','rb'))\n",
    "else:\n",
    "    walklets_co_auth_embeddings = compute_walklets(G_authors_co_auth)\n",
    "    pickle.dump(walklets_co_auth_embeddings,open('Data/embeddings/walklets_co_auth_embeddings.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1\n",
      "fold: 2\n",
      "fold: 3\n",
      "fold: 4\n",
      "fold: 5\n"
     ]
    }
   ],
   "source": [
    "# those embeddings depend on the fold\n",
    "for i in range(number_of_folds):\n",
    "    print(f\"fold: {i+1}\")\n",
    "\n",
    "    train_set = pd.read_csv(f\"Data/folds/train_set_{i+1}\")\n",
    "    articles_graph = create_articles_graph(train_set,information_df)\n",
    "    authors_citation_graph = create_authors_co_citation_graph(train_set, information_df, authors2idx)\n",
    "\n",
    "    if os.path.isfile(f'Data/embeddings/articles_walklets_{i+1}.pkl') == False:\n",
    "        walklets_articles_embeddings = compute_walklets(articles_graph)\n",
    "        pickle.dump(walklets_articles_embeddings, open(f'Data/embeddings/articles_walklets_{i+1}.pkl','wb'))\n",
    "    if os.path.isfile(f'Data/embeddings/articles_node2vec_{i+1}.pkl') == False:\n",
    "        node2vec_articles_embeddings = compute_node2vec(articles_graph)\n",
    "        pickle.dump(node2vec_articles_embeddings, open(f'Data/embeddings/articles_node2vec_{i+1}.pkl','wb'))\n",
    "    if os.path.isfile(f'Data/embeddings/articles_walklets_{i+1}.pkl') == False:\n",
    "        walklets_co_citation_embeddings = compute_walklets(authors_citation_graph)\n",
    "        pickle.dump(walklets_co_citation_embeddings, open(f'Data/embeddings/co_citation_walklets_{i+1}.pkl','wb'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_non_embeddings_features(df, information_df, G_articles):\n",
    "    information_df['new_ID'] = information_df.ID.apply(lambda x:id_old2new[x])\n",
    "    useful_information_df = information_df[['new_ID','authors','pub_year', 'title_lemma']]\n",
    "\n",
    "    # prepare data frame for common authors computation\n",
    "    df = (df\n",
    "    .merge(useful_information_df, how ='left', left_on = ['node1'], right_on = ['new_ID'])\n",
    "    .rename(columns = {'authors':'authors_node_1', 'pub_year':'pub_year1', 'title_lemma':'title_lemma1'})\n",
    "    .merge(useful_information_df, how ='left', left_on = ['node2'], right_on = ['new_ID'])\n",
    "    .rename(columns = {'authors':'authors_node_2', 'pub_year':'pub_year2', 'title_lemma':'title_lemma2'})\n",
    "    )\n",
    "\n",
    "    ### Compute page rank\n",
    "    page_rank_dict = nx.pagerank(G_articles)\n",
    "\n",
    "    ### compute degree centrality\n",
    "    centrality_dict = nx.degree_centrality(G_articles)\n",
    "\n",
    "    print(\"compute ressource allocation index\")\n",
    "    df['ressource_allocation_index'] = df.apply(lambda x: nx.resource_allocation_index(G_articles, [(x.node1, x.node2)])[0],axis = 1)\n",
    "\n",
    "    print(\"common_journal\")\n",
    "    df['common_journals'] = df.apply(lambda x: common_journal(information_df, x.node1, x.node2),axis = 1)\n",
    "    \n",
    "    print('computing common authors')\n",
    "    #  compute common authors\n",
    "    df['common_authors'] = df.apply(lambda x:len(set(x.authors_node_1)&set(x.authors_node_2)),axis = 1)\n",
    "\n",
    "    print('computing common words')\n",
    "    #  compute common words in titles\n",
    "    df['common_title_words'] = df.apply(lambda x:len(set(x.title_lemma1)&set(x.title_lemma2)),axis = 1)\n",
    "\n",
    "    print('computing delta publication year')\n",
    "    # compute delta publication year\n",
    "    df['delta_publication'] = df.apply(lambda x:np.abs(x.pub_year2 - x.pub_year1),axis = 1)\n",
    "\n",
    "    # compute edges features\n",
    "    print('computing jacard index')\n",
    "    df['jacard'] = df.apply(lambda x: Jaccard(G_articles, (x.node1, x.node2)),axis = 1)\n",
    "\n",
    "    print('computing preferential attachement')\n",
    "    df['pa'] = df.apply(lambda x: preferential_attachement(G_articles, (x.node1, x.node2)),axis = 1)\n",
    "\n",
    "    print('computing adamic_adar')\n",
    "    df['adamic_adar'] = df.apply(lambda x: AdamicAdar(G_articles, (x.node1, x.node2)),axis = 1)\n",
    "\n",
    "    print('are connected')\n",
    "    df['connection'] = df.apply(lambda x: are_connected(G_articles, (x.node1, x.node2)),axis = 1)\n",
    "\n",
    "    print('page ranks')\n",
    "    df['page_rank1'] = df.apply(lambda x: page_rank_dict[x.node1],axis = 1)\n",
    "    df['page_rank2'] = df.apply(lambda x: page_rank_dict[x.node2],axis = 1)\n",
    "    \n",
    "    print('compute degree')\n",
    "\n",
    "    df['degree1'] = df.apply(lambda x: centrality_dict[x.node1],axis = 1)\n",
    "    df['degree2'] = df.apply(lambda x: centrality_dict[x.node2],axis = 1)\n",
    "\n",
    "    \n",
    "    df = df.fillna({ 'jacard':df.jacard.mean(),\n",
    "                     'adamic_adar':df.adamic_adar.mean()\n",
    "                     })\n",
    "\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding_features(df,\n",
    "                                information_df,\n",
    "                                abstracts_embeddings,\n",
    "                                walklets_articles_embeddings,\n",
    "                                walklets_co_auth_embeddings,\n",
    "                                walklets_co_citation_embeddings,\n",
    "                                node2vec_articles_embeddings,\n",
    "                                pca = False):\n",
    "\n",
    "    # for each article take a mean of the authors embedding as global autors embedding (idem for citation)\n",
    "    articles_authors_embedding = []\n",
    "    articles_authors_embedding_citation = []\n",
    "    for i in range(information_df.shape[0]):\n",
    "        value = information_df[information_df.new_ID == i]\n",
    "        authors_id = value.authors_id\n",
    "        embeddings = np.array([0 for i in range(128)]).astype('float64')\n",
    "        embeddings_citation = np.array([0 for i in range(128)]).astype('float64')\n",
    "        for author in authors_id:\n",
    "            embeddings+=walklets_co_auth_embeddings[author][0]\n",
    "            embeddings_citation+=walklets_co_citation_embeddings[author][0]\n",
    "        articles_authors_embedding.append(embeddings/len(authors_id))\n",
    "        articles_authors_embedding_citation.append(embeddings_citation/len(authors_id))\n",
    "\n",
    "\n",
    "    # compute some cosine and euclidian based distances\n",
    "    df['articles_walklets_cosine'] = df.apply(lambda x:cosine(walklets_articles_embeddings[x.node1],walklets_articles_embeddings[x.node2]), axis = 1)\n",
    "    df['articles_node2vec_cosine'] = df.apply(lambda x:cosine(node2vec_articles_embeddings[x.node1],node2vec_articles_embeddings[x.node2]), axis = 1)\n",
    "    df['abstracts_embeddings_cosine'] = df.apply(lambda x:cosine(abstracts_embeddings[x.node1][0],abstracts_embeddings[x.node2][0]), axis = 1)\n",
    "    \n",
    "    df['articles_walklets_euclidian'] = df.apply(lambda x:euclidian(walklets_articles_embeddings[x.node1],walklets_articles_embeddings[x.node2]), axis = 1)\n",
    "    df['articles_node2vec_euclidian'] = df.apply(lambda x:euclidian(node2vec_articles_embeddings[x.node1],node2vec_articles_embeddings[x.node2]), axis = 1)\n",
    "    df['abstracts_embeddings_euclidian'] = df.apply(lambda x:euclidian(abstracts_embeddings[x.node1][0],abstracts_embeddings[x.node2][0]), axis = 1)\n",
    "    \n",
    "    # compute some cosine and euclidian based distances for authors\n",
    "    df['co_authorship_embeddings_cosine'] = df.apply(lambda x:cosine(articles_authors_embedding[x.node1],articles_authors_embedding[x.node2]), axis = 1)\n",
    "    df['authors_embeddings_cosine_citation'] = df.apply(lambda x:cosine(articles_authors_embedding_citation[x.node1],articles_authors_embedding_citation[x.node2]), axis = 1)    \n",
    "    df['co_authorship_embeddings_euclidian'] = df.apply(lambda x:euclidian(articles_authors_embedding[x.node1],articles_authors_embedding[x.node2]), axis = 1)\n",
    "    df['authors_embeddings_euclidian_citation'] = df.apply(lambda x:euclidian(articles_authors_embedding_citation[x.node1],articles_authors_embedding_citation[x.node2]), axis = 1)\n",
    "\n",
    "    # node1 and node2 article embedding\n",
    "    print(\"add articles walklets embeddings\")\n",
    "    # only append vectors of size 10 that represent articles embeddings (quicker to compute)\n",
    "    if pca:\n",
    "        pca_walklets= PCA(n_components = 5)\n",
    "        walklets_articles_embeddings = pca_walklets.fit_transform(walklets_articles_embeddings)\n",
    "        pca_node2vec =  PCA(n_components = 5)\n",
    "        node2vec_articles_embeddings = pca_node2vec.fit_transform(node2vec_articles_embeddings)\n",
    "\n",
    "\n",
    "    walklets_node_embeddings_df = pd.DataFrame(walklets_articles_embeddings, columns = [f'emb_walklets_{i}' for i in range(len(walklets_articles_embeddings[0]))])\n",
    "    walklets_node_embeddings_df = walklets_node_embeddings_df.reset_index().rename(columns = {'index':'node'})\n",
    "    df = (df\n",
    "        .merge(walklets_node_embeddings_df, how ='left', left_on = ['node1'], right_on = ['node'])\n",
    "        .drop(columns = ['node'])\n",
    "        .merge(walklets_node_embeddings_df, how ='left', left_on = ['node2'], right_on = ['node'])\n",
    "        .drop(columns = ['node'])\n",
    "    )\n",
    "\n",
    "    print(\"add articles node2vecs embeddings\")\n",
    "    # only append vectors of size 10 that represent articles embeddings (quicker to compute)\n",
    "\n",
    "    node_node2vec_embeddings_df = pd.DataFrame(node2vec_articles_embeddings, columns = [f'emb_node2vec{i}' for i in range(len(node2vec_articles_embeddings[0]))])\n",
    "    node_node2vec_embeddings_df = node_node2vec_embeddings_df.reset_index().rename(columns = {'index':'node'})\n",
    "    df = (df\n",
    "        .merge(node_node2vec_embeddings_df, how ='left', left_on = ['node1'], right_on = ['node'])\n",
    "        .drop(columns = ['node'])\n",
    "        .merge(node_node2vec_embeddings_df, how ='left', left_on = ['node2'], right_on = ['node'])\n",
    "        .drop(columns = ['node'])\n",
    "    )\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Compute features for all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1\n",
      "fold: 2\n",
      "fold: 3\n",
      "fold: 4\n",
      "fold: 5\n"
     ]
    }
   ],
   "source": [
    "for i in range(number_of_folds):\n",
    "    print(f\"fold: {i+1}\")\n",
    "    if os.path.isfile(f\"Data/processed_data/train_set_features{i+1}.csv\") and os.path.isfile(f\"Data/processed_data/val_set_features{i+1}.csv\") and os.path.isfile(f\"Data/processed_data/test_set_features{i+1}.csv\"):\n",
    "        continue\n",
    "    else:\n",
    "        # load sets\n",
    "        train_set = pd.read_csv(f\"Data/folds/train_set_{i+1}\")\n",
    "        validation_set = pd.read_csv(f\"Data/folds/validation_set_{i+1}\")\n",
    "\n",
    "        # compute graphs\n",
    "        G_articles = create_articles_graph(train_set,information_df)\n",
    "\n",
    "        # load embeddings\n",
    "        walklets_articles_embeddings = pickle.load(open(f'Data/embeddings/articles_walklets_{i+1}.pkl','rb'))\n",
    "        walklets_co_citation_embeddings = pickle.load(open(f'Data/embeddings/co_citation_walklets_{i+1}.pkl','rb'))\n",
    "        node2vec_articles_embeddings = pickle.load(open(f'Data/embeddings/articles_node2vec_{i+1}.pkl','rb'))\n",
    "\n",
    "        # compute features for train\n",
    "        print(\"compute train features\")\n",
    "        train_set_with_features = compute_non_embeddings_features(train_set, information_df, G_articles)\n",
    "        train_set_with_features = compute_embedding_features(train_set_with_features, information_df, abstracts_embeddings,\n",
    "                                                                walklets_articles_embeddings,\n",
    "                                                                walklets_co_auth_embeddings,\n",
    "                                                                walklets_co_citation_embeddings,\n",
    "                                                                node2vec_articles_embeddings)\n",
    "        # compute features for val\n",
    "        print(\"compute validation features\")\n",
    "        val_set_with_features = compute_non_embeddings_features(validation_set, information_df, G_articles)\n",
    "        val_set_with_features = compute_embedding_features(val_set_with_features, information_df, abstracts_embeddings,\n",
    "                                                                walklets_articles_embeddings,\n",
    "                                                                walklets_co_auth_embeddings,\n",
    "                                                                walklets_co_citation_embeddings,\n",
    "                                                                node2vec_articles_embeddings)\n",
    "\n",
    "        # compute features for test\n",
    "        print(\"compute test features\")\n",
    "        test_set_with_features = compute_non_embeddings_features(test_set, information_df, G_articles)\n",
    "        test_set_with_features = compute_embedding_features(test_set_with_features, information_df, abstracts_embeddings,\n",
    "                                                                walklets_articles_embeddings,\n",
    "                                                                walklets_co_auth_embeddings,\n",
    "                                                                walklets_co_citation_embeddings,\n",
    "                                                                node2vec_articles_embeddings)\n",
    "\n",
    "        train_set_with_features.to_csv(f\"Data/processed_data/train_set_features{i+1}.csv\", index = False)\n",
    "        val_set_with_features.to_csv(f\"Data/processed_data/val_set_features{i+1}.csv\", index = False)\n",
    "        test_set_with_features.to_csv(f\"Data/processed_data/test_set_features{i+1}.csv\", index = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute columns of interest\n",
    "all_columns = set(pd.read_csv(\"Data/processed_data/train_set_features1.csv\").columns)\n",
    "\n",
    "to_remove = set(['node1', 'node2', 'label', 'new_ID_x', 'authors_node_1',\n",
    "       'title_lemma1', 'new_ID_y', 'authors_node_2', 'title_lemma2'])\n",
    "columns_to_keep= list(all_columns-to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1\n",
      "fold: 2\n",
      "fold: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\33631\\Documents\\Etudes\\centrale 3A\\MLNS\\MLNS_Kaggle_Challenge\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-aucpr:0.99004\n",
      "[1]\tvalidation_0-aucpr:0.99064\n",
      "[2]\tvalidation_0-aucpr:0.99125\n",
      "[3]\tvalidation_0-aucpr:0.99198\n",
      "[4]\tvalidation_0-aucpr:0.99202\n",
      "[5]\tvalidation_0-aucpr:0.99207\n",
      "[6]\tvalidation_0-aucpr:0.99216\n",
      "[7]\tvalidation_0-aucpr:0.99218\n",
      "[8]\tvalidation_0-aucpr:0.99233\n",
      "[9]\tvalidation_0-aucpr:0.99242\n",
      "[10]\tvalidation_0-aucpr:0.99246\n",
      "[11]\tvalidation_0-aucpr:0.99277\n",
      "[12]\tvalidation_0-aucpr:0.99299\n",
      "[13]\tvalidation_0-aucpr:0.99288\n",
      "[14]\tvalidation_0-aucpr:0.99289\n",
      "xgb\n",
      "accuracy: 0.963931762794476\n",
      "f1: 0.9667166416791604\n",
      "[13:40:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "xgb simple\n",
      "accuracy: 0.9603899268887084\n",
      "f1: 0.9637018729714439\n",
      "accuracy: 0.9648090982940699\n",
      "f1: 0.9676010410745803\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_neural_net() got an unexpected keyword argument 'gpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\33631\\Documents\\Etudes\\centrale 3A\\MLNS\\MLNS_Kaggle_Challenge\\run_models.ipynb Cell 37'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/run_models.ipynb#ch0000033?line=29'>30</a>\u001b[0m clf_xgb, thresh \u001b[39m=\u001b[39m get_xgb_simple(train_samples_scaled, \u001b[39mlist\u001b[39m(train_labels\u001b[39m.\u001b[39mlabel), validation_samples_scaled, \u001b[39mlist\u001b[39m(validation_labels\u001b[39m.\u001b[39mlabel),gpu \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, verbose \u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/run_models.ipynb#ch0000033?line=30'>31</a>\u001b[0m clf_xgb, thresh \u001b[39m=\u001b[39m get_MLP(train_samples_scaled, \u001b[39mlist\u001b[39m(train_labels\u001b[39m.\u001b[39mlabel), validation_samples_scaled, \u001b[39mlist\u001b[39m(validation_labels\u001b[39m.\u001b[39mlabel),gpu \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, verbose \u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/run_models.ipynb#ch0000033?line=31'>32</a>\u001b[0m clf_xgb, thresh \u001b[39m=\u001b[39m get_neural_net(train_samples_scaled, \u001b[39mlist\u001b[39;49m(train_labels\u001b[39m.\u001b[39;49mlabel), validation_samples_scaled, \u001b[39mlist\u001b[39;49m(validation_labels\u001b[39m.\u001b[39;49mlabel),gpu \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m, verbose \u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/run_models.ipynb#ch0000033?line=38'>39</a>\u001b[0m \u001b[39m#preds_xgb = np.array(np.int32(clf_xgb.predict_proba(test_set_with_features)[:,1] >= thresh))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/run_models.ipynb#ch0000033?line=39'>40</a>\u001b[0m \u001b[39m#clf = get_neural_net(train_samples_scaled, list(train_labels.label), validation_samples_scaled, list(validation_labels.label))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/run_models.ipynb#ch0000033?line=40'>41</a>\u001b[0m clf\u001b[39m.\u001b[39meval()\n",
      "\u001b[1;31mTypeError\u001b[0m: get_neural_net() got an unexpected keyword argument 'gpu'"
     ]
    }
   ],
   "source": [
    "for i in range(number_of_folds):\n",
    "    print(f'fold: {i+1}')\n",
    "    if os.path.isfile(f\"Data/models/scaler_{i+1}.pkl\"):\n",
    "        continue\n",
    "    else:\n",
    "        train_set_with_features = pd.read_csv(f\"Data/processed_data/train_set_features{i+1}.csv\")\n",
    "        validation_set_with_features = pd.read_csv(f\"Data/processed_data/val_set_features{i+1}.csv\")\n",
    "\n",
    "        # only keep columns of interest\n",
    "        train_set_with_features = train_set_with_features[columns_to_keep+['label']]\n",
    "        validation_set_with_features = validation_set_with_features[columns_to_keep+['label']]\n",
    "\n",
    "        train_samples, train_labels = train_set_with_features.drop(columns = ['label']), train_set_with_features[['label']]\n",
    "        validation_samples, validation_labels = validation_set_with_features.drop(columns = ['label']), validation_set_with_features[['label']]\n",
    "        \n",
    "\n",
    "        # scale data\n",
    "        scaler = StandardScaler()\n",
    "        train_samples_scaled = scaler.fit_transform(np.float32(train_samples))\n",
    "        validation_samples_scaled = scaler.fit_transform(np.float32(validation_samples))\n",
    "\n",
    "\n",
    "        # train classifier (grid search best params)\n",
    "        clf_xgb, thresh_xgb = get_best_xgb(train_samples_scaled, list(train_labels.label), validation_samples_scaled, list(validation_labels.label),gpu = True, verbose =1)\n",
    "        clf_mlp, thresh_mlp = get_best_MLP(train_samples_scaled, list(train_labels.label), validation_samples_scaled, list(validation_labels.label))\n",
    "     \n",
    "    \n",
    "        pickle.dump((clf_xgb,thresh_xgb), open(f\"Data/models/clf_xgb_{i+1}.pkl\", 'wb'))\n",
    "        pickle.dump((clf_mlp,thresh_mlp), open(f\"Data/models/clf_mlp_{i+1}.pkl\", 'wb'))\n",
    "        pickle.dump(scaler, open(f\"Data/models/scaler_{i+1}.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array([])\n",
    "for i in range(number_of_folds):\n",
    "    \n",
    "    test_set_with_features = pd.read_csv(f\"Data/processed_data/test_set_features{i+1}.csv\")\n",
    "\n",
    "    # scale data\n",
    "    scaler = pickle.load(open(f\"Data/models/scaler_{i+1}.pkl\", 'rb'))\n",
    "\n",
    "    # only keep columns of interest\n",
    "    test_set_with_features = test_set_with_features[columns_to_keep]\n",
    "\n",
    "    test_set_with_features = scaler.transform(np.float32(test_set_with_features))\n",
    "\n",
    "\n",
    "    # train classifier (grid search best params)\n",
    "    clf, thresh = pickle.load(open(f\"Data/models/clf_{i+1}.pkl\", 'rb'))\n",
    "\n",
    "\n",
    "    if preds.shape[0]==0:\n",
    "        preds = np.array(np.int32(clf.predict_proba(test_set_with_features)[:,1] >= thresh))\n",
    "    else:\n",
    "        preds += np.array(np.int32(clf.predict_proba(test_set_with_features)[:,1] >= thresh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voting \n",
    "test_set['category'] = preds\n",
    "\n",
    "test_set = (test_set\n",
    ".reset_index()\n",
    ".rename(columns = {'index':'id'})\n",
    ".drop(columns = ['node1','node2'])\n",
    ")\n",
    "\n",
    "test_set.to_csv('final_predictions_no_emb.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6657d8cf73e4192045730bbde1f7a947d4725a27f96025bfcb1ab47bc67665b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
