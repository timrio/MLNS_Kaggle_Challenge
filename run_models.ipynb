{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: OMP_NUM_THREADS=None =>\n",
      "... If you are using openblas if you are using openblas set OMP_NUM_THREADS=1 or risk subprocess calls hanging indefinitely\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\33631\\AppData\\Local\\Temp\\ipykernel_188596\\934934288.py:26: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
      "  from scipy.stats.stats import pearsonr\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import os\n",
    "import networkx as nx\n",
    "from gensim.models import Word2Vec\n",
    "from node2vec import Node2Vec\n",
    "import pickle\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from hpsklearn import HyperoptEstimator, any_classifier, any_preprocessing\n",
    "from hyperopt import tpe\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score \n",
    "from sklearn.preprocessing import LabelBinarizer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from link_pred.utils import cosine, euclidian, retrieve_and_pre_processed_informations, compute_unique_names\n",
    "from link_pred.folds_creation import create_and_save_folds\n",
    "from link_pred.create_graphs import create_articles_graph, create_co_authorship_graph, create_authors_co_citation_graph\n",
    "from link_pred.node_embeddings import compute_abstracts_embeddings, compute_walklets, compute_node2vec, compute_deep_walks\n",
    "from link_pred.edges_features import Jaccard, AdamicAdar, preferential_attachement, are_connected, common_journal\n",
    "from link_pred.grid_search import get_best_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_path = \"Data/raw_data/node_information.csv\"\n",
    "test_set_path = \"Data/raw_data/testing_set.txt\"\n",
    "train_set_path = \"Data/raw_data/training_set.txt\"\n",
    "random_preds_path = \"Data/raw_data/random_predictions.csv\"\n",
    "\n",
    "number_of_folds = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and pre_process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_df = retrieve_and_pre_processed_informations(information_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign new id to each node (needs to start from 0)\n",
    "id_old2new = {k: v for v, k in enumerate(list(information_df.ID))}\n",
    "id_new2old = {v: k for v, k in id_old2new.items()}\n",
    "\n",
    "information_df['new_ID'] = information_df.ID.apply(lambda x: id_old2new[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_train_set = pd.read_csv(train_set_path, sep =\" \", header = None)\n",
    "initial_train_set.columns = ['node1','node2','label']\n",
    "\n",
    "## update nodes values to new indices\n",
    "initial_train_set.node1 = initial_train_set.apply(lambda x:id_old2new[x.node1], axis = 1)\n",
    "initial_train_set.node2 = initial_train_set.apply(lambda x:id_old2new[x.node2], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load test set\n",
    "\n",
    "test_set = pd.read_csv(test_set_path, sep =\" \", header = None)\n",
    "test_set.columns = ['node1','node2']\n",
    "\n",
    "## update nodes values to new indices\n",
    "test_set.node1 = test_set.apply(lambda x:id_old2new[x.node1], axis = 1)\n",
    "test_set.node2 = test_set.apply(lambda x:id_old2new[x.node2], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1 already exists !\n",
      "fold 2 already exists !\n",
      "fold 3 already exists !\n",
      "fold 4 already exists !\n",
      "fold 5 already exists !\n"
     ]
    }
   ],
   "source": [
    "create_and_save_folds(initial_train_set, number_of_folds = number_of_folds, validation_size = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with authors various names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# convert to lower case, remove punctuation, strip the names\n",
    "authors_raw_set = set([auth.strip().lower().translate(str.maketrans('', '', string.punctuation)) for list_auth in information_df.authors for auth in list_auth if len(auth)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# several authors can be named differently (eg. Jean DUPONT, J.Dupont, etc.)\n",
    "# we create a name matcher function to try to indentify each author and assign each denomination a \"representant\"\n",
    "\n",
    "if os.path.isfile('Data/processed_data/representant_dict.pkl'):\n",
    "    representant_dict = pickle.load(open('Data/processed_data/representant_dict.pkl','rb'))\n",
    "else:\n",
    "    representant_dict = compute_unique_names(authors_raw_set)\n",
    "    pickle.dump(representant_dict, open('Data/processed_data/representant_dict.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set each name to its representant value\n",
    "information_df.authors = information_df.authors.apply(lambda x: [representant_dict[auth.strip().lower().translate(str.maketrans('', '', string.punctuation))] for auth in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>pub_year</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title_lemma</th>\n",
       "      <th>new_ID</th>\n",
       "      <th>authors_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>4046</td>\n",
       "      <td>2000</td>\n",
       "      <td>irreducible hamiltonian brst symmetry for redu...</td>\n",
       "      <td>[c bizdadea, e m cioroianu, s o saliu]</td>\n",
       "      <td>Int.J.Mod.Phys.</td>\n",
       "      <td>an irreducible hamiltonian brst quantization m...</td>\n",
       "      <td>[irreducible, hamiltonian, brst, symmetry, red...</td>\n",
       "      <td>800</td>\n",
       "      <td>[3921, 1033, 7459]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5260</th>\n",
       "      <td>109197</td>\n",
       "      <td>2001</td>\n",
       "      <td>novel type i compactifications</td>\n",
       "      <td>[david r morrison, savdeep sethi]</td>\n",
       "      <td>JHEP</td>\n",
       "      <td>we argue that there are two distinct classes o...</td>\n",
       "      <td>[novel, type, compactification]</td>\n",
       "      <td>5260</td>\n",
       "      <td>[9158, 12804]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  pub_year                                              title  \\\n",
       "800     4046      2000  irreducible hamiltonian brst symmetry for redu...   \n",
       "5260  109197      2001                     novel type i compactifications   \n",
       "\n",
       "                                     authors     journal_name  \\\n",
       "800   [c bizdadea, e m cioroianu, s o saliu]  Int.J.Mod.Phys.   \n",
       "5260       [david r morrison, savdeep sethi]             JHEP   \n",
       "\n",
       "                                               abstract  \\\n",
       "800   an irreducible hamiltonian brst quantization m...   \n",
       "5260  we argue that there are two distinct classes o...   \n",
       "\n",
       "                                            title_lemma  new_ID  \\\n",
       "800   [irreducible, hamiltonian, brst, symmetry, red...     800   \n",
       "5260                    [novel, type, compactification]    5260   \n",
       "\n",
       "              authors_id  \n",
       "800   [3921, 1033, 7459]  \n",
       "5260       [9158, 12804]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a unique index for each author\n",
    "representants_list = list(set(representant_dict.values()))\n",
    "authors2idx = {k: v for v, k in enumerate(representants_list)}\n",
    "information_df[\"authors_id\"] = information_df.authors.apply(lambda x: [authors2idx[auth] for auth in x])\n",
    "\n",
    "information_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_authors_co_auth = create_co_authorship_graph(information_df, authors2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# those embeddings do not depend on the fold\n",
    "abstracts_embeddings = compute_abstracts_embeddings(information_df)\n",
    "if os.path.isfile(f'Data/embeddings/walklets_co_auth_embeddings.pkl'):\n",
    "    walklets_co_auth_embeddings = pickle.load(open('Data/embeddings/walklets_co_auth_embeddings.pkl','rb'))\n",
    "else:\n",
    "    walklets_co_auth_embeddings = compute_walklets(G_authors_co_auth)\n",
    "    pickle.dump(walklets_co_auth_embeddings,open('Data/embeddings/walklets_co_auth_embeddings.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1\n",
      "fold: 2\n",
      "fold: 3\n",
      "fold: 4\n",
      "fold: 5\n"
     ]
    }
   ],
   "source": [
    "# those embeddings depend on the fold\n",
    "for i in range(number_of_folds):\n",
    "    print(f\"fold: {i+1}\")\n",
    "\n",
    "    train_set = pd.read_csv(f\"Data/folds/train_set_{i+1}\")\n",
    "    articles_graph = create_articles_graph(train_set,information_df)\n",
    "    authors_citation_graph = create_authors_co_citation_graph(train_set, information_df, authors2idx)\n",
    "\n",
    "    if os.path.isfile(f'Data/embeddings/articles_walklets_{i+1}.pkl') == False:\n",
    "        walklets_articles_embeddings = compute_walklets(articles_graph)\n",
    "        pickle.dump(walklets_articles_embeddings, open(f'Data/embeddings/articles_walklets_{i+1}.pkl','wb'))\n",
    "    if os.path.isfile(f'Data/embeddings/articles_node2vec_{i+1}.pkl') == False:\n",
    "        node2vec_articles_embeddings = compute_node2vec(articles_graph)\n",
    "        pickle.dump(node2vec_articles_embeddings, open(f'Data/embeddings/articles_node2vec_{i+1}.pkl','wb'))\n",
    "    if os.path.isfile(f'Data/embeddings/articles_walklets_{i+1}.pkl') == False:\n",
    "        walklets_co_citation_embeddings = compute_walklets(authors_citation_graph)\n",
    "        pickle.dump(walklets_co_citation_embeddings, open(f'Data/embeddings/co_citation_walklets_{i+1}.pkl','wb'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_non_embeddings_features(df, information_df, G_articles):\n",
    "    information_df['new_ID'] = information_df.ID.apply(lambda x:id_old2new[x])\n",
    "    useful_information_df = information_df[['new_ID','authors','pub_year', 'title_lemma']]\n",
    "\n",
    "    # prepare data frame for common authors computation\n",
    "    df = (df\n",
    "    .merge(useful_information_df, how ='left', left_on = ['node1'], right_on = ['new_ID'])\n",
    "    .rename(columns = {'authors':'authors_node_1', 'pub_year':'pub_year1', 'title_lemma':'title_lemma1'})\n",
    "    .merge(useful_information_df, how ='left', left_on = ['node2'], right_on = ['new_ID'])\n",
    "    .rename(columns = {'authors':'authors_node_2', 'pub_year':'pub_year2', 'title_lemma':'title_lemma2'})\n",
    "    )\n",
    "\n",
    "    ### Compute page rank\n",
    "    page_rank_dict = nx.pagerank(G_articles)\n",
    "\n",
    "    ### compute degree centrality\n",
    "    centrality_dict = nx.degree_centrality(G_articles)\n",
    "\n",
    "    print(\"common_journal\")\n",
    "    df['common_journals'] = df.apply(lambda x: common_journal(information_df, x.node1, x.node2),axis = 1)\n",
    "    \n",
    "    print('computing common authors')\n",
    "    #  compute common authors\n",
    "    df['common_authors'] = df.apply(lambda x:len(set(x.authors_node_1)&set(x.authors_node_2)),axis = 1)\n",
    "\n",
    "    print('computing common words')\n",
    "    #  compute common words in titles\n",
    "    df['common_title_words'] = df.apply(lambda x:len(set(x.title_lemma1)&set(x.title_lemma2)),axis = 1)\n",
    "\n",
    "    print('computing delta publication year')\n",
    "    # compute delta publication year\n",
    "    df['delta_publication'] = df.apply(lambda x:np.abs(x.pub_year2 - x.pub_year1),axis = 1)\n",
    "\n",
    "    # compute edges features\n",
    "    print('computing jacard index')\n",
    "    df['jacard'] = df.apply(lambda x: Jaccard(G_articles, (x.node1, x.node2)),axis = 1)\n",
    "\n",
    "    print('computing preferential attachement')\n",
    "    df['pa'] = df.apply(lambda x: preferential_attachement(G_articles, (x.node1, x.node2)),axis = 1)\n",
    "\n",
    "    print('computing adamic_adar')\n",
    "    df['adamic_adar'] = df.apply(lambda x: AdamicAdar(G_articles, (x.node1, x.node2)),axis = 1)\n",
    "\n",
    "    print('are connected')\n",
    "    df['connection'] = df.apply(lambda x: are_connected(G_articles, (x.node1, x.node2)),axis = 1)\n",
    "\n",
    "    print('page ranks')\n",
    "    df['page_rank1'] = df.apply(lambda x: page_rank_dict[x.node1],axis = 1)\n",
    "    df['page_rank2'] = df.apply(lambda x: page_rank_dict[x.node2],axis = 1)\n",
    "    \n",
    "    print('compute degree')\n",
    "\n",
    "    df['degree1'] = df.apply(lambda x: centrality_dict[x.node1],axis = 1)\n",
    "    df['degree2'] = df.apply(lambda x: centrality_dict[x.node2],axis = 1)\n",
    "\n",
    "    \n",
    "    df = df.fillna({ 'jacard':df.jacard.mean(),\n",
    "                     'adamic_adar':df.adamic_adar.mean()\n",
    "                     })\n",
    "\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding_features(df,\n",
    "                                information_df,\n",
    "                                abstracts_embeddings,\n",
    "                                walklets_articles_embeddings,\n",
    "                                walklets_co_auth_embeddings,\n",
    "                                walklets_co_citation_embeddings,\n",
    "                                node2vec_articles_embeddings):\n",
    "\n",
    "    # for each article take a mean of the authors embedding as global autors embedding (idem for citation)\n",
    "    articles_authors_embedding = []\n",
    "    articles_authors_embedding_citation = []\n",
    "    for i in range(information_df.shape[0]):\n",
    "        value = information_df[information_df.new_ID == i]\n",
    "        authors_id = value.authors_id\n",
    "        embeddings = np.array([0 for i in range(128)]).astype('float64')\n",
    "        embeddings_citation = np.array([0 for i in range(128)]).astype('float64')\n",
    "        for author in authors_id:\n",
    "            embeddings+=walklets_co_auth_embeddings[author][0]\n",
    "            embeddings_citation+=walklets_co_citation_embeddings[author][0]\n",
    "        articles_authors_embedding.append(embeddings/len(authors_id))\n",
    "        articles_authors_embedding_citation.append(embeddings_citation/len(authors_id))\n",
    "\n",
    "\n",
    "    # compute some cosine and euclidian based distances\n",
    "    df['articles_walklets_cosine'] = df.apply(lambda x:cosine(walklets_articles_embeddings[x.node1],walklets_articles_embeddings[x.node2]), axis = 1)\n",
    "    df['articles_node2vec_cosine'] = df.apply(lambda x:cosine(node2vec_articles_embeddings[x.node1],node2vec_articles_embeddings[x.node2]), axis = 1)\n",
    "    df['abstracts_embeddings_cosine'] = df.apply(lambda x:cosine(abstracts_embeddings[x.node1][0],abstracts_embeddings[x.node2][0]), axis = 1)\n",
    "    \n",
    "    df['articles_walklets_euclidian'] = df.apply(lambda x:euclidian(walklets_articles_embeddings[x.node1],walklets_articles_embeddings[x.node2]), axis = 1)\n",
    "    df['articles_node2vec_euclidian'] = df.apply(lambda x:euclidian(node2vec_articles_embeddings[x.node1],node2vec_articles_embeddings[x.node2]), axis = 1)\n",
    "    df['abstracts_embeddings_euclidian'] = df.apply(lambda x:euclidian(abstracts_embeddings[x.node1][0],abstracts_embeddings[x.node2][0]), axis = 1)\n",
    "    \n",
    "    # compute some cosine and euclidian based distances for authors\n",
    "    df['co_authorship_embeddings_cosine'] = df.apply(lambda x:cosine(articles_authors_embedding[x.node1],articles_authors_embedding[x.node2]), axis = 1)\n",
    "    df['authors_embeddings_cosine_citation'] = df.apply(lambda x:cosine(articles_authors_embedding_citation[x.node1],articles_authors_embedding_citation[x.node2]), axis = 1)    \n",
    "    df['co_authorship_embeddings_euclidian'] = df.apply(lambda x:euclidian(articles_authors_embedding[x.node1],articles_authors_embedding[x.node2]), axis = 1)\n",
    "    df['authors_embeddings_euclidian_citation'] = df.apply(lambda x:euclidian(articles_authors_embedding_citation[x.node1],articles_authors_embedding_citation[x.node2]), axis = 1)\n",
    "\n",
    "    # node1 and node2 article embedding\n",
    "    print(\"add articles walklets embeddings\")\n",
    "    # only append vectors of size 10 that represent articles embeddings (quicker to compute)\n",
    "    pca_walklets= PCA(n_components = 5)\n",
    "    walklets_articles_embeddings = pca_walklets.fit_transform(walklets_articles_embeddings)\n",
    "    walklets_node_embeddings_df = pd.DataFrame(walklets_articles_embeddings, columns = [f'emb_walklets_{i}' for i in range(len(walklets_articles_embeddings[0]))])\n",
    "    walklets_node_embeddings_df = walklets_node_embeddings_df.reset_index().rename(columns = {'index':'node'})\n",
    "    df = (df\n",
    "        .merge(walklets_node_embeddings_df, how ='left', left_on = ['node1'], right_on = ['node'])\n",
    "        .drop(columns = ['node'])\n",
    "        .merge(walklets_node_embeddings_df, how ='left', left_on = ['node2'], right_on = ['node'])\n",
    "        .drop(columns = ['node'])\n",
    "    )\n",
    "\n",
    "    print(\"add articles node2vecs embeddings\")\n",
    "    # only append vectors of size 10 that represent articles embeddings (quicker to compute)\n",
    "    pca_node2vec =  PCA(n_components = 5)\n",
    "    node2vec_articles_embeddings = pca_node2vec.fit_transform(node2vec_articles_embeddings)\n",
    "    node_node2vec_embeddings_df = pd.DataFrame(node2vec_articles_embeddings, columns = [f'emb_node2vec{i}' for i in range(len(node2vec_articles_embeddings[0]))])\n",
    "    node_node2vec_embeddings_df = node_node2vec_embeddings_df.reset_index().rename(columns = {'index':'node'})\n",
    "    df = (df\n",
    "        .merge(node_node2vec_embeddings_df, how ='left', left_on = ['node1'], right_on = ['node'])\n",
    "        .drop(columns = ['node'])\n",
    "        .merge(node_node2vec_embeddings_df, how ='left', left_on = ['node2'], right_on = ['node'])\n",
    "        .drop(columns = ['node'])\n",
    "    )\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Compute features for all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1\n",
      "fold: 2\n",
      "fold: 3\n",
      "fold: 4\n",
      "fold: 5\n"
     ]
    }
   ],
   "source": [
    "for i in range(number_of_folds):\n",
    "    print(f\"fold: {i+1}\")\n",
    "    if os.path.isfile(f\"Data/processed_data/train_set_features{i+1}.csv\") and os.path.isfile(f\"Data/processed_data/val_set_features{i+1}.csv\") and os.path.isfile(f\"Data/processed_data/test_set_features{i+1}.csv\"):\n",
    "        continue\n",
    "    else:\n",
    "        # load sets\n",
    "        train_set = pd.read_csv(f\"Data/folds/train_set_{i+1}\")\n",
    "        validation_set = pd.read_csv(f\"Data/folds/validation_set_{i+1}\")\n",
    "\n",
    "        # compute graphs\n",
    "        G_articles = create_articles_graph(train_set,information_df)\n",
    "\n",
    "        # load embeddings\n",
    "        walklets_articles_embeddings = pickle.load(open(f'Data/embeddings/articles_walklets_{i+1}.pkl','rb'))\n",
    "        walklets_co_citation_embeddings = pickle.load(open(f'Data/embeddings/co_citation_walklets_{i+1}.pkl','rb'))\n",
    "\n",
    "        # compute features for train\n",
    "        print(\"compute train features\")\n",
    "        train_set_with_features = compute_non_embeddings_features(train_set, information_df, G_articles)\n",
    "        train_set_with_features = compute_embedding_features(train_set_with_features, information_df, abstracts_embeddings,\n",
    "                                                                walklets_articles_embeddings,\n",
    "                                                                walklets_co_auth_embeddings,\n",
    "                                                                walklets_co_citation_embeddings,\n",
    "                                                                node2vec_articles_embeddings)\n",
    "        # compute features for val\n",
    "        print(\"compute validation features\")\n",
    "        val_set_with_features = compute_non_embeddings_features(validation_set, information_df, G_articles)\n",
    "        val_set_with_features = compute_embedding_features(val_set_with_features, information_df, abstracts_embeddings,\n",
    "                                                                walklets_articles_embeddings,\n",
    "                                                                walklets_co_auth_embeddings,\n",
    "                                                                walklets_co_citation_embeddings,\n",
    "                                                                node2vec_articles_embeddings)\n",
    "\n",
    "        # compute features for test\n",
    "        print(\"compute test features\")\n",
    "        test_set_with_features = compute_non_embeddings_features(test_set, information_df, G_articles)\n",
    "        test_set_with_features = compute_embedding_features(test_set_with_features, information_df, abstracts_embeddings,\n",
    "                                                                walklets_articles_embeddings,\n",
    "                                                                walklets_co_auth_embeddings,\n",
    "                                                                walklets_co_citation_embeddings,\n",
    "                                                                node2vec_articles_embeddings)\n",
    "\n",
    "        train_set_with_features.to_csv(f\"Data/processed_data/train_set_features{i+1}.csv\", index = False)\n",
    "        val_set_with_features.to_csv(f\"Data/processed_data/val_set_features{i+1}.csv\", index = False)\n",
    "        test_set_with_features.to_csv(f\"Data/processed_data/test_set_features{i+1}.csv\", index = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute columns of interest\n",
    "all_columns = set(pd.read_csv(\"Data/processed_data/train_set_features1.csv\").columns)\n",
    "\n",
    "to_remove = set(['node1', 'node2', 'label', 'new_ID_x', 'authors_node_1', 'pub_year1',\n",
    "       'title_lemma1', 'new_ID_y', 'authors_node_2', 'pub_year2',\n",
    "       'title_lemma2'])\n",
    "columns_to_keep= list(all_columns-to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_Estimators | Max_Depth | Min Child Weights | Learning_Rate |    Thresh    |   Accuracy   |    F1     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\33631\\Documents\\Etudes\\centrale 3A\\MLNS\\MLNS_Kaggle_Challenge\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\33631\\Documents\\Etudes\\centrale 3A\\MLNS\\MLNS_Kaggle_Challenge\\run_models.ipynb Cell 28'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/run_models.ipynb#ch0000027?line=17'>18</a>\u001b[0m validation_samples_scaled \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mtransform(np\u001b[39m.\u001b[39mfloat32(validation_samples))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/run_models.ipynb#ch0000027?line=20'>21</a>\u001b[0m \u001b[39m# train classifier (grid search best params)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/run_models.ipynb#ch0000027?line=21'>22</a>\u001b[0m clf, thresh \u001b[39m=\u001b[39m get_best_xgb(train_samples_scaled, \u001b[39mlist\u001b[39;49m(train_labels\u001b[39m.\u001b[39;49mlabel), validation_samples_scaled, \u001b[39mlist\u001b[39;49m(validation_labels\u001b[39m.\u001b[39;49mlabel))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/run_models.ipynb#ch0000027?line=23'>24</a>\u001b[0m pickle\u001b[39m.\u001b[39mdump((clf, thresh), \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mData/models/clf_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.pkl\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/run_models.ipynb#ch0000027?line=24'>25</a>\u001b[0m pickle\u001b[39m.\u001b[39mdump(scaler, \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mData/models/scaler_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.pkl\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\users\\33631\\documents\\etudes\\centrale 3a\\mlns\\mlns_kaggle_challenge\\src\\link_pred\\grid_search.py:20\u001b[0m, in \u001b[0;36mget_best_xgb\u001b[1;34m(train_samples_scaled, train_labels, validation_samples_scaled, validation_labels)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/users/33631/documents/etudes/centrale%203a/mlns/mlns_kaggle_challenge/src/link_pred/grid_search.py?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m min_child_weight \u001b[39min\u001b[39;00m min_child_weights:\n\u001b[0;32m     <a href='file:///c%3A/users/33631/documents/etudes/centrale%203a/mlns/mlns_kaggle_challenge/src/link_pred/grid_search.py?line=18'>19</a>\u001b[0m     clf \u001b[39m=\u001b[39m XGBClassifier(max_depth\u001b[39m=\u001b[39mmax_depth, learning_rate\u001b[39m=\u001b[39mlr, min_child_weight\u001b[39m=\u001b[39mmin_child_weight, n_estimators\u001b[39m=\u001b[39mn_estim, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, tree_method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgpu_hist\u001b[39m\u001b[39m'\u001b[39m, predictor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpu_predictor\u001b[39m\u001b[39m\"\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m---> <a href='file:///c%3A/users/33631/documents/etudes/centrale%203a/mlns/mlns_kaggle_challenge/src/link_pred/grid_search.py?line=19'>20</a>\u001b[0m     clf\u001b[39m.\u001b[39;49mfit(train_samples_scaled, train_labels, eval_metric\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauc\u001b[39;49m\u001b[39m\"\u001b[39;49m, early_stopping_rounds\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, eval_set\u001b[39m=\u001b[39;49m[(validation_samples_scaled, validation_labels)], verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     <a href='file:///c%3A/users/33631/documents/etudes/centrale%203a/mlns/mlns_kaggle_challenge/src/link_pred/grid_search.py?line=20'>21</a>\u001b[0m     y_pred \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict_proba(validation_samples_scaled)\n\u001b[0;32m     <a href='file:///c%3A/users/33631/documents/etudes/centrale%203a/mlns/mlns_kaggle_challenge/src/link_pred/grid_search.py?line=21'>22</a>\u001b[0m     probas \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(y_pred)[:,\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\33631\\Documents\\Etudes\\centrale 3A\\MLNS\\MLNS_Kaggle_Challenge\\venv\\lib\\site-packages\\xgboost\\core.py:506\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/core.py?line=503'>504</a>\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/core.py?line=504'>505</a>\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/core.py?line=505'>506</a>\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\33631\\Documents\\Etudes\\centrale 3A\\MLNS\\MLNS_Kaggle_Challenge\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1250\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/sklearn.py?line=1229'>1230</a>\u001b[0m model, feval, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_configure_fit(xgb_model, eval_metric, params)\n\u001b[0;32m   <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/sklearn.py?line=1230'>1231</a>\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/sklearn.py?line=1231'>1232</a>\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[0;32m   <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/sklearn.py?line=1232'>1233</a>\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/sklearn.py?line=1246'>1247</a>\u001b[0m     label_transform\u001b[39m=\u001b[39mlabel_transform,\n\u001b[0;32m   <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/sklearn.py?line=1247'>1248</a>\u001b[0m )\n\u001b[1;32m-> <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/sklearn.py?line=1249'>1250</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[0;32m   <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/sklearn.py?line=1250'>1251</a>\u001b[0m     params,\n\u001b[0;32m   <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/sklearn.py?line=1251'>1252</a>\u001b[0m     train_dmatrix,\n\u001b[0;32m   <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/sklearn.py?line=1252'>1253</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[0;32m   <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/sklearn.py?line=1253'>1254</a>\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[0;32m   <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/sklearn.py?line=1254'>1255</a>\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[0;32m   <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/sklearn.py?line=1255'>1256</a>\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[0;32m   <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/sklearn.py?line=1256'>1257</a>\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[0;32m   <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/sklearn.py?line=1257'>1258</a>\u001b[0m     feval\u001b[39m=\u001b[39;49mfeval,\n\u001b[0;32m   <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/sklearn.py?line=1258'>1259</a>\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/sklearn.py?line=1259'>1260</a>\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/sklearn.py?line=1260'>1261</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/sklearn.py?line=1261'>1262</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/sklearn.py?line=1263'>1264</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[0;32m   <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/sklearn.py?line=1264'>1265</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\33631\\Documents\\Etudes\\centrale 3A\\MLNS\\MLNS_Kaggle_Challenge\\venv\\lib\\site-packages\\xgboost\\training.py:188\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/training.py?line=114'>115</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(params, dtrain, num_boost_round\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, evals\u001b[39m=\u001b[39m(), obj\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, feval\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/training.py?line=115'>116</a>\u001b[0m           maximize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, early_stopping_rounds\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, evals_result\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/training.py?line=116'>117</a>\u001b[0m           verbose_eval\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, xgb_model\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, callbacks\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/training.py?line=117'>118</a>\u001b[0m     \u001b[39m# pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/training.py?line=118'>119</a>\u001b[0m     \u001b[39m\"\"\"Train a booster with given parameters.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/training.py?line=119'>120</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/training.py?line=120'>121</a>\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/training.py?line=185'>186</a>\u001b[0m \u001b[39m    Booster : a trained booster model\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/training.py?line=186'>187</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/training.py?line=187'>188</a>\u001b[0m     bst \u001b[39m=\u001b[39m _train_internal(params, dtrain,\n\u001b[0;32m    <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/training.py?line=188'>189</a>\u001b[0m                           num_boost_round\u001b[39m=\u001b[39;49mnum_boost_round,\n\u001b[0;32m    <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/training.py?line=189'>190</a>\u001b[0m                           evals\u001b[39m=\u001b[39;49mevals,\n\u001b[0;32m    <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/training.py?line=190'>191</a>\u001b[0m                           obj\u001b[39m=\u001b[39;49mobj, feval\u001b[39m=\u001b[39;49mfeval,\n\u001b[0;32m    <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/training.py?line=191'>192</a>\u001b[0m                           xgb_model\u001b[39m=\u001b[39;49mxgb_model, callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/training.py?line=192'>193</a>\u001b[0m                           verbose_eval\u001b[39m=\u001b[39;49mverbose_eval,\n\u001b[0;32m    <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/training.py?line=193'>194</a>\u001b[0m                           evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[0;32m    <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/training.py?line=194'>195</a>\u001b[0m                           maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/training.py?line=195'>196</a>\u001b[0m                           early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds)\n\u001b[0;32m    <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/training.py?line=196'>197</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m bst\n",
      "File \u001b[1;32mc:\\Users\\33631\\Documents\\Etudes\\centrale 3A\\MLNS\\MLNS_Kaggle_Challenge\\venv\\lib\\site-packages\\xgboost\\training.py:81\u001b[0m, in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/training.py?line=78'>79</a>\u001b[0m \u001b[39mif\u001b[39;00m callbacks\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m     <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/training.py?line=79'>80</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/training.py?line=80'>81</a>\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[0;32m     <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/training.py?line=81'>82</a>\u001b[0m \u001b[39mif\u001b[39;00m callbacks\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m     <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/training.py?line=82'>83</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\33631\\Documents\\Etudes\\centrale 3A\\MLNS\\MLNS_Kaggle_Challenge\\venv\\lib\\site-packages\\xgboost\\core.py:1680\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/core.py?line=1676'>1677</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_features(dtrain)\n\u001b[0;32m   <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/core.py?line=1678'>1679</a>\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/core.py?line=1679'>1680</a>\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[0;32m   <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/core.py?line=1680'>1681</a>\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[0;32m   <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/core.py?line=1681'>1682</a>\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[0;32m   <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/core.py?line=1682'>1683</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/33631/Documents/Etudes/centrale%203A/MLNS/MLNS_Kaggle_Challenge/venv/lib/site-packages/xgboost/core.py?line=1683'>1684</a>\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(number_of_folds):\n",
    "    print(f'fold: {i+1}')\n",
    "    if os.path.isfile(f\"Data/models/clf_{i+1}.pkl\"):\n",
    "        continue\n",
    "    else:\n",
    "        train_set_with_features = pd.read_csv(f\"Data/processed_data/train_set_features{i+1}.csv\")\n",
    "        validation_set_with_features = pd.read_csv(f\"Data/processed_data/val_set_features{i+1}.csv\")\n",
    "        \n",
    "        # only keep columns of interest\n",
    "        train_set_with_features = train_set_with_features[columns_to_keep+['label']]\n",
    "        validation_set_with_features = validation_set_with_features[columns_to_keep+['label']]\n",
    "\n",
    "        train_samples, train_labels = train_set_with_features.drop(columns = ['label']), train_set_with_features[['label']]\n",
    "        validation_samples, validation_labels = validation_set_with_features.drop(columns = ['label']), validation_set_with_features[['label']]\n",
    "\n",
    "        # scale data\n",
    "        scaler = StandardScaler()\n",
    "        train_samples_scaled = scaler.fit_transform(np.float32(train_samples))\n",
    "        validation_samples_scaled = scaler.transform(np.float32(validation_samples))\n",
    "\n",
    "\n",
    "        # train classifier (grid search best params)\n",
    "        clf, thresh = get_best_xgb(train_samples_scaled, list(train_labels.label), validation_samples_scaled, list(validation_labels.label))\n",
    "        \n",
    "        pickle.dump((clf, thresh), open(f\"Data/models/clf_{i+1}.pkl\", 'wb'))\n",
    "        pickle.dump(scaler, open(f\"Data/models/scaler_{i+1}.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable XGBClassifier object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m test_set_with_features \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit_transform(np\u001b[39m.\u001b[39mfloat32(test_set_with_features))\n\u001b[0;32m     15\u001b[0m \u001b[39m# train classifier (grid search best params)\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m clf, thresh \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mData/models/clf_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.pkl\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     18\u001b[0m \u001b[39mif\u001b[39;00m preds\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[0;32m     19\u001b[0m     preds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(clf\u001b[39m.\u001b[39mpredict_proba(test_set_with_features)[:,\u001b[39m1\u001b[39m]\u001b[39m>\u001b[39mthresh)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable XGBClassifier object"
     ]
    }
   ],
   "source": [
    "preds = np.array([])\n",
    "for i in range(number_of_folds):\n",
    "    \n",
    "    test_set_with_features = pd.read_csv(f\"Data/processed_data/test_set_features{i+1}.csv\")\n",
    "\n",
    "    # scale data\n",
    "    scaler = pickle.load(open(f\"Data/models/scaler_{i+1}.pkl\", 'rb'))\n",
    "\n",
    "    # only keep columns of interest\n",
    "    test_set_with_features = test_set_with_features[columns_to_keep]\n",
    "\n",
    "    test_set_with_features = scaler.fit_transform(np.float32(test_set_with_features))\n",
    "\n",
    "\n",
    "    # train classifier (grid search best params)\n",
    "    clf, thresh = pickle.load(open(f\"Data/models/clf_{i+1}.pkl\", 'rb'))\n",
    "\n",
    "    if preds.shape[0]==0:\n",
    "        preds = np.array(clf.predict_proba(test_set_with_features)[:,1]>thresh)\n",
    "    else:\n",
    "        preds += np.array(clf.predict_proba(test_set_with_features)[:,1]>thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voting \n",
    "test_set['category'] = np.int32(preds>(number_of_folds/2))\n",
    "\n",
    "test_set = (test_set\n",
    ".reset_index()\n",
    ".rename(columns = {'index':'id'})\n",
    ".drop(columns = ['node1','node2'])\n",
    ")\n",
    "\n",
    "test_set.to_csv('final_predictions_no_emb.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6657d8cf73e4192045730bbde1f7a947d4725a27f96025bfcb1ab47bc67665b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
