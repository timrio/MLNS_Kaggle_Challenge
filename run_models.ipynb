{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: OMP_NUM_THREADS=None =>\n",
      "... If you are using openblas if you are using openblas set OMP_NUM_THREADS=1 or risk subprocess calls hanging indefinitely\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\33631\\AppData\\Local\\Temp\\ipykernel_152728\\997310428.py:24: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
      "  from scipy.stats.stats import pearsonr\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import os\n",
    "import networkx as nx\n",
    "from gensim.models import Word2Vec\n",
    "from node2vec import Node2Vec\n",
    "import pickle\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from hpsklearn import HyperoptEstimator, any_classifier, any_preprocessing\n",
    "from hyperopt import tpe\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score \n",
    "from sklearn.preprocessing import LabelBinarizer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from link_pred.utils import cosine, euclidian, retrieve_and_pre_processed_informations, compute_unique_names\n",
    "from link_pred.folds_creation import create_and_save_folds\n",
    "from link_pred.create_graphs import create_articles_graph, create_co_authorship_graph, create_authors_co_citation_graph\n",
    "from link_pred.node_embeddings import compute_abstracts_embeddings, compute_walklets, compute_node2vec, compute_deep_walks\n",
    "from link_pred.edges_features import Jaccard, AdamicAdar, preferential_attachement, are_connected, common_journal\n",
    "from link_pred.grid_search import get_best_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_path = \"Data/raw_data/node_information.csv\"\n",
    "test_set_path = \"Data/raw_data/testing_set.txt\"\n",
    "train_set_path = \"Data/raw_data/training_set.txt\"\n",
    "random_preds_path = \"Data/raw_data/random_predictions.csv\"\n",
    "\n",
    "number_of_folds = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and pre_process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_df = retrieve_and_pre_processed_informations(information_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign new id to each node (needs to start from 0)\n",
    "id_old2new = {k: v for v, k in enumerate(list(information_df.ID))}\n",
    "id_new2old = {v: k for v, k in id_old2new.items()}\n",
    "\n",
    "information_df['new_ID'] = information_df.ID.apply(lambda x: id_old2new[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_train_set = pd.read_csv(train_set_path, sep =\" \", header = None)\n",
    "initial_train_set.columns = ['node1','node2','label']\n",
    "\n",
    "## update nodes values to new indices\n",
    "initial_train_set.node1 = initial_train_set.apply(lambda x:id_old2new[x.node1], axis = 1)\n",
    "initial_train_set.node2 = initial_train_set.apply(lambda x:id_old2new[x.node2], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load test set\n",
    "\n",
    "test_set = pd.read_csv(test_set_path, sep =\" \", header = None)\n",
    "test_set.columns = ['node1','node2']\n",
    "\n",
    "## update nodes values to new indices\n",
    "test_set.node1 = test_set.apply(lambda x:id_old2new[x.node1], axis = 1)\n",
    "test_set.node2 = test_set.apply(lambda x:id_old2new[x.node2], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1 already exists !\n",
      "fold 2 already exists !\n",
      "fold 3 already exists !\n",
      "fold 4 already exists !\n",
      "fold 5 already exists !\n"
     ]
    }
   ],
   "source": [
    "create_and_save_folds(initial_train_set, number_of_folds = number_of_folds, validation_size = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with authors various names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# convert to lower case, remove punctuation, strip the names\n",
    "authors_raw_set = set([auth.strip().lower().translate(str.maketrans('', '', string.punctuation)) for list_auth in information_df.authors for auth in list_auth if len(auth)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# several authors can be named differently (eg. Jean DUPONT, J.Dupont, etc.)\n",
    "# we create a name matcher function to try to indentify each author and assign each denomination a \"representant\"\n",
    "\n",
    "if os.path.isfile('Data/processed_data/representant_dict.pkl'):\n",
    "    representant_dict = pickle.load(open('Data/processed_data/representant_dict.pkl','rb'))\n",
    "else:\n",
    "    representant_dict = compute_unique_names(authors_raw_set)\n",
    "    pickle.dump(representant_dict, open('Data/processed_data/representant_dict.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set each name to its representant value\n",
    "information_df.authors = information_df.authors.apply(lambda x: [representant_dict[auth.strip().lower().translate(str.maketrans('', '', string.punctuation))] for auth in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>pub_year</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title_lemma</th>\n",
       "      <th>new_ID</th>\n",
       "      <th>authors_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>6206</td>\n",
       "      <td>2000</td>\n",
       "      <td>quantization conditions from the group theory</td>\n",
       "      <td>[d chruscinski]</td>\n",
       "      <td></td>\n",
       "      <td>we show that the requirement of the relativist...</td>\n",
       "      <td>[quantization, condition, group, theory]</td>\n",
       "      <td>1433</td>\n",
       "      <td>[8485]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22846</th>\n",
       "      <td>9803053</td>\n",
       "      <td>1998</td>\n",
       "      <td>particles on ads 4 7 and primary operators on ...</td>\n",
       "      <td>[shiraz minwalla]</td>\n",
       "      <td>JHEP</td>\n",
       "      <td>worldvolumes 1 identify a correspondence betwe...</td>\n",
       "      <td>[particle, ad, primary, operator, m, brane]</td>\n",
       "      <td>22846</td>\n",
       "      <td>[5578]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  pub_year                                              title  \\\n",
       "1433      6206      2000      quantization conditions from the group theory   \n",
       "22846  9803053      1998  particles on ads 4 7 and primary operators on ...   \n",
       "\n",
       "                 authors journal_name  \\\n",
       "1433     [d chruscinski]                \n",
       "22846  [shiraz minwalla]         JHEP   \n",
       "\n",
       "                                                abstract  \\\n",
       "1433   we show that the requirement of the relativist...   \n",
       "22846  worldvolumes 1 identify a correspondence betwe...   \n",
       "\n",
       "                                       title_lemma  new_ID authors_id  \n",
       "1433      [quantization, condition, group, theory]    1433     [8485]  \n",
       "22846  [particle, ad, primary, operator, m, brane]   22846     [5578]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a unique index for each author\n",
    "representants_list = list(set(representant_dict.values()))\n",
    "authors2idx = {k: v for v, k in enumerate(representants_list)}\n",
    "information_df[\"authors_id\"] = information_df.authors.apply(lambda x: [authors2idx[auth] for auth in x])\n",
    "\n",
    "information_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_authors_co_auth = create_co_authorship_graph(information_df, authors2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# those embeddings do not depend on the fold\n",
    "abstracts_embeddings = compute_abstracts_embeddings(information_df)\n",
    "if os.path.isfile(f'Data/embeddings/walklets_co_auth_embeddings.pkl'):\n",
    "    walklets_co_auth_embeddings = pickle.load(open('Data/embeddings/walklets_co_auth_embeddings.pkl','rb'))\n",
    "else:\n",
    "    walklets_co_auth_embeddings = compute_walklets(G_authors_co_auth)\n",
    "    pickle.dump(walklets_co_auth_embeddings,open('Data/embeddings/walklets_co_auth_embeddings.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# those embeddings depend on the fold\n",
    "for i in range(number_of_folds):\n",
    "    if os.path.isfile(f'Data/embeddings/articles_walklets_{i+1}.pkl'):\n",
    "        continue\n",
    "    else:\n",
    "        train_set = pd.read_csv(f\"Data/folds/train_set_{i+1}\")\n",
    "        articles_graph = create_articles_graph(train_set,information_df)\n",
    "        authors_citation_graph = create_authors_co_citation_graph(train_set, information_df, authors2idx)\n",
    "\n",
    "        walklets_articles_embeddings = compute_walklets(articles_graph)\n",
    "        walklets_co_citation_embeddings = compute_walklets(authors_citation_graph)\n",
    "\n",
    "        pickle.dump(walklets_articles_embeddings, open(f'Data/embeddings/articles_walklets_{i+1}.pkl','wb'))\n",
    "        pickle.dump(walklets_co_citation_embeddings, open(f'Data/embeddings/co_citation_walklets_{i+1}.pkl','wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_non_embeddings_features(df, information_df, G_articles):\n",
    "    information_df['new_ID'] = information_df.ID.apply(lambda x:id_old2new[x])\n",
    "    useful_information_df = information_df[['new_ID','authors','pub_year', 'title_lemma']]\n",
    "\n",
    "    # prepare data frame for common authors computation\n",
    "    df = (df\n",
    "    .merge(useful_information_df, how ='left', left_on = ['node1'], right_on = ['new_ID'])\n",
    "    .rename(columns = {'authors':'authors_node_1', 'pub_year':'pub_year1', 'title_lemma':'title_lemma1'})\n",
    "    .merge(useful_information_df, how ='left', left_on = ['node2'], right_on = ['new_ID'])\n",
    "    .rename(columns = {'authors':'authors_node_2', 'pub_year':'pub_year2', 'title_lemma':'title_lemma2'})\n",
    "    )\n",
    "\n",
    "    ### Compute page rank\n",
    "    page_rank_dict = nx.pagerank(G_articles)\n",
    "\n",
    "    ### compute degree centrality\n",
    "    centrality_dict = nx.degree_centrality(G_articles)\n",
    "\n",
    "    print(\"common_journal\")\n",
    "    df['common_journals'] = df.apply(lambda x: common_journal(information_df, x.node1, x.node2),axis = 1)\n",
    "    \n",
    "    print('computing common authors')\n",
    "    #  compute common authors\n",
    "    df['common_authors'] = df.apply(lambda x:len(set(x.authors_node_1)&set(x.authors_node_2)),axis = 1)\n",
    "\n",
    "    print('computing common words')\n",
    "    #  compute common words in titles\n",
    "    df['common_title_words'] = df.apply(lambda x:len(set(x.title_lemma1)&set(x.title_lemma2)),axis = 1)\n",
    "\n",
    "    print('computing delta publication year')\n",
    "    # compute delta publication year\n",
    "    df['delta_publication'] = df.apply(lambda x:np.abs(x.pub_year2 - x.pub_year1),axis = 1)\n",
    "\n",
    "    # compute edges features\n",
    "    print('computing jacard index')\n",
    "    df['jacard'] = df.apply(lambda x: Jaccard(G_articles, (x.node1, x.node2)),axis = 1)\n",
    "\n",
    "    print('computing preferential attachement')\n",
    "    df['pa'] = df.apply(lambda x: preferential_attachement(G_articles, (x.node1, x.node2)),axis = 1)\n",
    "\n",
    "    print('computing adamic_adar')\n",
    "    df['adamic_adar'] = df.apply(lambda x: AdamicAdar(G_articles, (x.node1, x.node2)),axis = 1)\n",
    "\n",
    "    print('are connected')\n",
    "    df['connection'] = df.apply(lambda x: are_connected(G_articles, (x.node1, x.node2)),axis = 1)\n",
    "\n",
    "    print('page ranks')\n",
    "    df['page_rank1'] = df.apply(lambda x: page_rank_dict[x.node1],axis = 1)\n",
    "    df['page_rank2'] = df.apply(lambda x: page_rank_dict[x.node2],axis = 1)\n",
    "    \n",
    "    print('compute degree')\n",
    "\n",
    "    df['degree1'] = df.apply(lambda x: centrality_dict[x.node1],axis = 1)\n",
    "    df['degree2'] = df.apply(lambda x: centrality_dict[x.node2],axis = 1)\n",
    "\n",
    "    \n",
    "    df = df.fillna({ 'jacard':df.jacard.mean(),\n",
    "                     'adamic_adar':df.adamic_adar.mean()\n",
    "                     })\n",
    "\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding_features(df,\n",
    "                                information_df,\n",
    "                                abstracts_embeddings,\n",
    "                                walklets_articles_embeddings,\n",
    "                                walklets_co_auth_embeddings,\n",
    "                                walklets_co_citation_embeddings):\n",
    "\n",
    "    # for each article take a mean of the authors embedding as global autors embedding (idem for citation)\n",
    "    articles_authors_embedding = []\n",
    "    articles_authors_embedding_citation = []\n",
    "    for i in range(information_df.shape[0]):\n",
    "        value = information_df[information_df.new_ID == i]\n",
    "        authors_id = value.authors_id\n",
    "        embeddings = np.array([0 for i in range(128)]).astype('float64')\n",
    "        embeddings_citation = np.array([0 for i in range(128)]).astype('float64')\n",
    "        for author in authors_id:\n",
    "            embeddings+=walklets_co_auth_embeddings[author][0]\n",
    "            embeddings_citation+=walklets_co_citation_embeddings[author][0]\n",
    "        articles_authors_embedding.append(embeddings/len(authors_id))\n",
    "        articles_authors_embedding_citation.append(embeddings_citation/len(authors_id))\n",
    "\n",
    "\n",
    "    # compute some cosine and euclidian based distances\n",
    "    df['articles_walklets_cosine'] = df.apply(lambda x:cosine(walklets_articles_embeddings[x.node1],walklets_articles_embeddings[x.node2]), axis = 1)\n",
    "    df['abstracts_embeddings_cosine'] = df.apply(lambda x:cosine(abstracts_embeddings[x.node1][0],abstracts_embeddings[x.node2][0]), axis = 1)\n",
    "    \n",
    "    df['articles_walklets_euclidian'] = df.apply(lambda x:euclidian(walklets_articles_embeddings[x.node1],walklets_articles_embeddings[x.node2]), axis = 1)\n",
    "    df['abstracts_embeddings_euclidian'] = df.apply(lambda x:euclidian(abstracts_embeddings[x.node1][0],abstracts_embeddings[x.node2][0]), axis = 1)\n",
    "    \n",
    "    # compute some cosine and euclidian based distances for authors\n",
    "    df['co_authorship_embeddings_cosine'] = df.apply(lambda x:cosine(articles_authors_embedding[x.node1],articles_authors_embedding[x.node2]), axis = 1)\n",
    "    df['authors_embeddings_cosine_citation'] = df.apply(lambda x:cosine(articles_authors_embedding_citation[x.node1],articles_authors_embedding_citation[x.node2]), axis = 1)    \n",
    "    df['co_authorship_embeddings_euclidian'] = df.apply(lambda x:euclidian(articles_authors_embedding[x.node1],articles_authors_embedding[x.node2]), axis = 1)\n",
    "    df['authors_embeddings_euclidian_citation'] = df.apply(lambda x:euclidian(articles_authors_embedding_citation[x.node1],articles_authors_embedding_citation[x.node2]), axis = 1)\n",
    "\n",
    "    # node1 and node2 article embedding\n",
    "    print(\"add articles embeddings\")\n",
    "    node_embeddings_df = pd.DataFrame(walklets_articles_embeddings, columns = [f'emb_{i}' for i in range(len(walklets_articles_embeddings[0]))])\n",
    "    node_embeddings_df = node_embeddings_df.reset_index().rename(columns = {'index':'node'})\n",
    "    df = (df\n",
    "        .merge(node_embeddings_df, how ='left', left_on = ['node1'], right_on = ['node'])\n",
    "        .drop(columns = ['node'])\n",
    "        .merge(node_embeddings_df, how ='left', left_on = ['node2'], right_on = ['node'])\n",
    "        .drop(columns = ['node'])\n",
    "    )\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Compute features for all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0\n",
      "fold: 1\n",
      "fold: 2\n",
      "fold: 3\n",
      "fold: 4\n"
     ]
    }
   ],
   "source": [
    "for i in range(number_of_folds):\n",
    "    print(f\"fold: {i+1}\")\n",
    "    if os.path.isfile(f\"Data/processed_data/train_set_features{i+1}.csv\") and os.path.isfile(f\"Data/processed_data/val_set_features{i+1}.csv\") and os.path.isfile(f\"Data/processed_data/test_set_features{i+1}.csv\"):\n",
    "        continue\n",
    "    else:\n",
    "        # load sets\n",
    "        train_set = pd.read_csv(f\"Data/folds/train_set_{i+1}\")\n",
    "        validation_set = pd.read_csv(f\"Data/folds/validation_set_{i+1}\")\n",
    "\n",
    "        # compute graphs\n",
    "        G_articles = create_articles_graph(train_set,information_df)\n",
    "\n",
    "        # load embeddings\n",
    "        walklets_articles_embeddings = pickle.load(open(f'Data/embeddings/articles_walklets_{i+1}.pkl','rb'))\n",
    "        walklets_co_citation_embeddings = pickle.load(open(f'Data/embeddings/co_citation_walklets_{i+1}.pkl','rb'))\n",
    "\n",
    "        # compute features for train\n",
    "        print(\"compute train features\")\n",
    "        train_set_with_features = compute_non_embeddings_features(train_set, information_df, G_articles)\n",
    "        train_set_with_features = compute_embedding_features(train_set_with_features, information_df, abstracts_embeddings,\n",
    "                                                                walklets_articles_embeddings,\n",
    "                                                                walklets_co_auth_embeddings,\n",
    "                                                                walklets_co_citation_embeddings)\n",
    "        # compute features for val\n",
    "        print(\"compute validation features\")\n",
    "        val_set_with_features = compute_non_embeddings_features(validation_set, information_df, G_articles)\n",
    "        val_set_with_features = compute_embedding_features(val_set_with_features, information_df, abstracts_embeddings,\n",
    "                                                                walklets_articles_embeddings,\n",
    "                                                                walklets_co_auth_embeddings,\n",
    "                                                                walklets_co_citation_embeddings)\n",
    "\n",
    "        # compute features for test\n",
    "        print(\"compute test features\")\n",
    "        test_set_with_features = compute_non_embeddings_features(test_set, information_df, G_articles)\n",
    "        test_set_with_features = compute_embedding_features(test_set_with_features, information_df, abstracts_embeddings,\n",
    "                                                                walklets_articles_embeddings,\n",
    "                                                                walklets_co_auth_embeddings,\n",
    "                                                                walklets_co_citation_embeddings)\n",
    "\n",
    "        train_set_with_features.to_csv(f\"Data/processed_data/train_set_features{i+1}.csv\", index = False)\n",
    "        val_set_with_features.to_csv(f\"Data/processed_data/val_set_features{i+1}.csv\", index = False)\n",
    "        test_set_with_features.to_csv(f\"Data/processed_data/test_set_features{i+1}.csv\", index = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute columns of interest\n",
    "all_columns = set(pd.read_csv(\"Data/processed_data/train_set_features1.csv\").columns)\n",
    "\n",
    "to_remove = set(['node1', 'node2', 'label', 'new_ID_x', 'authors_node_1', 'pub_year1',\n",
    "       'title_lemma1', 'new_ID_y', 'authors_node_2', 'pub_year2',\n",
    "       'title_lemma2'])\n",
    "columns_to_keep= list(all_columns-to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score \n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def get_best_xgb(train_samples_scaled, train_labels, validation_samples_scaled, validation_labels):\n",
    "\n",
    "    n_estim = 2000\n",
    "    max_depth_candidates = [10, 15, 20]\n",
    "    learning_rates = [0.1, 0.05, 0.01]\n",
    "    min_child_weights = [1, 3, 4, 5]\n",
    "    best = 0.0\n",
    "    best_model = None\n",
    "    print(f\"{'N_Estimators':^7} | {'Max_Depth':^7} | {'Min Child Weights':^7} | {'Learning_Rate':^7} | {'Accuracy':^12} | {'F1':^9} \")\n",
    "\n",
    "    for max_depth in tqdm(max_depth_candidates):\n",
    "        for lr in learning_rates:\n",
    "            for min_child_weight in min_child_weights:\n",
    "                clf = XGBClassifier(max_depth=max_depth, learning_rate=lr, min_child_weight=min_child_weight, n_estimators=n_estim, n_jobs=4, tree_method='gpu_hist', predictor=\"gpu_predictor\", random_state=42, seed=42)\n",
    "                clf.fit(train_samples_scaled, train_labels, eval_metric=\"auc\", early_stopping_rounds=300, eval_set=[(validation_samples_scaled, validation_labels)], verbose=0)\n",
    "                y_pred = clf.predict(validation_samples_scaled)\n",
    "                acc = accuracy_score(validation_labels, y_pred)\n",
    "                f1 = f1_score(validation_labels, y_pred)\n",
    "                return(clf)\n",
    "                if f1 > best:\n",
    "                    best_params = (clf.best_ntree_limit, max_depth, lr, acc, f1)\n",
    "                    best = f1\n",
    "                    best_model  = clf\n",
    "                print(f\"{clf.best_ntree_limit:^7} | {max_depth:^7} | {min_child_weight:^7} | {lr:^7} | {acc:^12} | {f1:^9}\")\n",
    "            ############\n",
    "            print()\n",
    "            print(f\"Best Params:\")\n",
    "            print(f\"{best_params[0]:^7} | {best_params[1]:^7} | {best_params[2]:^7} | {best_params[3]:^12} | {best_params[4]:^9}\")\n",
    "    return(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_Estimators | Max_Depth | Min Child Weights | Learning_Rate |   Accuracy   |    F1     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]c:\\Users\\33631\\Documents\\Etudes\\centrale 3A\\MLNS\\MLNS_Kaggle_Challenge\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\33631\\Documents\\Etudes\\centrale 3A\\MLNS\\MLNS_Kaggle_Challenge\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "  0%|          | 0/3 [06:33<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_Estimators | Max_Depth | Min Child Weights | Learning_Rate |   Accuracy   |    F1     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]c:\\Users\\33631\\Documents\\Etudes\\centrale 3A\\MLNS\\MLNS_Kaggle_Challenge\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\33631\\Documents\\Etudes\\centrale 3A\\MLNS\\MLNS_Kaggle_Challenge\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "  0%|          | 0/3 [06:37<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_Estimators | Max_Depth | Min Child Weights | Learning_Rate |   Accuracy   |    F1     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]c:\\Users\\33631\\Documents\\Etudes\\centrale 3A\\MLNS\\MLNS_Kaggle_Challenge\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\33631\\Documents\\Etudes\\centrale 3A\\MLNS\\MLNS_Kaggle_Challenge\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "  0%|          | 0/3 [06:45<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_Estimators | Max_Depth | Min Child Weights | Learning_Rate |   Accuracy   |    F1     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]c:\\Users\\33631\\Documents\\Etudes\\centrale 3A\\MLNS\\MLNS_Kaggle_Challenge\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\33631\\Documents\\Etudes\\centrale 3A\\MLNS\\MLNS_Kaggle_Challenge\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "  0%|          | 0/3 [06:22<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_Estimators | Max_Depth | Min Child Weights | Learning_Rate |   Accuracy   |    F1     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]c:\\Users\\33631\\Documents\\Etudes\\centrale 3A\\MLNS\\MLNS_Kaggle_Challenge\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\33631\\Documents\\Etudes\\centrale 3A\\MLNS\\MLNS_Kaggle_Challenge\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "  0%|          | 0/3 [06:44<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in range(number_of_folds):\n",
    "    if os.path.isfile(f\"Data/models/clf_{i+1}.pkl\"):\n",
    "        continue\n",
    "    else:\n",
    "        train_set_with_features = pd.read_csv(f\"Data/processed_data/train_set_features{i+1}.csv\")\n",
    "        validation_set_with_features = pd.read_csv(f\"Data/processed_data/val_set_features{i+1}.csv\")\n",
    "        \n",
    "        # only keep columns of interest\n",
    "        train_set_with_features = train_set_with_features[columns_to_keep+['label']]\n",
    "        validation_set_with_features = validation_set_with_features[columns_to_keep+['label']]\n",
    "\n",
    "        train_samples, train_labels = train_set_with_features.drop(columns = ['label']), train_set_with_features[['label']]\n",
    "        validation_samples, validation_labels = validation_set_with_features.drop(columns = ['label']), validation_set_with_features[['label']]\n",
    "\n",
    "        # scale data\n",
    "        scaler = StandardScaler()\n",
    "        train_samples_scaled = scaler.fit_transform(np.float32(train_samples))\n",
    "        validation_samples_scaled = scaler.transform(np.float32(validation_samples))\n",
    "\n",
    "\n",
    "        # train classifier (grid search best params)\n",
    "        clf = get_best_xgb(train_samples_scaled, train_labels, validation_samples_scaled, validation_labels)\n",
    "        \n",
    "        pickle.dump(clf, open(f\"Data/models/clf_{i+1}.pkl\", 'wb'))\n",
    "        pickle.dump(scaler, open(f\"Data/models/scaler_{i+1}.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array([])\n",
    "for i in range(number_of_folds):\n",
    "    \n",
    "    test_set_with_features = pd.read_csv(f\"Data/processed_data/test_set_features{i+1}.csv\")\n",
    "\n",
    "    # scale data\n",
    "    scaler = pickle.load(open(f\"Data/models/scaler_{i+1}.pkl\", 'rb'))\n",
    "\n",
    "    # only keep columns of interest\n",
    "    test_set_with_features = test_set_with_features[columns_to_keep]\n",
    "\n",
    "    test_set_with_features = scaler.fit_transform(np.float32(test_set_with_features))\n",
    "\n",
    "\n",
    "    # train classifier (grid search best params)\n",
    "    clf = pickle.load(open(f\"Data/models/clf_{i+1}.pkl\", 'rb'))\n",
    "\n",
    "    if preds.shape[0]==0:\n",
    "        preds = np.array(clf.predict(test_set_with_features))\n",
    "    else:\n",
    "        preds += np.array(clf.predict(test_set_with_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voting \n",
    "test_set['category'] = np.int32(preds>=1)\n",
    "\n",
    "test_set = (test_set\n",
    ".reset_index()\n",
    ".rename(columns = {'index':'id'})\n",
    ".drop(columns = ['node1','node2'])\n",
    ")\n",
    "\n",
    "test_set.to_csv('final_predictions_no_emb.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6657d8cf73e4192045730bbde1f7a947d4725a27f96025bfcb1ab47bc67665b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
